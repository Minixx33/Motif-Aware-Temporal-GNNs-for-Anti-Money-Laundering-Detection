
================================================================================
Logging started at: 2025-11-27 12:41:09
Log file: C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\logs\HI-Small_Trans\graphsage_t\baseline_GraphSAGE_T_20251127_124109.txt
================================================================================

[INFO] Logging to: C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\logs\HI-Small_Trans\graphsage_t\baseline_GraphSAGE_T_20251127_124109.txt
================================================================================
EXPERIMENT CONFIGURATION
================================================================================

[DATASET]
  Theory:       baseline
  Intensity:    N/A (baseline)
  Full name:    HI-Small_Trans

[MODEL] GraphSAGE-T
  Hidden dim:   128
  Num layers:   2
  Dropout:      0.2

[TRAINING]
  Device:       cuda
  Batch size:   8192
  Learning rate: 0.0005
  Epochs:       350
  Early stop:   25 epochs

[PATHS]
  Graphs:       C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\graphs\HI-Small_Trans
  Splits:       C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\splits\HI-Small_Trans
  Results:      C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\results\HI-Small_Trans\graphsage_t
  Logs:         C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\logs\HI-Small_Trans\graphsage_t

[EXPERIMENT]
  Seed:         42
  Exp name:     default_experiment

================================================================================

[DEVICE] cuda


======= GRAPH SAGE-T TRAINING (FP32) =======
Batch size:       8192
Eval batch size:  16384
Device:           cuda
===========================================


Starting training...

Epoch 001 | train_loss=10.4756 val_loss=0.5538 P=0.002 R=0.028 F1=0.004 ROC-AUC=0.572 AUPR=0.001 time=55.02s
Epoch 002 | train_loss=0.5019 val_loss=0.3814 P=0.002 R=0.686 F1=0.004 ROC-AUC=0.648 AUPR=0.002 time=53.64s
Epoch 003 | train_loss=0.4335 val_loss=0.3532 P=0.047 R=0.034 F1=0.039 ROC-AUC=0.698 AUPR=0.005 time=53.16s
Epoch 004 | train_loss=0.3770 val_loss=0.3130 P=0.041 R=0.117 F1=0.061 ROC-AUC=0.773 AUPR=0.011 time=52.12s
Epoch 005 | train_loss=0.3524 val_loss=0.2928 P=0.046 R=0.092 F1=0.061 ROC-AUC=0.802 AUPR=0.017 time=51.03s
Epoch 006 | train_loss=0.3344 val_loss=0.2831 P=0.051 R=0.095 F1=0.066 ROC-AUC=0.807 AUPR=0.017 time=53.76s
Epoch 007 | train_loss=0.3118 val_loss=0.2602 P=0.050 R=0.105 F1=0.068 ROC-AUC=0.841 AUPR=0.020 time=58.79s
Epoch 008 | train_loss=0.2847 val_loss=0.1753 P=0.090 R=0.218 F1=0.128 ROC-AUC=0.915 AUPR=0.058 time=54.53s
Epoch 009 | train_loss=0.2028 val_loss=0.1418 P=0.141 R=0.191 F1=0.162 ROC-AUC=0.966 AUPR=0.083 time=54.10s
Epoch 010 | train_loss=0.1731 val_loss=0.1355 P=0.133 R=0.281 F1=0.181 ROC-AUC=0.970 AUPR=0.104 time=55.95s
Epoch 011 | train_loss=0.1614 val_loss=0.1303 P=0.188 R=0.240 F1=0.211 ROC-AUC=0.974 AUPR=0.128 time=51.29s
Epoch 012 | train_loss=0.1884 val_loss=0.1233 P=0.190 R=0.269 F1=0.223 ROC-AUC=0.974 AUPR=0.135 time=50.69s
Epoch 013 | train_loss=0.1900 val_loss=0.1250 P=0.246 R=0.224 F1=0.234 ROC-AUC=0.976 AUPR=0.148 time=53.08s
Epoch 014 | train_loss=0.2012 val_loss=0.1799 P=0.262 R=0.199 F1=0.226 ROC-AUC=0.927 AUPR=0.132 time=56.52s
Epoch 015 | train_loss=0.1498 val_loss=0.1257 P=0.265 R=0.239 F1=0.252 ROC-AUC=0.969 AUPR=0.154 time=58.03s
Epoch 016 | train_loss=0.1518 val_loss=0.1242 P=0.242 R=0.265 F1=0.253 ROC-AUC=0.970 AUPR=0.160 time=55.02s
Epoch 017 | train_loss=0.1432 val_loss=0.1218 P=0.256 R=0.271 F1=0.263 ROC-AUC=0.972 AUPR=0.167 time=54.85s
Epoch 018 | train_loss=0.1604 val_loss=0.1219 P=0.290 R=0.248 F1=0.267 ROC-AUC=0.973 AUPR=0.171 time=57.44s
Epoch 019 | train_loss=0.1411 val_loss=0.1204 P=0.248 R=0.278 F1=0.262 ROC-AUC=0.973 AUPR=0.166 time=61.13s
Epoch 020 | train_loss=0.1614 val_loss=0.1205 P=0.269 R=0.271 F1=0.270 ROC-AUC=0.973 AUPR=0.173 time=55.89s
Epoch 021 | train_loss=0.1544 val_loss=0.1208 P=0.251 R=0.297 F1=0.272 ROC-AUC=0.974 AUPR=0.174 time=50.77s
Epoch 022 | train_loss=0.2454 val_loss=0.1195 P=0.315 R=0.248 F1=0.278 ROC-AUC=0.974 AUPR=0.179 time=52.57s
Epoch 023 | train_loss=0.1586 val_loss=0.1189 P=0.276 R=0.273 F1=0.274 ROC-AUC=0.974 AUPR=0.180 time=54.33s
Epoch 024 | train_loss=0.1735 val_loss=0.1182 P=0.276 R=0.260 F1=0.268 ROC-AUC=0.974 AUPR=0.174 time=57.38s
Epoch 025 | train_loss=0.1354 val_loss=0.1170 P=0.297 R=0.251 F1=0.272 ROC-AUC=0.975 AUPR=0.178 time=58.38s
Epoch 026 | train_loss=0.1527 val_loss=0.1172 P=0.261 R=0.291 F1=0.275 ROC-AUC=0.975 AUPR=0.180 time=53.28s
Epoch 027 | train_loss=0.1989 val_loss=0.1165 P=0.281 R=0.260 F1=0.270 ROC-AUC=0.975 AUPR=0.179 time=56.70s
Epoch 028 | train_loss=0.1582 val_loss=0.1168 P=0.340 R=0.230 F1=0.274 ROC-AUC=0.975 AUPR=0.184 time=57.45s
Epoch 029 | train_loss=0.1598 val_loss=0.1163 P=0.290 R=0.280 F1=0.285 ROC-AUC=0.975 AUPR=0.193 time=55.39s
Epoch 030 | train_loss=0.1621 val_loss=0.1163 P=0.304 R=0.275 F1=0.289 ROC-AUC=0.975 AUPR=0.198 time=63.15s
Epoch 031 | train_loss=0.1488 val_loss=0.1165 P=0.266 R=0.290 F1=0.278 ROC-AUC=0.976 AUPR=0.192 time=62.51s
Epoch 032 | train_loss=0.1460 val_loss=0.1158 P=0.333 R=0.249 F1=0.285 ROC-AUC=0.976 AUPR=0.190 time=62.36s
Epoch 033 | train_loss=0.1562 val_loss=0.1146 P=0.334 R=0.237 F1=0.278 ROC-AUC=0.976 AUPR=0.192 time=54.03s
