
================================================================================
Logging started at: 2025-11-29 21:02:09.955128
Log file: C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\logs\HI-Small_Trans\seed1337_experiment2\graphsage-t\seed1337_experiment2_20251129_210209.txt
================================================================================

[INFO] Logging to: C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\logs\HI-Small_Trans\seed1337_experiment2\graphsage-t\seed1337_experiment2_20251129_210209.txt
================================================================================
EXPERIMENT CONFIG SUMMARY
================================================================================

[DATASET]
  Name:       HI-Small_Trans
  Theory:     baseline
  Intensity:  None

[MODEL]
  GraphSAGE-T
  Hidden dim: 128

[PATHS]
  Graphs:     C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\graphs\HI-Small_Trans
  Splits:     C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\splits\HI-Small_Trans
  Results:    C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\results\HI-Small_Trans\seed1337_experiment2\graphsage-t
  Logs:       C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\logs\HI-Small_Trans\seed1337_experiment2\graphsage-t
================================================================================

[DEVICE] cuda


======= GRAPHSAGE-T TRAINING (FP32) =======
Batch size:       8192
Eval batch size:  16384
Device:           cuda
===========================================


Starting training for 350 epochs...

Epoch 001 | train_loss=7.8763 val_loss=0.4633 P=0.001 R=0.271 F1=0.003 ROC-AUC=0.531 AUPR=0.001 time=52.22s
Epoch 002 | train_loss=0.5294 val_loss=0.5052 P=0.002 R=0.077 F1=0.003 ROC-AUC=0.582 AUPR=0.001 time=53.21s
Epoch 003 | train_loss=0.4680 val_loss=0.3936 P=0.002 R=0.162 F1=0.004 ROC-AUC=0.638 AUPR=0.002 time=53.92s
Epoch 004 | train_loss=0.4243 val_loss=0.3619 P=0.002 R=0.728 F1=0.003 ROC-AUC=0.654 AUPR=0.002 time=56.12s
Epoch 005 | train_loss=0.3979 val_loss=0.3653 P=0.075 R=0.021 F1=0.033 ROC-AUC=0.682 AUPR=0.005 time=56.46s
Epoch 006 | train_loss=0.3788 val_loss=0.3473 P=0.154 R=0.020 F1=0.036 ROC-AUC=0.705 AUPR=0.007 time=55.21s
Epoch 007 | train_loss=0.3912 val_loss=0.3098 P=0.064 R=0.060 F1=0.062 ROC-AUC=0.785 AUPR=0.018 time=52.93s
Epoch 008 | train_loss=0.3695 val_loss=0.3041 P=0.071 R=0.089 F1=0.079 ROC-AUC=0.769 AUPR=0.020 time=54.76s
Epoch 009 | train_loss=0.3242 val_loss=0.2808 P=0.062 R=0.085 F1=0.072 ROC-AUC=0.806 AUPR=0.021 time=53.76s
Epoch 010 | train_loss=0.3140 val_loss=0.2581 P=0.046 R=0.137 F1=0.069 ROC-AUC=0.850 AUPR=0.022 time=54.05s
Epoch 011 | train_loss=0.3175 val_loss=0.2380 P=0.093 R=0.097 F1=0.095 ROC-AUC=0.868 AUPR=0.035 time=52.95s
Epoch 012 | train_loss=0.2695 val_loss=0.1859 P=0.161 R=0.165 F1=0.163 ROC-AUC=0.926 AUPR=0.080 time=55.30s
Epoch 013 | train_loss=0.2081 val_loss=0.1436 P=0.192 R=0.266 F1=0.223 ROC-AUC=0.954 AUPR=0.143 time=56.56s
Epoch 014 | train_loss=0.1543 val_loss=0.1234 P=0.280 R=0.244 F1=0.261 ROC-AUC=0.974 AUPR=0.167 time=54.99s
Epoch 015 | train_loss=0.1516 val_loss=0.1169 P=0.235 R=0.276 F1=0.254 ROC-AUC=0.977 AUPR=0.168 time=55.08s
Epoch 016 | train_loss=0.1629 val_loss=0.1171 P=0.307 R=0.236 F1=0.267 ROC-AUC=0.978 AUPR=0.178 time=53.73s
Epoch 017 | train_loss=0.1368 val_loss=0.1136 P=0.253 R=0.283 F1=0.267 ROC-AUC=0.978 AUPR=0.176 time=54.37s
Epoch 018 | train_loss=0.1733 val_loss=0.1769 P=0.271 R=0.270 F1=0.271 ROC-AUC=0.926 AUPR=0.184 time=54.75s
Epoch 019 | train_loss=0.1512 val_loss=0.1110 P=0.272 R=0.274 F1=0.273 ROC-AUC=0.979 AUPR=0.187 time=55.68s
Epoch 020 | train_loss=0.1810 val_loss=0.1333 P=0.339 R=0.227 F1=0.272 ROC-AUC=0.960 AUPR=0.182 time=55.42s
Epoch 021 | train_loss=0.1613 val_loss=0.1131 P=0.255 R=0.286 F1=0.269 ROC-AUC=0.978 AUPR=0.186 time=55.93s
Epoch 022 | train_loss=0.1674 val_loss=0.1214 P=0.306 R=0.245 F1=0.272 ROC-AUC=0.971 AUPR=0.182 time=55.27s
Epoch 023 | train_loss=0.1546 val_loss=0.1236 P=0.253 R=0.280 F1=0.266 ROC-AUC=0.972 AUPR=0.176 time=55.78s
Epoch 024 | train_loss=0.1443 val_loss=0.1199 P=0.274 R=0.257 F1=0.265 ROC-AUC=0.972 AUPR=0.178 time=54.73s
Epoch 025 | train_loss=0.1545 val_loss=0.1192 P=0.316 R=0.241 F1=0.274 ROC-AUC=0.973 AUPR=0.184 time=53.42s
Epoch 026 | train_loss=0.1378 val_loss=0.1231 P=0.275 R=0.265 F1=0.270 ROC-AUC=0.973 AUPR=0.182 time=56.76s
Epoch 027 | train_loss=0.1381 val_loss=0.1169 P=0.284 R=0.258 F1=0.270 ROC-AUC=0.975 AUPR=0.186 time=58.34s
Epoch 028 | train_loss=0.1543 val_loss=0.1175 P=0.298 R=0.248 F1=0.271 ROC-AUC=0.974 AUPR=0.184 time=58.22s
Epoch 029 | train_loss=0.1673 val_loss=0.1203 P=0.308 R=0.251 F1=0.276 ROC-AUC=0.974 AUPR=0.186 time=55.94s
Epoch 030 | train_loss=0.1417 val_loss=0.1187 P=0.285 R=0.254 F1=0.269 ROC-AUC=0.975 AUPR=0.185 time=56.07s
Epoch 031 | train_loss=0.1463 val_loss=0.1164 P=0.256 R=0.275 F1=0.265 ROC-AUC=0.976 AUPR=0.182 time=57.83s
Epoch 032 | train_loss=0.1680 val_loss=0.1169 P=0.335 R=0.230 F1=0.273 ROC-AUC=0.976 AUPR=0.192 time=54.56s
Epoch 033 | train_loss=0.1500 val_loss=0.1141 P=0.272 R=0.279 F1=0.276 ROC-AUC=0.977 AUPR=0.190 time=59.73s
Epoch 034 | train_loss=0.1392 val_loss=0.1132 P=0.326 R=0.252 F1=0.284 ROC-AUC=0.977 AUPR=0.205 time=54.39s
Epoch 035 | train_loss=0.1397 val_loss=0.1218 P=0.304 R=0.267 F1=0.284 ROC-AUC=0.971 AUPR=0.192 time=53.86s
Epoch 036 | train_loss=0.1538 val_loss=0.1129 P=0.256 R=0.289 F1=0.271 ROC-AUC=0.978 AUPR=0.196 time=56.41s
Epoch 037 | train_loss=0.1363 val_loss=0.1108 P=0.245 R=0.339 F1=0.284 ROC-AUC=0.979 AUPR=0.208 time=54.67s
Epoch 038 | train_loss=0.1517 val_loss=0.1109 P=0.333 R=0.254 F1=0.288 ROC-AUC=0.978 AUPR=0.210 time=55.87s
Epoch 039 | train_loss=0.1581 val_loss=0.1099 P=0.297 R=0.273 F1=0.285 ROC-AUC=0.978 AUPR=0.216 time=55.17s
Epoch 040 | train_loss=0.1333 val_loss=0.1138 P=0.317 R=0.285 F1=0.300 ROC-AUC=0.979 AUPR=0.222 time=54.78s
Epoch 041 | train_loss=0.1321 val_loss=0.1113 P=0.332 R=0.250 F1=0.285 ROC-AUC=0.978 AUPR=0.211 time=52.41s
Epoch 042 | train_loss=0.1342 val_loss=0.1145 P=0.336 R=0.294 F1=0.314 ROC-AUC=0.979 AUPR=0.237 time=54.93s
Epoch 043 | train_loss=0.1330 val_loss=0.1076 P=0.320 R=0.284 F1=0.301 ROC-AUC=0.979 AUPR=0.230 time=54.47s
Epoch 044 | train_loss=0.1309 val_loss=0.1083 P=0.388 R=0.256 F1=0.308 ROC-AUC=0.979 AUPR=0.235 time=55.42s
Epoch 045 | train_loss=0.1605 val_loss=0.1079 P=0.326 R=0.282 F1=0.302 ROC-AUC=0.979 AUPR=0.232 time=57.09s
Epoch 046 | train_loss=0.1291 val_loss=0.1073 P=0.361 R=0.260 F1=0.302 ROC-AUC=0.979 AUPR=0.230 time=53.77s
Epoch 047 | train_loss=0.1297 val_loss=0.1065 P=0.339 R=0.264 F1=0.297 ROC-AUC=0.979 AUPR=0.230 time=52.75s
Epoch 048 | train_loss=0.1294 val_loss=0.1058 P=0.309 R=0.289 F1=0.299 ROC-AUC=0.980 AUPR=0.234 time=53.24s
Epoch 049 | train_loss=0.1238 val_loss=0.1059 P=0.323 R=0.305 F1=0.314 ROC-AUC=0.980 AUPR=0.240 time=52.48s
Epoch 050 | train_loss=0.1346 val_loss=0.1052 P=0.301 R=0.300 F1=0.301 ROC-AUC=0.980 AUPR=0.230 time=52.12s
Epoch 051 | train_loss=0.1484 val_loss=0.1051 P=0.308 R=0.314 F1=0.311 ROC-AUC=0.980 AUPR=0.248 time=53.33s
Epoch 052 | train_loss=0.1507 val_loss=0.1061 P=0.363 R=0.271 F1=0.310 ROC-AUC=0.980 AUPR=0.243 time=55.76s
Epoch 053 | train_loss=0.1382 val_loss=0.1087 P=0.334 R=0.281 F1=0.305 ROC-AUC=0.980 AUPR=0.240 time=55.29s
Epoch 054 | train_loss=0.1553 val_loss=0.1053 P=0.373 R=0.257 F1=0.304 ROC-AUC=0.980 AUPR=0.241 time=54.36s
Epoch 055 | train_loss=0.1354 val_loss=0.1043 P=0.382 R=0.264 F1=0.312 ROC-AUC=0.980 AUPR=0.249 time=55.40s
Epoch 056 | train_loss=0.1562 val_loss=0.1067 P=0.352 R=0.302 F1=0.325 ROC-AUC=0.980 AUPR=0.260 time=55.60s
Epoch 057 | train_loss=0.1266 val_loss=0.1056 P=0.353 R=0.307 F1=0.328 ROC-AUC=0.980 AUPR=0.258 time=53.50s
Epoch 058 | train_loss=0.1693 val_loss=0.1101 P=0.347 R=0.298 F1=0.321 ROC-AUC=0.980 AUPR=0.258 time=54.75s
Epoch 059 | train_loss=0.1723 val_loss=0.1065 P=0.326 R=0.322 F1=0.324 ROC-AUC=0.980 AUPR=0.263 time=54.24s
Epoch 060 | train_loss=0.1265 val_loss=0.1060 P=0.371 R=0.290 F1=0.325 ROC-AUC=0.980 AUPR=0.261 time=56.16s
Epoch 061 | train_loss=0.1536 val_loss=0.1036 P=0.313 R=0.302 F1=0.307 ROC-AUC=0.981 AUPR=0.254 time=54.81s
Epoch 062 | train_loss=0.1367 val_loss=0.1433 P=0.320 R=0.330 F1=0.325 ROC-AUC=0.941 AUPR=0.264 time=54.87s
Epoch 063 | train_loss=0.1356 val_loss=0.1043 P=0.317 R=0.333 F1=0.325 ROC-AUC=0.980 AUPR=0.267 time=56.12s
Epoch 064 | train_loss=0.1512 val_loss=0.1032 P=0.331 R=0.300 F1=0.315 ROC-AUC=0.981 AUPR=0.261 time=56.53s
Epoch 065 | train_loss=0.1282 val_loss=0.1034 P=0.381 R=0.294 F1=0.332 ROC-AUC=0.980 AUPR=0.271 time=59.13s
Epoch 066 | train_loss=0.1427 val_loss=0.1046 P=0.363 R=0.305 F1=0.332 ROC-AUC=0.980 AUPR=0.274 time=56.97s
Epoch 067 | train_loss=0.1757 val_loss=0.1279 P=0.388 R=0.286 F1=0.329 ROC-AUC=0.953 AUPR=0.274 time=56.89s
Epoch 068 | train_loss=0.1819 val_loss=0.1040 P=0.316 R=0.274 F1=0.294 ROC-AUC=0.981 AUPR=0.242 time=55.84s
Epoch 069 | train_loss=0.1641 val_loss=0.1021 P=0.309 R=0.317 F1=0.313 ROC-AUC=0.981 AUPR=0.261 time=56.92s
Epoch 070 | train_loss=0.1515 val_loss=0.3680 P=0.370 R=0.259 F1=0.305 ROC-AUC=0.914 AUPR=0.251 time=56.09s
Epoch 071 | train_loss=0.1483 val_loss=0.1005 P=0.421 R=0.266 F1=0.326 ROC-AUC=0.981 AUPR=0.276 time=53.92s
Epoch 072 | train_loss=0.1299 val_loss=0.1006 P=0.374 R=0.303 F1=0.335 ROC-AUC=0.982 AUPR=0.278 time=55.21s
Epoch 073 | train_loss=0.1571 val_loss=0.1009 P=0.420 R=0.292 F1=0.345 ROC-AUC=0.982 AUPR=0.285 time=54.37s
Epoch 074 | train_loss=0.1471 val_loss=0.1006 P=0.364 R=0.310 F1=0.335 ROC-AUC=0.983 AUPR=0.286 time=54.58s
Epoch 075 | train_loss=0.1446 val_loss=0.1003 P=0.446 R=0.266 F1=0.334 ROC-AUC=0.982 AUPR=0.276 time=53.03s
Epoch 076 | train_loss=0.1513 val_loss=0.0998 P=0.399 R=0.303 F1=0.344 ROC-AUC=0.982 AUPR=0.287 time=53.91s
Epoch 077 | train_loss=0.1517 val_loss=0.0996 P=0.451 R=0.271 F1=0.339 ROC-AUC=0.982 AUPR=0.284 time=55.32s
Epoch 078 | train_loss=0.1440 val_loss=0.6456 P=0.331 R=0.286 F1=0.307 ROC-AUC=0.856 AUPR=0.242 time=52.86s
Epoch 079 | train_loss=0.1338 val_loss=0.0986 P=0.404 R=0.297 F1=0.342 ROC-AUC=0.982 AUPR=0.290 time=53.02s
Epoch 080 | train_loss=0.1298 val_loss=0.0975 P=0.377 R=0.336 F1=0.355 ROC-AUC=0.983 AUPR=0.300 time=52.68s
Epoch 081 | train_loss=0.1358 val_loss=0.0986 P=0.466 R=0.295 F1=0.361 ROC-AUC=0.983 AUPR=0.304 time=53.14s
Epoch 082 | train_loss=0.1727 val_loss=1.0527 P=0.378 R=0.238 F1=0.292 ROC-AUC=0.869 AUPR=0.228 time=52.58s
Epoch 083 | train_loss=0.1438 val_loss=0.3956 P=0.345 R=0.296 F1=0.319 ROC-AUC=0.894 AUPR=0.256 time=52.65s
Epoch 084 | train_loss=0.1667 val_loss=0.0982 P=0.433 R=0.303 F1=0.356 ROC-AUC=0.982 AUPR=0.307 time=53.79s
Epoch 085 | train_loss=0.1129 val_loss=0.0980 P=0.423 R=0.296 F1=0.349 ROC-AUC=0.983 AUPR=0.301 time=53.39s
Epoch 086 | train_loss=0.1265 val_loss=0.1016 P=0.440 R=0.292 F1=0.352 ROC-AUC=0.982 AUPR=0.297 time=53.51s
Epoch 087 | train_loss=0.1420 val_loss=0.2944 P=0.388 R=0.269 F1=0.318 ROC-AUC=0.919 AUPR=0.257 time=54.11s
Epoch 088 | train_loss=0.1457 val_loss=0.0977 P=0.478 R=0.276 F1=0.350 ROC-AUC=0.982 AUPR=0.303 time=54.60s
Epoch 089 | train_loss=0.1075 val_loss=0.0984 P=0.467 R=0.288 F1=0.356 ROC-AUC=0.982 AUPR=0.299 time=54.79s
Epoch 090 | train_loss=0.1473 val_loss=0.0983 P=0.433 R=0.304 F1=0.357 ROC-AUC=0.982 AUPR=0.315 time=58.11s
Epoch 091 | train_loss=0.1370 val_loss=0.0976 P=0.407 R=0.312 F1=0.353 ROC-AUC=0.983 AUPR=0.303 time=56.27s
Epoch 092 | train_loss=0.1059 val_loss=0.0977 P=0.426 R=0.312 F1=0.360 ROC-AUC=0.983 AUPR=0.310 time=54.85s
Epoch 093 | train_loss=0.1230 val_loss=0.0993 P=0.450 R=0.282 F1=0.347 ROC-AUC=0.982 AUPR=0.302 time=55.79s
Epoch 094 | train_loss=0.1183 val_loss=0.0974 P=0.444 R=0.311 F1=0.365 ROC-AUC=0.983 AUPR=0.310 time=57.33s
Epoch 095 | train_loss=0.1392 val_loss=0.0975 P=0.392 R=0.324 F1=0.355 ROC-AUC=0.982 AUPR=0.306 time=57.69s
Epoch 096 | train_loss=0.1161 val_loss=0.3832 P=0.333 R=0.316 F1=0.324 ROC-AUC=0.876 AUPR=0.261 time=55.14s
Epoch 097 | train_loss=0.1224 val_loss=0.7907 P=0.430 R=0.250 F1=0.316 ROC-AUC=0.856 AUPR=0.250 time=57.58s
Epoch 098 | train_loss=0.1433 val_loss=0.0973 P=0.437 R=0.303 F1=0.358 ROC-AUC=0.983 AUPR=0.313 time=59.59s
Epoch 099 | train_loss=0.1308 val_loss=0.0999 P=0.397 R=0.335 F1=0.363 ROC-AUC=0.982 AUPR=0.306 time=56.02s
Epoch 100 | train_loss=0.1301 val_loss=0.0982 P=0.443 R=0.293 F1=0.353 ROC-AUC=0.982 AUPR=0.310 time=56.11s
Epoch 101 | train_loss=0.1452 val_loss=0.0983 P=0.403 R=0.315 F1=0.354 ROC-AUC=0.983 AUPR=0.307 time=56.67s
Epoch 102 | train_loss=0.1272 val_loss=0.0985 P=0.458 R=0.328 F1=0.382 ROC-AUC=0.982 AUPR=0.323 time=55.48s
Epoch 103 | train_loss=0.1390 val_loss=0.0987 P=0.449 R=0.318 F1=0.372 ROC-AUC=0.983 AUPR=0.321 time=55.45s
Epoch 104 | train_loss=0.1416 val_loss=0.0994 P=0.441 R=0.317 F1=0.369 ROC-AUC=0.982 AUPR=0.309 time=55.68s
Epoch 105 | train_loss=0.1348 val_loss=0.0972 P=0.398 R=0.348 F1=0.372 ROC-AUC=0.983 AUPR=0.323 time=56.10s
Epoch 106 | train_loss=0.1269 val_loss=0.0974 P=0.427 R=0.321 F1=0.367 ROC-AUC=0.982 AUPR=0.327 time=55.73s
Epoch 107 | train_loss=0.1223 val_loss=0.0995 P=0.398 R=0.341 F1=0.367 ROC-AUC=0.982 AUPR=0.321 time=61.26s
Epoch 108 | train_loss=0.1285 val_loss=0.0978 P=0.438 R=0.313 F1=0.365 ROC-AUC=0.982 AUPR=0.312 time=52.79s
Epoch 109 | train_loss=0.1375 val_loss=0.0966 P=0.436 R=0.308 F1=0.361 ROC-AUC=0.983 AUPR=0.319 time=55.88s
Epoch 110 | train_loss=0.1081 val_loss=0.1844 P=0.410 R=0.304 F1=0.349 ROC-AUC=0.937 AUPR=0.293 time=59.07s
Epoch 111 | train_loss=0.1127 val_loss=0.0957 P=0.426 R=0.343 F1=0.380 ROC-AUC=0.983 AUPR=0.326 time=57.09s
Epoch 112 | train_loss=0.1311 val_loss=0.2542 P=0.409 R=0.313 F1=0.354 ROC-AUC=0.923 AUPR=0.301 time=54.98s
Epoch 113 | train_loss=0.1159 val_loss=0.1007 P=0.424 R=0.334 F1=0.373 ROC-AUC=0.983 AUPR=0.330 time=56.26s
Epoch 114 | train_loss=0.1257 val_loss=0.1971 P=0.425 R=0.316 F1=0.362 ROC-AUC=0.933 AUPR=0.312 time=55.43s
Epoch 115 | train_loss=0.1201 val_loss=0.0969 P=0.427 R=0.335 F1=0.376 ROC-AUC=0.983 AUPR=0.330 time=54.63s
Epoch 116 | train_loss=0.1533 val_loss=0.0965 P=0.430 R=0.313 F1=0.362 ROC-AUC=0.983 AUPR=0.322 time=55.40s
Epoch 117 | train_loss=0.1212 val_loss=0.0956 P=0.466 R=0.314 F1=0.375 ROC-AUC=0.983 AUPR=0.332 time=52.03s
Epoch 118 | train_loss=0.1136 val_loss=0.0981 P=0.417 R=0.320 F1=0.362 ROC-AUC=0.983 AUPR=0.325 time=54.44s
Epoch 119 | train_loss=0.1201 val_loss=0.0960 P=0.421 R=0.350 F1=0.382 ROC-AUC=0.983 AUPR=0.328 time=56.28s
Epoch 120 | train_loss=0.1202 val_loss=0.0971 P=0.430 R=0.344 F1=0.382 ROC-AUC=0.983 AUPR=0.332 time=55.32s
Epoch 121 | train_loss=0.1228 val_loss=0.0970 P=0.482 R=0.318 F1=0.383 ROC-AUC=0.983 AUPR=0.332 time=53.08s
Epoch 122 | train_loss=0.1888 val_loss=0.0962 P=0.495 R=0.309 F1=0.380 ROC-AUC=0.983 AUPR=0.341 time=52.60s
Epoch 123 | train_loss=0.1194 val_loss=0.1636 P=0.102 R=0.283 F1=0.150 ROC-AUC=0.961 AUPR=0.061 time=55.03s
Epoch 124 | train_loss=0.1413 val_loss=0.0970 P=0.475 R=0.318 F1=0.381 ROC-AUC=0.983 AUPR=0.328 time=58.90s
Epoch 125 | train_loss=0.1171 val_loss=0.0969 P=0.467 R=0.325 F1=0.383 ROC-AUC=0.983 AUPR=0.333 time=55.66s
Epoch 126 | train_loss=0.1279 val_loss=0.0982 P=0.406 R=0.339 F1=0.369 ROC-AUC=0.983 AUPR=0.327 time=56.88s
Epoch 127 | train_loss=0.1094 val_loss=0.0960 P=0.444 R=0.325 F1=0.375 ROC-AUC=0.983 AUPR=0.330 time=56.12s
Epoch 128 | train_loss=0.1181 val_loss=0.0965 P=0.425 R=0.331 F1=0.372 ROC-AUC=0.983 AUPR=0.332 time=54.91s
Epoch 129 | train_loss=0.1583 val_loss=0.0999 P=0.430 R=0.339 F1=0.379 ROC-AUC=0.982 AUPR=0.328 time=53.94s
Epoch 130 | train_loss=0.1190 val_loss=0.0971 P=0.449 R=0.299 F1=0.359 ROC-AUC=0.983 AUPR=0.318 time=53.78s
Epoch 131 | train_loss=0.1599 val_loss=0.0957 P=0.444 R=0.324 F1=0.375 ROC-AUC=0.983 AUPR=0.333 time=56.00s
Epoch 132 | train_loss=0.1130 val_loss=0.0959 P=0.457 R=0.315 F1=0.373 ROC-AUC=0.983 AUPR=0.333 time=58.10s
Epoch 133 | train_loss=0.1357 val_loss=0.0964 P=0.480 R=0.294 F1=0.365 ROC-AUC=0.983 AUPR=0.326 time=56.30s
Epoch 134 | train_loss=0.1318 val_loss=0.0945 P=0.488 R=0.326 F1=0.391 ROC-AUC=0.984 AUPR=0.336 time=55.54s
Epoch 135 | train_loss=0.1144 val_loss=0.0967 P=0.467 R=0.332 F1=0.388 ROC-AUC=0.984 AUPR=0.336 time=55.87s
Epoch 136 | train_loss=0.1398 val_loss=0.0989 P=0.459 R=0.315 F1=0.373 ROC-AUC=0.983 AUPR=0.324 time=54.50s
Epoch 137 | train_loss=0.1400 val_loss=0.0970 P=0.420 R=0.355 F1=0.385 ROC-AUC=0.983 AUPR=0.330 time=54.53s
Epoch 138 | train_loss=0.1444 val_loss=0.0967 P=0.477 R=0.325 F1=0.387 ROC-AUC=0.983 AUPR=0.333 time=54.80s
Epoch 139 | train_loss=0.1330 val_loss=0.0973 P=0.502 R=0.306 F1=0.380 ROC-AUC=0.983 AUPR=0.333 time=56.74s
Epoch 140 | train_loss=0.1258 val_loss=0.0959 P=0.477 R=0.312 F1=0.377 ROC-AUC=0.983 AUPR=0.332 time=55.56s
Epoch 141 | train_loss=0.1466 val_loss=0.0967 P=0.461 R=0.351 F1=0.399 ROC-AUC=0.983 AUPR=0.340 time=55.53s
Epoch 142 | train_loss=0.1454 val_loss=0.0957 P=0.463 R=0.314 F1=0.374 ROC-AUC=0.983 AUPR=0.336 time=54.75s
Epoch 143 | train_loss=0.1385 val_loss=0.0965 P=0.442 R=0.323 F1=0.373 ROC-AUC=0.983 AUPR=0.330 time=54.96s
Epoch 144 | train_loss=0.1387 val_loss=0.0968 P=0.530 R=0.293 F1=0.378 ROC-AUC=0.983 AUPR=0.333 time=52.89s
Epoch 145 | train_loss=0.1530 val_loss=0.0966 P=0.414 R=0.350 F1=0.380 ROC-AUC=0.983 AUPR=0.336 time=53.34s
Epoch 146 | train_loss=0.1437 val_loss=0.5600 P=0.448 R=0.261 F1=0.329 ROC-AUC=0.870 AUPR=0.271 time=53.32s
Epoch 147 | train_loss=0.1089 val_loss=0.0988 P=0.486 R=0.316 F1=0.383 ROC-AUC=0.982 AUPR=0.334 time=53.16s

Early stopping at epoch 147

Total training time: 8109.32s (135.2 min)

Loading best model from epoch 122...
Evaluating final model...
======================================================================
EVALUATION RESULTS: seed1337_experiment2 TRAIN
======================================================================

STANDARD METRICS:
  Precision:        0.3840
  Recall:           0.3719
  F1-Score:         0.3778
  ROC-AUC:          0.9872
  AUPR:             0.3385
  Balanced Acc:     0.6856

IMBALANCED-AWARE METRICS:
  MCC:              0.3772
  Cohen Kappa:      0.3772
  Specificity:      0.9994

THRESHOLD: 0.941

CONFUSION MATRIX:
  True Negatives:   3042048
  False Positives:  1853
  False Negatives:  1951
  True Positives:   1155

TOP-K PRECISION:
  precision_at_100: 0.8600
  precision_at_500: 0.7760
  precision_at_1000: 0.6490
======================================================================
======================================================================
EVALUATION RESULTS: seed1337_experiment2 VAL
======================================================================

STANDARD METRICS:
  Precision:        0.4954
  Recall:           0.3089
  F1-Score:         0.3805
  ROC-AUC:          0.9833
  AUPR:             0.3408
  Balanced Acc:     0.6543

IMBALANCED-AWARE METRICS:
  MCC:              0.3907
  Cohen Kappa:      0.3800
  Specificity:      0.9997

THRESHOLD: 0.965

CONFUSION MATRIX:
  True Negatives:   1014307
  False Positives:  326
  False Negatives:  716
  True Positives:   320

TOP-K PRECISION:
  precision_at_100: 0.9000
  precision_at_500: 0.5720
  precision_at_1000: 0.3750
======================================================================
======================================================================
EVALUATION RESULTS: seed1337_experiment2 TEST
======================================================================

STANDARD METRICS:
  Precision:        0.3674
  Recall:           0.3720
  F1-Score:         0.3697
  ROC-AUC:          0.9793
  AUPR:             0.3222
  Balanced Acc:     0.6857

IMBALANCED-AWARE METRICS:
  MCC:              0.3690
  Cohen Kappa:      0.3690
  Specificity:      0.9993

THRESHOLD: 0.941

CONFUSION MATRIX:
  True Negatives:   1013971
  False Positives:  663
  False Negatives:  650
  True Positives:   385

TOP-K PRECISION:
  precision_at_100: 0.8900
  precision_at_500: 0.5400
  precision_at_1000: 0.3750
======================================================================

Saving results...
 Saved metrics to: C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\results\HI-Small_Trans\seed1337_experiment2\graphsage-t\metrics.json
 Saved prediction probabilities
 Saved experiment config

======================================================================
GraphSAGE-T Training Complete!
Results saved to: C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\results\HI-Small_Trans\seed1337_experiment2\graphsage-t
======================================================================

================================================================================
Logging ended at: 2025-11-29 23:18:51.849289
================================================================================
