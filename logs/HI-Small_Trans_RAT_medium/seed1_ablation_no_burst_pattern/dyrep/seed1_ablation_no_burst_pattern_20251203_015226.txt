
================================================================================
Logging started at: 2025-12-03 01:52:26.588089
Log file: C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\logs\HI-Small_Trans_RAT_medium\seed1_ablation_no_burst_pattern\dyrep\seed1_ablation_no_burst_pattern_20251203_015226.txt
================================================================================

[INFO] Logging to: C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\logs\HI-Small_Trans_RAT_medium\seed1_ablation_no_burst_pattern\dyrep\seed1_ablation_no_burst_pattern_20251203_015226.txt
================================================================================
EXPERIMENT CONFIG SUMMARY
================================================================================

[DATASET]
  Name:       HI-Small_Trans_RAT_medium
  Theory:     RAT
  Intensity:  medium

[MODEL]
  DyRep
  Hidden dim: 128

[PATHS]
  Graphs:     C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\graphs\HI-Small_Trans_RAT_medium
  Splits:     C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\splits\HI-Small_Trans_RAT_medium
  Results:    C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\results\HI-Small_Trans_RAT_medium\seed1_ablation_no_burst_pattern\dyrep
  Logs:       C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\logs\HI-Small_Trans_RAT_medium\seed1_ablation_no_burst_pattern\dyrep
================================================================================

[DEVICE] cuda

[INFO] Using default graph directory: C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\graphs_dyrep\HI-Small_Trans_RAT_medium__no_burst_pattern

======= DYREP-STYLE EVENT MODEL TRAINING (FP32) =======
Graph folder:    C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\graphs_dyrep\HI-Small_Trans_RAT_medium__no_burst_pattern
Splits folder:   C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\splits_dyrep\HI-Small_Trans_RAT_medium
Results folder:  C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\results\HI-Small_Trans_RAT_medium\seed1_ablation_no_burst_pattern\dyrep
Batch size:      8192
Eval batch size: 16384
Device:          cuda
=======================================================

Num nodes:        515,080
Num events/edges: 5,078,415
Node feat dim:    12
Edge feat dim:    52
Event types:      7

Split sizes:
  Train: 3,047,049
  Val:   1,015,683
  Test:  1,015,683

pos_weight (train) = 100.000

Starting training for up to 350 epochs...

Epoch 001 | train_loss=10.3355 val_loss=4.6971 P=0.050 R=0.098 F1=0.066 ROC-AUC=0.833 AUPR=0.031 time=41.49s
Epoch 002 | train_loss=1.0799 val_loss=0.1806 P=0.074 R=0.258 F1=0.115 ROC-AUC=0.946 AUPR=0.047 time=38.74s
Epoch 003 | train_loss=0.5435 val_loss=0.9321 P=0.076 R=0.189 F1=0.108 ROC-AUC=0.866 AUPR=0.051 time=36.57s
Epoch 004 | train_loss=0.4740 val_loss=0.1618 P=0.157 R=0.243 F1=0.191 ROC-AUC=0.968 AUPR=0.113 time=37.39s
Epoch 005 | train_loss=0.4542 val_loss=0.2985 P=0.144 R=0.238 F1=0.180 ROC-AUC=0.908 AUPR=0.101 time=34.61s
Epoch 006 | train_loss=0.4865 val_loss=0.3523 P=0.137 R=0.221 F1=0.169 ROC-AUC=0.908 AUPR=0.108 time=33.74s
Epoch 007 | train_loss=0.3770 val_loss=0.1469 P=0.126 R=0.218 F1=0.160 ROC-AUC=0.963 AUPR=0.078 time=34.54s
Epoch 008 | train_loss=0.3863 val_loss=0.1319 P=0.172 R=0.238 F1=0.200 ROC-AUC=0.971 AUPR=0.113 time=33.85s
Epoch 009 | train_loss=0.3258 val_loss=0.1594 P=0.214 R=0.211 F1=0.212 ROC-AUC=0.940 AUPR=0.130 time=33.28s
Epoch 010 | train_loss=0.4671 val_loss=0.1246 P=0.171 R=0.270 F1=0.209 ROC-AUC=0.976 AUPR=0.117 time=32.45s
Epoch 011 | train_loss=0.4038 val_loss=2.8342 P=0.207 R=0.213 F1=0.210 ROC-AUC=0.865 AUPR=0.135 time=32.57s
Epoch 012 | train_loss=0.2328 val_loss=0.5662 P=0.226 R=0.215 F1=0.221 ROC-AUC=0.908 AUPR=0.140 time=37.56s
Epoch 013 | train_loss=0.2832 val_loss=0.1221 P=0.190 R=0.241 F1=0.213 ROC-AUC=0.976 AUPR=0.132 time=40.63s
Epoch 014 | train_loss=0.2616 val_loss=0.1179 P=0.243 R=0.267 F1=0.254 ROC-AUC=0.978 AUPR=0.185 time=38.43s
Epoch 015 | train_loss=0.2246 val_loss=0.1231 P=0.203 R=0.209 F1=0.206 ROC-AUC=0.976 AUPR=0.127 time=38.35s
Epoch 016 | train_loss=0.1614 val_loss=0.1167 P=0.225 R=0.264 F1=0.243 ROC-AUC=0.979 AUPR=0.159 time=36.27s
Epoch 017 | train_loss=0.1929 val_loss=0.3486 P=0.193 R=0.260 F1=0.221 ROC-AUC=0.913 AUPR=0.140 time=36.55s
Epoch 018 | train_loss=0.2458 val_loss=0.1174 P=0.222 R=0.274 F1=0.245 ROC-AUC=0.977 AUPR=0.160 time=34.99s
Epoch 019 | train_loss=0.1610 val_loss=0.1133 P=0.223 R=0.274 F1=0.246 ROC-AUC=0.979 AUPR=0.163 time=33.55s
Epoch 020 | train_loss=0.1415 val_loss=0.1194 P=0.331 R=0.271 F1=0.298 ROC-AUC=0.978 AUPR=0.240 time=34.60s
Epoch 021 | train_loss=0.1613 val_loss=0.1158 P=0.228 R=0.288 F1=0.255 ROC-AUC=0.979 AUPR=0.179 time=34.10s
Epoch 022 | train_loss=0.1078 val_loss=0.1098 P=0.253 R=0.262 F1=0.258 ROC-AUC=0.981 AUPR=0.184 time=34.44s
Epoch 023 | train_loss=0.1204 val_loss=0.1071 P=0.292 R=0.297 F1=0.294 ROC-AUC=0.981 AUPR=0.232 time=33.21s
Epoch 024 | train_loss=0.2127 val_loss=0.1147 P=0.242 R=0.287 F1=0.263 ROC-AUC=0.980 AUPR=0.196 time=33.72s
Epoch 025 | train_loss=0.1447 val_loss=0.1269 P=0.245 R=0.260 F1=0.252 ROC-AUC=0.975 AUPR=0.170 time=34.38s
Epoch 026 | train_loss=0.2051 val_loss=0.1079 P=0.321 R=0.260 F1=0.287 ROC-AUC=0.981 AUPR=0.211 time=40.99s
Epoch 027 | train_loss=0.1621 val_loss=0.1168 P=0.283 R=0.230 F1=0.254 ROC-AUC=0.978 AUPR=0.190 time=41.95s
Epoch 028 | train_loss=0.1063 val_loss=0.1037 P=0.311 R=0.293 F1=0.302 ROC-AUC=0.982 AUPR=0.245 time=39.67s
Epoch 029 | train_loss=0.1000 val_loss=0.1050 P=0.328 R=0.279 F1=0.302 ROC-AUC=0.983 AUPR=0.239 time=36.93s
Epoch 030 | train_loss=0.1413 val_loss=0.1047 P=0.345 R=0.249 F1=0.289 ROC-AUC=0.982 AUPR=0.235 time=36.30s
Epoch 031 | train_loss=0.1434 val_loss=0.5616 P=0.398 R=0.255 F1=0.311 ROC-AUC=0.910 AUPR=0.263 time=37.90s
Epoch 032 | train_loss=0.2139 val_loss=0.1037 P=0.321 R=0.286 F1=0.302 ROC-AUC=0.983 AUPR=0.247 time=40.76s
Epoch 033 | train_loss=0.0964 val_loss=0.1060 P=0.340 R=0.297 F1=0.317 ROC-AUC=0.982 AUPR=0.271 time=38.76s
Epoch 034 | train_loss=0.0929 val_loss=0.1031 P=0.336 R=0.294 F1=0.313 ROC-AUC=0.982 AUPR=0.254 time=39.16s
Epoch 035 | train_loss=0.1321 val_loss=0.1238 P=0.309 R=0.272 F1=0.289 ROC-AUC=0.977 AUPR=0.229 time=38.24s
Epoch 036 | train_loss=0.1012 val_loss=0.6176 P=0.011 R=0.764 F1=0.022 ROC-AUC=0.929 AUPR=0.007 time=35.49s
Epoch 037 | train_loss=0.1762 val_loss=0.1062 P=0.333 R=0.274 F1=0.301 ROC-AUC=0.982 AUPR=0.253 time=34.67s
Epoch 038 | train_loss=0.1349 val_loss=0.1449 P=0.365 R=0.293 F1=0.325 ROC-AUC=0.970 AUPR=0.246 time=42.92s
Epoch 039 | train_loss=0.0985 val_loss=0.1002 P=0.396 R=0.284 F1=0.331 ROC-AUC=0.983 AUPR=0.291 time=39.80s
Epoch 040 | train_loss=0.1184 val_loss=0.1029 P=0.351 R=0.293 F1=0.320 ROC-AUC=0.983 AUPR=0.269 time=39.21s
Epoch 041 | train_loss=0.1685 val_loss=0.1095 P=0.375 R=0.249 F1=0.299 ROC-AUC=0.981 AUPR=0.255 time=39.31s
Epoch 042 | train_loss=0.1148 val_loss=0.1057 P=0.369 R=0.285 F1=0.321 ROC-AUC=0.982 AUPR=0.273 time=36.33s
Epoch 043 | train_loss=0.0904 val_loss=0.1012 P=0.429 R=0.270 F1=0.331 ROC-AUC=0.983 AUPR=0.290 time=35.28s
Epoch 044 | train_loss=0.1148 val_loss=0.1047 P=0.343 R=0.312 F1=0.327 ROC-AUC=0.982 AUPR=0.286 time=40.09s
Epoch 045 | train_loss=0.1590 val_loss=0.1071 P=0.356 R=0.298 F1=0.324 ROC-AUC=0.982 AUPR=0.273 time=39.20s
Epoch 046 | train_loss=0.1057 val_loss=0.1063 P=0.352 R=0.288 F1=0.317 ROC-AUC=0.981 AUPR=0.275 time=39.42s
Epoch 047 | train_loss=0.1122 val_loss=0.1084 P=0.444 R=0.269 F1=0.335 ROC-AUC=0.981 AUPR=0.286 time=41.29s
Epoch 048 | train_loss=0.1011 val_loss=0.1259 P=0.293 R=0.299 F1=0.296 ROC-AUC=0.977 AUPR=0.243 time=38.74s
Epoch 049 | train_loss=0.1506 val_loss=0.1034 P=0.387 R=0.301 F1=0.339 ROC-AUC=0.983 AUPR=0.295 time=35.64s
Epoch 050 | train_loss=0.1324 val_loss=0.1092 P=0.370 R=0.265 F1=0.309 ROC-AUC=0.981 AUPR=0.266 time=37.40s
Epoch 051 | train_loss=0.1439 val_loss=0.1102 P=0.312 R=0.287 F1=0.299 ROC-AUC=0.980 AUPR=0.256 time=37.98s
Epoch 052 | train_loss=0.0820 val_loss=0.1056 P=0.425 R=0.312 F1=0.360 ROC-AUC=0.981 AUPR=0.321 time=41.72s
Epoch 053 | train_loss=0.1359 val_loss=0.1100 P=0.347 R=0.259 F1=0.296 ROC-AUC=0.980 AUPR=0.249 time=36.35s
Epoch 054 | train_loss=0.0769 val_loss=0.1021 P=0.493 R=0.271 F1=0.350 ROC-AUC=0.982 AUPR=0.317 time=39.20s
Epoch 055 | train_loss=0.0962 val_loss=0.1060 P=0.431 R=0.276 F1=0.337 ROC-AUC=0.981 AUPR=0.301 time=34.91s
Epoch 056 | train_loss=0.0826 val_loss=0.0996 P=0.398 R=0.311 F1=0.350 ROC-AUC=0.983 AUPR=0.317 time=35.85s
Epoch 057 | train_loss=0.1192 val_loss=0.1018 P=0.410 R=0.299 F1=0.346 ROC-AUC=0.982 AUPR=0.307 time=38.86s
Epoch 058 | train_loss=0.0938 val_loss=0.1040 P=0.431 R=0.290 F1=0.347 ROC-AUC=0.982 AUPR=0.305 time=41.54s
Epoch 059 | train_loss=0.1006 val_loss=0.0982 P=0.344 R=0.356 F1=0.350 ROC-AUC=0.983 AUPR=0.321 time=39.80s
Epoch 060 | train_loss=0.0743 val_loss=0.1188 P=0.353 R=0.296 F1=0.322 ROC-AUC=0.979 AUPR=0.285 time=38.43s
Epoch 061 | train_loss=0.1092 val_loss=0.0963 P=0.374 R=0.309 F1=0.338 ROC-AUC=0.984 AUPR=0.309 time=37.19s
Epoch 062 | train_loss=0.0748 val_loss=0.1025 P=0.411 R=0.287 F1=0.338 ROC-AUC=0.983 AUPR=0.309 time=34.15s
Epoch 063 | train_loss=0.1032 val_loss=0.0986 P=0.484 R=0.311 F1=0.379 ROC-AUC=0.982 AUPR=0.349 time=21.70s
Epoch 064 | train_loss=0.0961 val_loss=0.1100 P=0.335 R=0.342 F1=0.339 ROC-AUC=0.981 AUPR=0.300 time=21.27s
Epoch 065 | train_loss=0.0918 val_loss=0.0999 P=0.479 R=0.285 F1=0.357 ROC-AUC=0.983 AUPR=0.330 time=21.62s
Epoch 066 | train_loss=0.0797 val_loss=0.0992 P=0.363 R=0.314 F1=0.337 ROC-AUC=0.983 AUPR=0.314 time=21.56s
Epoch 067 | train_loss=0.1081 val_loss=0.0945 P=0.389 R=0.351 F1=0.369 ROC-AUC=0.984 AUPR=0.343 time=21.00s
Epoch 068 | train_loss=0.0782 val_loss=0.1063 P=0.428 R=0.285 F1=0.342 ROC-AUC=0.981 AUPR=0.308 time=21.03s
Epoch 069 | train_loss=0.0840 val_loss=0.1113 P=0.372 R=0.325 F1=0.347 ROC-AUC=0.979 AUPR=0.325 time=21.72s
Epoch 070 | train_loss=0.1383 val_loss=0.1185 P=0.399 R=0.272 F1=0.323 ROC-AUC=0.979 AUPR=0.292 time=22.95s
Epoch 071 | train_loss=0.3121 val_loss=0.1109 P=0.418 R=0.298 F1=0.348 ROC-AUC=0.979 AUPR=0.325 time=34.72s
Epoch 072 | train_loss=0.0680 val_loss=0.1128 P=0.408 R=0.295 F1=0.342 ROC-AUC=0.979 AUPR=0.311 time=35.90s
Epoch 073 | train_loss=0.0655 val_loss=0.1120 P=0.426 R=0.323 F1=0.368 ROC-AUC=0.980 AUPR=0.341 time=39.19s
Epoch 074 | train_loss=0.0931 val_loss=0.1104 P=0.417 R=0.313 F1=0.358 ROC-AUC=0.980 AUPR=0.328 time=38.68s
Epoch 075 | train_loss=0.0763 val_loss=0.0984 P=0.531 R=0.331 F1=0.408 ROC-AUC=0.982 AUPR=0.382 time=38.52s
Epoch 076 | train_loss=0.0628 val_loss=0.1711 P=0.280 R=0.294 F1=0.287 ROC-AUC=0.970 AUPR=0.253 time=39.84s
Epoch 077 | train_loss=0.0757 val_loss=0.1399 P=0.472 R=0.219 F1=0.299 ROC-AUC=0.973 AUPR=0.258 time=37.17s
Epoch 078 | train_loss=0.1095 val_loss=0.1103 P=0.368 R=0.346 F1=0.357 ROC-AUC=0.980 AUPR=0.332 time=35.17s
Epoch 079 | train_loss=0.0868 val_loss=0.1129 P=0.316 R=0.338 F1=0.327 ROC-AUC=0.979 AUPR=0.298 time=37.32s
Epoch 080 | train_loss=0.1333 val_loss=0.1230 P=0.496 R=0.264 F1=0.345 ROC-AUC=0.976 AUPR=0.323 time=39.42s
Epoch 081 | train_loss=0.0615 val_loss=0.1041 P=0.532 R=0.300 F1=0.384 ROC-AUC=0.982 AUPR=0.368 time=41.06s
Epoch 082 | train_loss=0.2624 val_loss=0.1505 P=0.358 R=0.256 F1=0.299 ROC-AUC=0.970 AUPR=0.250 time=40.14s
Epoch 083 | train_loss=0.1868 val_loss=0.1480 P=0.377 R=0.311 F1=0.341 ROC-AUC=0.971 AUPR=0.294 time=38.79s
Epoch 084 | train_loss=0.0620 val_loss=0.1637 P=0.371 R=0.262 F1=0.307 ROC-AUC=0.967 AUPR=0.256 time=35.39s
Epoch 085 | train_loss=0.2345 val_loss=0.1828 P=0.349 R=0.284 F1=0.313 ROC-AUC=0.964 AUPR=0.258 time=38.43s
Epoch 086 | train_loss=0.0652 val_loss=0.1719 P=0.409 R=0.288 F1=0.338 ROC-AUC=0.965 AUPR=0.290 time=38.63s
Epoch 087 | train_loss=0.0886 val_loss=0.1069 P=0.418 R=0.324 F1=0.365 ROC-AUC=0.980 AUPR=0.351 time=38.35s
Epoch 088 | train_loss=0.2325 val_loss=0.2033 P=0.283 R=0.206 F1=0.239 ROC-AUC=0.958 AUPR=0.194 time=38.99s
Epoch 089 | train_loss=0.0705 val_loss=0.1460 P=0.432 R=0.250 F1=0.316 ROC-AUC=0.971 AUPR=0.279 time=39.08s
Epoch 090 | train_loss=0.0918 val_loss=0.1561 P=0.369 R=0.261 F1=0.305 ROC-AUC=0.969 AUPR=0.260 time=36.63s

Early stopping at epoch 90 (no val AUPR improvement for 15 epochs)

Total training time: 3259.82s (54.3 min)

Loading best model from epoch 75...
Evaluating final model on train/val/test...
======================================================================
EVALUATION RESULTS: seed1_ablation_no_burst_pattern TRAIN
======================================================================

STANDARD METRICS:
  Precision:        0.5510
  Recall:           0.4208
  F1-Score:         0.4772
  ROC-AUC:          0.9897
  AUPR:             0.5038
  Balanced Acc:     0.7103

IMBALANCED-AWARE METRICS:
  MCC:              0.4812
  Cohen Kappa:      0.4768
  Specificity:      0.9997

THRESHOLD: 0.970

CONFUSION MATRIX:
  True Negatives:   3043963
  False Positives:  788
  False Negatives:  1331
  True Positives:   967

TOP-K PRECISION:
  precision_at_100: 0.9900
  precision_at_500: 0.9100
  precision_at_1000: 0.7340
======================================================================
======================================================================
EVALUATION RESULTS: seed1_ablation_no_burst_pattern VAL
======================================================================

STANDARD METRICS:
  Precision:        0.5312
  Recall:           0.3309
  F1-Score:         0.4077
  ROC-AUC:          0.9823
  AUPR:             0.3825
  Balanced Acc:     0.6653

IMBALANCED-AWARE METRICS:
  MCC:              0.4187
  Cohen Kappa:      0.4073
  Specificity:      0.9997

THRESHOLD: 0.960

CONFUSION MATRIX:
  True Negatives:   1014285
  False Positives:  316
  False Negatives:  724
  True Positives:   358

TOP-K PRECISION:
  precision_at_100: 0.9500
  precision_at_500: 0.6260
  precision_at_1000: 0.4080
======================================================================
======================================================================
EVALUATION RESULTS: seed1_ablation_no_burst_pattern TEST
======================================================================

STANDARD METRICS:
  Precision:        0.4882
  Recall:           0.4257
  F1-Score:         0.4548
  ROC-AUC:          0.9836
  AUPR:             0.4521
  Balanced Acc:     0.7125

IMBALANCED-AWARE METRICS:
  MCC:              0.4550
  Cohen Kappa:      0.4539
  Specificity:      0.9992

THRESHOLD: 0.956

CONFUSION MATRIX:
  True Negatives:   1013084
  False Positives:  802
  False Negatives:  1032
  True Positives:   765

TOP-K PRECISION:
  precision_at_100: 0.9500
  precision_at_500: 0.8300
  precision_at_1000: 0.6200
======================================================================

Saving results...
  - Saved metrics to: C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\results\HI-Small_Trans_RAT_medium\seed1_ablation_no_burst_pattern\dyrep\metrics.json
  - Saved prediction probabilities
  - Saved experiment config

======================================================================
DyRep-style Training Complete!
Results saved to: C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\results\HI-Small_Trans_RAT_medium\seed1_ablation_no_burst_pattern\dyrep
======================================================================

================================================================================
Logging ended at: 2025-12-03 02:49:20.079555
================================================================================
