
================================================================================
Logging started at: 2026-02-25 17:15:07.000506
Log file: C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\logs\HI-Small_Trans_RAT_medium\seed4_seed5_experiment\graphsage-t\seed4_seed5_experiment_20260225_171506.txt
================================================================================

[INFO] Logging to: C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\logs\HI-Small_Trans_RAT_medium\seed4_seed5_experiment\graphsage-t\seed4_seed5_experiment_20260225_171506.txt
================================================================================
EXPERIMENT CONFIG SUMMARY
================================================================================

[DATASET]
  Name:       HI-Small_Trans_RAT_medium
  Theory:     RAT
  Intensity:  medium

[MODEL]
  GraphSAGE-T
  Hidden dim: 128

[PATHS]
  Graphs:     C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\graphs\HI-Small_Trans_RAT_medium
  Splits:     C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\splits\HI-Small_Trans_RAT_medium
  Results:    C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\results\HI-Small_Trans_RAT_medium\seed4_seed5_experiment\graphsage-t
  Logs:       C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\logs\HI-Small_Trans_RAT_medium\seed4_seed5_experiment\graphsage-t
================================================================================

[DEVICE] cuda


======= GRAPHSAGE-T TRAINING (FP32) =======
Batch size:       8192
Eval batch size:  16384
Device:           cuda
===========================================


Starting training for 350 epochs...

Epoch 001 | train_loss=11.2019 val_loss=0.3994 P=0.002 R=0.092 F1=0.004 ROC-AUC=0.615 AUPR=0.001 time=55.84s
Epoch 002 | train_loss=0.5042 val_loss=0.3584 P=0.066 R=0.023 F1=0.034 ROC-AUC=0.702 AUPR=0.005 time=55.16s
Epoch 003 | train_loss=0.4067 val_loss=0.3526 P=0.044 R=0.136 F1=0.066 ROC-AUC=0.725 AUPR=0.014 time=55.20s
Epoch 004 | train_loss=0.3889 val_loss=0.3234 P=0.067 R=0.083 F1=0.074 ROC-AUC=0.754 AUPR=0.016 time=55.81s
Epoch 005 | train_loss=0.3547 val_loss=0.3242 P=0.075 R=0.096 F1=0.084 ROC-AUC=0.759 AUPR=0.019 time=55.36s
Epoch 006 | train_loss=0.3619 val_loss=0.2877 P=0.092 R=0.103 F1=0.097 ROC-AUC=0.794 AUPR=0.025 time=56.52s
Epoch 007 | train_loss=0.3136 val_loss=0.2694 P=0.080 R=0.137 F1=0.101 ROC-AUC=0.813 AUPR=0.026 time=55.84s
Epoch 008 | train_loss=0.3096 val_loss=0.2684 P=0.087 R=0.143 F1=0.108 ROC-AUC=0.846 AUPR=0.032 time=55.88s
Epoch 009 | train_loss=0.2969 val_loss=0.2435 P=0.093 R=0.180 F1=0.122 ROC-AUC=0.846 AUPR=0.041 time=55.47s
Epoch 010 | train_loss=0.3023 val_loss=0.2359 P=0.123 R=0.143 F1=0.132 ROC-AUC=0.859 AUPR=0.051 time=55.44s
Epoch 011 | train_loss=0.2590 val_loss=0.1721 P=0.257 R=0.204 F1=0.227 ROC-AUC=0.929 AUPR=0.124 time=55.41s
Epoch 012 | train_loss=0.2467 val_loss=0.1264 P=0.252 R=0.285 F1=0.267 ROC-AUC=0.972 AUPR=0.177 time=55.58s
Epoch 013 | train_loss=0.1738 val_loss=0.1140 P=0.225 R=0.365 F1=0.279 ROC-AUC=0.977 AUPR=0.196 time=55.36s
Epoch 014 | train_loss=0.1634 val_loss=0.1101 P=0.256 R=0.330 F1=0.288 ROC-AUC=0.978 AUPR=0.190 time=55.18s
Epoch 015 | train_loss=0.1440 val_loss=0.1046 P=0.313 R=0.322 F1=0.318 ROC-AUC=0.980 AUPR=0.229 time=54.83s
Epoch 016 | train_loss=0.1436 val_loss=0.1091 P=0.293 R=0.348 F1=0.318 ROC-AUC=0.979 AUPR=0.240 time=55.58s
Epoch 017 | train_loss=0.1258 val_loss=0.0994 P=0.319 R=0.340 F1=0.329 ROC-AUC=0.982 AUPR=0.251 time=56.33s
Epoch 018 | train_loss=0.1275 val_loss=0.1001 P=0.352 R=0.317 F1=0.333 ROC-AUC=0.981 AUPR=0.257 time=54.75s
Epoch 019 | train_loss=0.1386 val_loss=0.0968 P=0.314 R=0.361 F1=0.336 ROC-AUC=0.983 AUPR=0.256 time=55.36s
Epoch 020 | train_loss=0.1518 val_loss=0.0979 P=0.341 R=0.333 F1=0.337 ROC-AUC=0.982 AUPR=0.266 time=55.49s
Epoch 021 | train_loss=0.1593 val_loss=0.0962 P=0.366 R=0.324 F1=0.344 ROC-AUC=0.983 AUPR=0.265 time=88.56s
Epoch 022 | train_loss=0.1105 val_loss=0.0959 P=0.337 R=0.370 F1=0.353 ROC-AUC=0.983 AUPR=0.269 time=94.29s
Epoch 023 | train_loss=0.1295 val_loss=0.0965 P=0.359 R=0.349 F1=0.354 ROC-AUC=0.983 AUPR=0.276 time=59.71s
Epoch 024 | train_loss=0.1726 val_loss=0.0945 P=0.364 R=0.343 F1=0.353 ROC-AUC=0.983 AUPR=0.271 time=62.29s
Epoch 025 | train_loss=0.1411 val_loss=0.1899 P=0.389 R=0.328 F1=0.356 ROC-AUC=0.925 AUPR=0.282 time=57.20s
Epoch 026 | train_loss=0.1868 val_loss=0.1040 P=0.380 R=0.315 F1=0.344 ROC-AUC=0.980 AUPR=0.265 time=56.61s
Epoch 027 | train_loss=0.1395 val_loss=0.1219 P=0.349 R=0.315 F1=0.331 ROC-AUC=0.971 AUPR=0.261 time=56.02s
Epoch 028 | train_loss=0.1119 val_loss=0.1003 P=0.332 R=0.380 F1=0.354 ROC-AUC=0.981 AUPR=0.292 time=55.33s
Epoch 029 | train_loss=0.1759 val_loss=0.0982 P=0.374 R=0.351 F1=0.362 ROC-AUC=0.982 AUPR=0.300 time=56.33s
Epoch 030 | train_loss=0.1158 val_loss=0.1009 P=0.351 R=0.346 F1=0.348 ROC-AUC=0.981 AUPR=0.281 time=56.01s
Epoch 031 | train_loss=0.1245 val_loss=0.0982 P=0.406 R=0.318 F1=0.356 ROC-AUC=0.982 AUPR=0.289 time=56.27s
Epoch 032 | train_loss=0.1061 val_loss=0.0971 P=0.383 R=0.355 F1=0.368 ROC-AUC=0.982 AUPR=0.302 time=56.23s
Epoch 033 | train_loss=0.1290 val_loss=0.0976 P=0.365 R=0.389 F1=0.376 ROC-AUC=0.982 AUPR=0.313 time=56.00s
Epoch 034 | train_loss=0.1044 val_loss=0.0963 P=0.405 R=0.347 F1=0.374 ROC-AUC=0.983 AUPR=0.304 time=56.25s
Epoch 035 | train_loss=0.1625 val_loss=0.0947 P=0.384 R=0.387 F1=0.385 ROC-AUC=0.983 AUPR=0.316 time=55.20s
Epoch 036 | train_loss=0.1159 val_loss=0.0975 P=0.410 R=0.340 F1=0.372 ROC-AUC=0.982 AUPR=0.316 time=55.11s
Epoch 037 | train_loss=0.1460 val_loss=0.0952 P=0.406 R=0.343 F1=0.372 ROC-AUC=0.983 AUPR=0.311 time=55.75s
Epoch 038 | train_loss=0.1823 val_loss=0.0940 P=0.402 R=0.366 F1=0.383 ROC-AUC=0.983 AUPR=0.322 time=55.62s
Epoch 039 | train_loss=0.1007 val_loss=0.0930 P=0.411 R=0.358 F1=0.383 ROC-AUC=0.983 AUPR=0.326 time=56.76s
Epoch 040 | train_loss=0.1097 val_loss=0.0938 P=0.465 R=0.345 F1=0.396 ROC-AUC=0.983 AUPR=0.327 time=56.33s
Epoch 041 | train_loss=0.1224 val_loss=0.0941 P=0.429 R=0.364 F1=0.394 ROC-AUC=0.983 AUPR=0.333 time=55.90s
Epoch 042 | train_loss=0.1202 val_loss=0.0935 P=0.474 R=0.346 F1=0.400 ROC-AUC=0.983 AUPR=0.339 time=56.36s
Epoch 043 | train_loss=0.0931 val_loss=0.0937 P=0.483 R=0.335 F1=0.396 ROC-AUC=0.983 AUPR=0.340 time=55.42s
Epoch 044 | train_loss=0.1928 val_loss=0.0933 P=0.405 R=0.391 F1=0.398 ROC-AUC=0.984 AUPR=0.340 time=55.67s
Epoch 045 | train_loss=0.1024 val_loss=0.0952 P=0.452 R=0.356 F1=0.398 ROC-AUC=0.983 AUPR=0.342 time=55.84s
Epoch 046 | train_loss=0.1075 val_loss=0.0939 P=0.438 R=0.373 F1=0.403 ROC-AUC=0.984 AUPR=0.341 time=56.01s
Epoch 047 | train_loss=0.1046 val_loss=0.0934 P=0.467 R=0.368 F1=0.411 ROC-AUC=0.983 AUPR=0.352 time=55.83s
Epoch 048 | train_loss=0.1805 val_loss=0.0934 P=0.422 R=0.397 F1=0.409 ROC-AUC=0.983 AUPR=0.359 time=56.38s
Epoch 049 | train_loss=0.0993 val_loss=0.0920 P=0.469 R=0.356 F1=0.405 ROC-AUC=0.984 AUPR=0.350 time=55.91s
Epoch 050 | train_loss=0.0989 val_loss=0.4123 P=0.443 R=0.368 F1=0.402 ROC-AUC=0.918 AUPR=0.352 time=55.58s
Epoch 051 | train_loss=0.2245 val_loss=0.0956 P=0.482 R=0.344 F1=0.401 ROC-AUC=0.984 AUPR=0.349 time=57.30s
Epoch 052 | train_loss=0.1406 val_loss=0.0922 P=0.443 R=0.370 F1=0.403 ROC-AUC=0.984 AUPR=0.359 time=57.21s
Epoch 053 | train_loss=0.0940 val_loss=0.0951 P=0.528 R=0.317 F1=0.396 ROC-AUC=0.983 AUPR=0.353 time=56.83s
Epoch 054 | train_loss=0.1175 val_loss=0.0943 P=0.482 R=0.355 F1=0.409 ROC-AUC=0.984 AUPR=0.363 time=56.17s
Epoch 055 | train_loss=0.1472 val_loss=0.1009 P=0.418 R=0.381 F1=0.399 ROC-AUC=0.984 AUPR=0.354 time=55.50s
Epoch 056 | train_loss=0.1249 val_loss=0.0930 P=0.469 R=0.360 F1=0.407 ROC-AUC=0.984 AUPR=0.361 time=55.96s
Epoch 057 | train_loss=0.1580 val_loss=0.0920 P=0.466 R=0.373 F1=0.414 ROC-AUC=0.984 AUPR=0.368 time=55.54s
Epoch 058 | train_loss=0.1351 val_loss=0.0928 P=0.488 R=0.352 F1=0.409 ROC-AUC=0.983 AUPR=0.366 time=55.49s
Epoch 059 | train_loss=0.1350 val_loss=0.0920 P=0.474 R=0.365 F1=0.412 ROC-AUC=0.984 AUPR=0.368 time=54.76s
Epoch 060 | train_loss=0.1047 val_loss=0.0919 P=0.451 R=0.365 F1=0.403 ROC-AUC=0.984 AUPR=0.354 time=55.35s
Epoch 061 | train_loss=0.0991 val_loss=0.0950 P=0.407 R=0.427 F1=0.417 ROC-AUC=0.984 AUPR=0.372 time=55.76s
Epoch 062 | train_loss=0.1305 val_loss=0.0925 P=0.454 R=0.367 F1=0.406 ROC-AUC=0.984 AUPR=0.368 time=54.87s
Epoch 063 | train_loss=0.1619 val_loss=0.0924 P=0.494 R=0.358 F1=0.415 ROC-AUC=0.984 AUPR=0.372 time=55.93s
Epoch 064 | train_loss=0.1099 val_loss=0.0923 P=0.504 R=0.344 F1=0.409 ROC-AUC=0.984 AUPR=0.367 time=55.28s
Epoch 065 | train_loss=0.0994 val_loss=0.0916 P=0.480 R=0.367 F1=0.416 ROC-AUC=0.984 AUPR=0.374 time=55.06s
Epoch 066 | train_loss=0.2238 val_loss=0.0911 P=0.497 R=0.375 F1=0.427 ROC-AUC=0.984 AUPR=0.377 time=55.46s
Epoch 067 | train_loss=0.1357 val_loss=0.0911 P=0.467 R=0.369 F1=0.412 ROC-AUC=0.984 AUPR=0.382 time=54.99s
Epoch 068 | train_loss=0.1124 val_loss=0.0910 P=0.457 R=0.375 F1=0.412 ROC-AUC=0.984 AUPR=0.375 time=55.70s
Epoch 069 | train_loss=0.0907 val_loss=0.0927 P=0.425 R=0.399 F1=0.412 ROC-AUC=0.984 AUPR=0.375 time=55.89s
Epoch 070 | train_loss=0.0888 val_loss=0.0922 P=0.490 R=0.346 F1=0.405 ROC-AUC=0.984 AUPR=0.362 time=55.03s
Epoch 071 | train_loss=0.1329 val_loss=0.0912 P=0.491 R=0.359 F1=0.415 ROC-AUC=0.984 AUPR=0.375 time=54.93s
Epoch 072 | train_loss=0.0964 val_loss=0.0917 P=0.465 R=0.362 F1=0.407 ROC-AUC=0.984 AUPR=0.373 time=54.62s
Epoch 073 | train_loss=0.1245 val_loss=0.0917 P=0.455 R=0.386 F1=0.418 ROC-AUC=0.984 AUPR=0.385 time=55.10s
Epoch 074 | train_loss=0.1183 val_loss=0.0903 P=0.458 R=0.399 F1=0.426 ROC-AUC=0.984 AUPR=0.387 time=55.39s
Epoch 075 | train_loss=0.1400 val_loss=0.0915 P=0.487 R=0.366 F1=0.418 ROC-AUC=0.984 AUPR=0.388 time=55.24s
Epoch 076 | train_loss=0.1265 val_loss=0.1236 P=0.461 R=0.349 F1=0.397 ROC-AUC=0.972 AUPR=0.352 time=55.04s
Epoch 077 | train_loss=0.1415 val_loss=0.0919 P=0.476 R=0.364 F1=0.412 ROC-AUC=0.984 AUPR=0.373 time=55.44s
Epoch 078 | train_loss=0.1074 val_loss=0.0935 P=0.520 R=0.344 F1=0.414 ROC-AUC=0.983 AUPR=0.379 time=54.97s
Epoch 079 | train_loss=0.1043 val_loss=0.0958 P=0.499 R=0.332 F1=0.399 ROC-AUC=0.983 AUPR=0.368 time=54.49s
Epoch 080 | train_loss=0.1470 val_loss=0.0912 P=0.426 R=0.419 F1=0.423 ROC-AUC=0.984 AUPR=0.388 time=54.83s
Epoch 081 | train_loss=0.1077 val_loss=0.0902 P=0.479 R=0.403 F1=0.438 ROC-AUC=0.984 AUPR=0.399 time=55.28s
Epoch 082 | train_loss=0.1281 val_loss=0.0914 P=0.440 R=0.415 F1=0.427 ROC-AUC=0.984 AUPR=0.391 time=55.16s
Epoch 083 | train_loss=0.1113 val_loss=0.0927 P=0.474 R=0.386 F1=0.426 ROC-AUC=0.984 AUPR=0.378 time=54.46s
Epoch 084 | train_loss=0.0984 val_loss=0.0894 P=0.471 R=0.374 F1=0.417 ROC-AUC=0.985 AUPR=0.381 time=54.90s
Epoch 085 | train_loss=0.1086 val_loss=0.0911 P=0.447 R=0.389 F1=0.416 ROC-AUC=0.984 AUPR=0.378 time=54.29s
Epoch 086 | train_loss=0.1395 val_loss=0.0933 P=0.458 R=0.356 F1=0.401 ROC-AUC=0.984 AUPR=0.369 time=55.10s
Epoch 087 | train_loss=0.0970 val_loss=0.0910 P=0.466 R=0.398 F1=0.429 ROC-AUC=0.984 AUPR=0.399 time=54.68s
Epoch 088 | train_loss=0.1390 val_loss=0.0907 P=0.544 R=0.356 F1=0.431 ROC-AUC=0.984 AUPR=0.396 time=54.53s
Epoch 089 | train_loss=0.1168 val_loss=0.0929 P=0.441 R=0.400 F1=0.419 ROC-AUC=0.984 AUPR=0.386 time=54.10s
Epoch 090 | train_loss=0.1534 val_loss=0.0904 P=0.524 R=0.358 F1=0.425 ROC-AUC=0.984 AUPR=0.395 time=54.45s
Epoch 091 | train_loss=0.1372 val_loss=0.1099 P=0.464 R=0.359 F1=0.405 ROC-AUC=0.982 AUPR=0.373 time=55.04s
Epoch 092 | train_loss=0.1057 val_loss=0.0896 P=0.481 R=0.387 F1=0.429 ROC-AUC=0.984 AUPR=0.398 time=54.84s
Epoch 093 | train_loss=0.1418 val_loss=0.0917 P=0.517 R=0.340 F1=0.410 ROC-AUC=0.984 AUPR=0.375 time=54.68s
Epoch 094 | train_loss=0.1553 val_loss=0.1516 P=0.523 R=0.338 F1=0.411 ROC-AUC=0.962 AUPR=0.369 time=54.24s
Epoch 095 | train_loss=0.1331 val_loss=0.0879 P=0.524 R=0.395 F1=0.450 ROC-AUC=0.985 AUPR=0.417 time=54.81s
Epoch 096 | train_loss=0.0937 val_loss=0.1105 P=0.463 R=0.390 F1=0.423 ROC-AUC=0.978 AUPR=0.378 time=54.39s
Epoch 097 | train_loss=0.1135 val_loss=0.0878 P=0.494 R=0.431 F1=0.461 ROC-AUC=0.985 AUPR=0.428 time=55.00s
Epoch 098 | train_loss=0.1135 val_loss=0.0871 P=0.535 R=0.400 F1=0.457 ROC-AUC=0.985 AUPR=0.429 time=55.36s
Epoch 099 | train_loss=0.1062 val_loss=0.0879 P=0.550 R=0.389 F1=0.456 ROC-AUC=0.985 AUPR=0.431 time=54.63s
Epoch 100 | train_loss=0.1169 val_loss=0.0893 P=0.551 R=0.423 F1=0.478 ROC-AUC=0.985 AUPR=0.450 time=54.64s
Epoch 101 | train_loss=0.1131 val_loss=0.0885 P=0.485 R=0.427 F1=0.454 ROC-AUC=0.985 AUPR=0.426 time=55.71s
Epoch 102 | train_loss=0.1328 val_loss=0.0867 P=0.505 R=0.400 F1=0.446 ROC-AUC=0.985 AUPR=0.428 time=54.93s
Epoch 103 | train_loss=0.0997 val_loss=0.0876 P=0.493 R=0.415 F1=0.450 ROC-AUC=0.985 AUPR=0.425 time=54.83s
Epoch 104 | train_loss=0.0931 val_loss=0.0898 P=0.487 R=0.440 F1=0.462 ROC-AUC=0.985 AUPR=0.429 time=55.02s
Epoch 105 | train_loss=0.1093 val_loss=0.0901 P=0.565 R=0.385 F1=0.458 ROC-AUC=0.985 AUPR=0.430 time=54.69s
Epoch 106 | train_loss=0.0888 val_loss=0.0874 P=0.514 R=0.430 F1=0.468 ROC-AUC=0.985 AUPR=0.441 time=55.19s
Epoch 107 | train_loss=0.1025 val_loss=0.0880 P=0.501 R=0.417 F1=0.455 ROC-AUC=0.985 AUPR=0.432 time=54.80s
Epoch 108 | train_loss=0.1057 val_loss=0.1104 P=0.521 R=0.402 F1=0.453 ROC-AUC=0.979 AUPR=0.426 time=55.00s
Epoch 109 | train_loss=0.0993 val_loss=0.0857 P=0.619 R=0.380 F1=0.471 ROC-AUC=0.985 AUPR=0.455 time=54.99s
Epoch 110 | train_loss=0.1012 val_loss=0.0865 P=0.519 R=0.407 F1=0.456 ROC-AUC=0.985 AUPR=0.427 time=54.82s
Epoch 111 | train_loss=0.1000 val_loss=0.0897 P=0.518 R=0.416 F1=0.461 ROC-AUC=0.985 AUPR=0.430 time=55.60s
Epoch 112 | train_loss=0.0886 val_loss=0.0868 P=0.565 R=0.403 F1=0.471 ROC-AUC=0.985 AUPR=0.444 time=55.52s
Epoch 113 | train_loss=0.1083 val_loss=0.0878 P=0.555 R=0.427 F1=0.482 ROC-AUC=0.985 AUPR=0.453 time=55.01s
Epoch 114 | train_loss=0.1004 val_loss=0.0883 P=0.597 R=0.404 F1=0.482 ROC-AUC=0.985 AUPR=0.454 time=54.86s
Epoch 115 | train_loss=0.0978 val_loss=0.0876 P=0.640 R=0.375 F1=0.473 ROC-AUC=0.985 AUPR=0.454 time=55.35s
Epoch 116 | train_loss=0.0873 val_loss=0.0862 P=0.555 R=0.430 F1=0.484 ROC-AUC=0.985 AUPR=0.455 time=55.76s
Epoch 117 | train_loss=0.1382 val_loss=0.0876 P=0.511 R=0.455 F1=0.481 ROC-AUC=0.985 AUPR=0.452 time=55.88s
Epoch 118 | train_loss=0.0951 val_loss=0.0861 P=0.575 R=0.412 F1=0.480 ROC-AUC=0.985 AUPR=0.460 time=55.18s
Epoch 119 | train_loss=0.1097 val_loss=0.0862 P=0.603 R=0.392 F1=0.475 ROC-AUC=0.985 AUPR=0.452 time=54.92s
Epoch 120 | train_loss=0.0818 val_loss=0.0858 P=0.632 R=0.380 F1=0.475 ROC-AUC=0.985 AUPR=0.461 time=54.81s
Epoch 121 | train_loss=0.1096 val_loss=0.0857 P=0.528 R=0.443 F1=0.482 ROC-AUC=0.985 AUPR=0.460 time=55.63s
Epoch 122 | train_loss=0.0934 val_loss=0.0879 P=0.575 R=0.431 F1=0.492 ROC-AUC=0.985 AUPR=0.461 time=56.00s
Epoch 123 | train_loss=0.1089 val_loss=0.0879 P=0.653 R=0.366 F1=0.469 ROC-AUC=0.985 AUPR=0.447 time=55.72s
Epoch 124 | train_loss=0.1484 val_loss=0.0863 P=0.564 R=0.398 F1=0.467 ROC-AUC=0.985 AUPR=0.441 time=55.18s
Epoch 125 | train_loss=0.1191 val_loss=0.0867 P=0.567 R=0.404 F1=0.472 ROC-AUC=0.985 AUPR=0.446 time=54.54s
Epoch 126 | train_loss=0.1054 val_loss=0.0861 P=0.572 R=0.424 F1=0.487 ROC-AUC=0.985 AUPR=0.466 time=55.81s
Epoch 127 | train_loss=0.0875 val_loss=0.0861 P=0.519 R=0.430 F1=0.470 ROC-AUC=0.985 AUPR=0.455 time=55.59s
Epoch 128 | train_loss=0.1067 val_loss=0.0877 P=0.588 R=0.402 F1=0.477 ROC-AUC=0.985 AUPR=0.446 time=55.77s
Epoch 129 | train_loss=0.1439 val_loss=0.0841 P=0.561 R=0.423 F1=0.482 ROC-AUC=0.986 AUPR=0.456 time=55.25s
Epoch 130 | train_loss=0.0979 val_loss=0.0917 P=0.515 R=0.420 F1=0.463 ROC-AUC=0.985 AUPR=0.448 time=55.48s
Epoch 131 | train_loss=0.1239 val_loss=0.0916 P=0.562 R=0.437 F1=0.492 ROC-AUC=0.984 AUPR=0.461 time=55.03s
Epoch 132 | train_loss=0.0971 val_loss=0.0866 P=0.620 R=0.410 F1=0.494 ROC-AUC=0.985 AUPR=0.467 time=54.84s
Epoch 133 | train_loss=0.1305 val_loss=0.0905 P=0.574 R=0.428 F1=0.490 ROC-AUC=0.985 AUPR=0.460 time=55.30s
Epoch 134 | train_loss=0.1254 val_loss=0.0873 P=0.630 R=0.370 F1=0.466 ROC-AUC=0.985 AUPR=0.440 time=55.52s
Epoch 135 | train_loss=0.1071 val_loss=0.0884 P=0.659 R=0.397 F1=0.495 ROC-AUC=0.985 AUPR=0.469 time=55.51s
Epoch 136 | train_loss=0.0837 val_loss=0.0864 P=0.599 R=0.387 F1=0.470 ROC-AUC=0.986 AUPR=0.454 time=55.38s
Epoch 137 | train_loss=0.0832 val_loss=0.0883 P=0.611 R=0.414 F1=0.494 ROC-AUC=0.985 AUPR=0.454 time=55.00s
Epoch 138 | train_loss=0.0983 val_loss=0.0889 P=0.596 R=0.417 F1=0.491 ROC-AUC=0.986 AUPR=0.462 time=56.39s
Epoch 139 | train_loss=0.1123 val_loss=0.0881 P=0.643 R=0.383 F1=0.480 ROC-AUC=0.985 AUPR=0.458 time=55.99s
Epoch 140 | train_loss=0.0826 val_loss=0.0862 P=0.632 R=0.410 F1=0.498 ROC-AUC=0.985 AUPR=0.470 time=55.83s
Epoch 141 | train_loss=0.1039 val_loss=0.0862 P=0.578 R=0.420 F1=0.487 ROC-AUC=0.986 AUPR=0.460 time=55.63s
Epoch 142 | train_loss=0.0926 val_loss=0.0871 P=0.627 R=0.385 F1=0.477 ROC-AUC=0.985 AUPR=0.453 time=55.44s
Epoch 143 | train_loss=0.1206 val_loss=0.0892 P=0.583 R=0.411 F1=0.482 ROC-AUC=0.986 AUPR=0.452 time=55.57s
Epoch 144 | train_loss=0.0986 val_loss=0.0848 P=0.567 R=0.431 F1=0.490 ROC-AUC=0.986 AUPR=0.464 time=55.44s
Epoch 145 | train_loss=0.0888 val_loss=0.0851 P=0.573 R=0.427 F1=0.489 ROC-AUC=0.986 AUPR=0.458 time=55.48s
Epoch 146 | train_loss=0.1043 val_loss=0.0890 P=0.527 R=0.438 F1=0.478 ROC-AUC=0.985 AUPR=0.452 time=55.20s
Epoch 147 | train_loss=0.0885 val_loss=0.0901 P=0.579 R=0.394 F1=0.469 ROC-AUC=0.985 AUPR=0.443 time=55.09s
Epoch 148 | train_loss=0.1000 val_loss=0.0879 P=0.559 R=0.426 F1=0.483 ROC-AUC=0.986 AUPR=0.454 time=55.55s
Epoch 149 | train_loss=0.1060 val_loss=0.4624 P=0.495 R=0.433 F1=0.462 ROC-AUC=0.892 AUPR=0.419 time=55.60s
Epoch 150 | train_loss=0.1107 val_loss=0.0858 P=0.582 R=0.413 F1=0.483 ROC-AUC=0.986 AUPR=0.459 time=56.03s
Epoch 151 | train_loss=0.1212 val_loss=0.2510 P=0.585 R=0.391 F1=0.469 ROC-AUC=0.931 AUPR=0.435 time=56.45s
Epoch 152 | train_loss=0.1212 val_loss=0.0872 P=0.557 R=0.453 F1=0.499 ROC-AUC=0.985 AUPR=0.469 time=55.18s
Epoch 153 | train_loss=0.0814 val_loss=0.0884 P=0.527 R=0.437 F1=0.478 ROC-AUC=0.985 AUPR=0.446 time=55.56s
Epoch 154 | train_loss=0.0999 val_loss=0.0897 P=0.546 R=0.423 F1=0.477 ROC-AUC=0.985 AUPR=0.448 time=56.43s
Epoch 155 | train_loss=0.1402 val_loss=0.0866 P=0.564 R=0.423 F1=0.483 ROC-AUC=0.986 AUPR=0.454 time=56.29s
Epoch 156 | train_loss=0.0866 val_loss=0.1186 P=0.539 R=0.415 F1=0.469 ROC-AUC=0.981 AUPR=0.434 time=56.21s
Epoch 157 | train_loss=0.1003 val_loss=0.0883 P=0.602 R=0.410 F1=0.488 ROC-AUC=0.985 AUPR=0.463 time=58.62s
Epoch 158 | train_loss=0.1406 val_loss=0.0925 P=0.515 R=0.441 F1=0.475 ROC-AUC=0.985 AUPR=0.444 time=57.77s
Epoch 159 | train_loss=0.1096 val_loss=0.0849 P=0.594 R=0.415 F1=0.489 ROC-AUC=0.986 AUPR=0.472 time=56.81s
Epoch 160 | train_loss=0.1487 val_loss=0.0861 P=0.517 R=0.448 F1=0.480 ROC-AUC=0.985 AUPR=0.465 time=56.68s
Epoch 161 | train_loss=0.1016 val_loss=0.0857 P=0.548 R=0.422 F1=0.477 ROC-AUC=0.986 AUPR=0.461 time=55.26s
Epoch 162 | train_loss=0.1259 val_loss=0.0861 P=0.566 R=0.418 F1=0.481 ROC-AUC=0.986 AUPR=0.466 time=56.07s
Epoch 163 | train_loss=0.0863 val_loss=0.0855 P=0.584 R=0.447 F1=0.506 ROC-AUC=0.986 AUPR=0.480 time=55.40s
Epoch 164 | train_loss=0.1043 val_loss=0.0898 P=0.614 R=0.404 F1=0.488 ROC-AUC=0.986 AUPR=0.465 time=55.75s
Epoch 165 | train_loss=0.1069 val_loss=0.1032 P=0.545 R=0.410 F1=0.468 ROC-AUC=0.980 AUPR=0.444 time=55.93s
Epoch 166 | train_loss=0.1598 val_loss=0.0885 P=0.653 R=0.387 F1=0.486 ROC-AUC=0.985 AUPR=0.465 time=55.96s
Epoch 167 | train_loss=0.1168 val_loss=0.0893 P=0.639 R=0.404 F1=0.495 ROC-AUC=0.985 AUPR=0.466 time=56.12s
Epoch 168 | train_loss=0.1228 val_loss=0.0856 P=0.620 R=0.396 F1=0.483 ROC-AUC=0.986 AUPR=0.462 time=55.07s
Epoch 169 | train_loss=0.1661 val_loss=0.0836 P=0.616 R=0.414 F1=0.495 ROC-AUC=0.986 AUPR=0.468 time=56.02s
Epoch 170 | train_loss=0.1008 val_loss=0.0912 P=0.564 R=0.419 F1=0.481 ROC-AUC=0.985 AUPR=0.454 time=55.46s
Epoch 171 | train_loss=0.1248 val_loss=0.0908 P=0.510 R=0.465 F1=0.486 ROC-AUC=0.985 AUPR=0.466 time=56.30s
Epoch 172 | train_loss=0.0905 val_loss=0.0867 P=0.520 R=0.435 F1=0.474 ROC-AUC=0.986 AUPR=0.468 time=55.68s
Epoch 173 | train_loss=0.1536 val_loss=0.1262 P=0.576 R=0.423 F1=0.488 ROC-AUC=0.978 AUPR=0.451 time=55.59s
Epoch 174 | train_loss=0.0954 val_loss=0.0865 P=0.623 R=0.422 F1=0.503 ROC-AUC=0.985 AUPR=0.474 time=55.12s
Epoch 175 | train_loss=0.1021 val_loss=0.0862 P=0.530 R=0.465 F1=0.495 ROC-AUC=0.986 AUPR=0.474 time=55.61s
Epoch 176 | train_loss=0.1322 val_loss=0.0866 P=0.609 R=0.425 F1=0.500 ROC-AUC=0.986 AUPR=0.478 time=54.80s
Epoch 177 | train_loss=0.1207 val_loss=0.0914 P=0.509 R=0.459 F1=0.483 ROC-AUC=0.984 AUPR=0.459 time=55.86s
Epoch 178 | train_loss=0.1471 val_loss=0.0886 P=0.587 R=0.418 F1=0.488 ROC-AUC=0.985 AUPR=0.461 time=55.58s
Epoch 179 | train_loss=0.0926 val_loss=0.0847 P=0.579 R=0.436 F1=0.498 ROC-AUC=0.986 AUPR=0.464 time=56.11s
Epoch 180 | train_loss=0.1064 val_loss=0.0850 P=0.594 R=0.429 F1=0.498 ROC-AUC=0.986 AUPR=0.479 time=56.76s
Epoch 181 | train_loss=0.1115 val_loss=0.0905 P=0.528 R=0.448 F1=0.485 ROC-AUC=0.985 AUPR=0.459 time=55.59s
Epoch 182 | train_loss=0.1192 val_loss=0.0853 P=0.550 R=0.430 F1=0.482 ROC-AUC=0.986 AUPR=0.462 time=55.89s
Epoch 183 | train_loss=0.0976 val_loss=0.0875 P=0.591 R=0.428 F1=0.496 ROC-AUC=0.985 AUPR=0.472 time=55.56s
Epoch 184 | train_loss=0.2115 val_loss=0.0852 P=0.573 R=0.428 F1=0.490 ROC-AUC=0.986 AUPR=0.473 time=57.24s
Epoch 185 | train_loss=0.0988 val_loss=0.0858 P=0.625 R=0.417 F1=0.500 ROC-AUC=0.986 AUPR=0.476 time=56.41s
Epoch 186 | train_loss=0.2496 val_loss=0.0883 P=0.565 R=0.435 F1=0.492 ROC-AUC=0.985 AUPR=0.461 time=56.45s
Epoch 187 | train_loss=0.1157 val_loss=0.0878 P=0.638 R=0.384 F1=0.480 ROC-AUC=0.986 AUPR=0.462 time=56.05s
Epoch 188 | train_loss=0.1652 val_loss=0.0860 P=0.509 R=0.458 F1=0.482 ROC-AUC=0.986 AUPR=0.459 time=66.59s

Early stopping at epoch 188

Total training time: 10538.00s (175.6 min)

Loading best model from epoch 163...
Evaluating final model...
======================================================================
EVALUATION RESULTS: seed4_seed5_experiment TRAIN
======================================================================

STANDARD METRICS:
  Precision:        0.6208
  Recall:           0.4549
  F1-Score:         0.5251
  ROC-AUC:          0.9931
  AUPR:             0.5309
  Balanced Acc:     0.7273

IMBALANCED-AWARE METRICS:
  MCC:              0.5310
  Cohen Kappa:      0.5247
  Specificity:      0.9997

THRESHOLD: 0.970

CONFUSION MATRIX:
  True Negatives:   3043080
  False Positives:  863
  False Negatives:  1693
  True Positives:   1413

TOP-K PRECISION:
  precision_at_100: 1.0000
  precision_at_500: 0.9420
  precision_at_1000: 0.8700
======================================================================
======================================================================
EVALUATION RESULTS: seed4_seed5_experiment VAL
======================================================================

STANDARD METRICS:
  Precision:        0.5839
  Recall:           0.4469
  F1-Score:         0.5063
  ROC-AUC:          0.9857
  AUPR:             0.4798
  Balanced Acc:     0.7233

IMBALANCED-AWARE METRICS:
  MCC:              0.5104
  Cohen Kappa:      0.5059
  Specificity:      0.9997

THRESHOLD: 0.970

CONFUSION MATRIX:
  True Negatives:   1014317
  False Positives:  330
  False Negatives:  573
  True Positives:   463

TOP-K PRECISION:
  precision_at_100: 0.9700
  precision_at_500: 0.7260
  precision_at_1000: 0.4940
======================================================================
======================================================================
EVALUATION RESULTS: seed4_seed5_experiment TEST
======================================================================

STANDARD METRICS:
  Precision:        0.5752
  Recall:           0.4174
  F1-Score:         0.4838
  ROC-AUC:          0.9845
  AUPR:             0.4712
  Balanced Acc:     0.7085

IMBALANCED-AWARE METRICS:
  MCC:              0.4896
  Cohen Kappa:      0.4833
  Specificity:      0.9997

THRESHOLD: 0.970

CONFUSION MATRIX:
  True Negatives:   1014329
  False Positives:  319
  False Negatives:  603
  True Positives:   432

TOP-K PRECISION:
  precision_at_100: 0.9600
  precision_at_500: 0.7300
  precision_at_1000: 0.4890
======================================================================

Saving results...
 Saved metrics to: C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\results\HI-Small_Trans_RAT_medium\seed4_seed5_experiment\graphsage-t\metrics.json
 Saved prediction probabilities
 Saved experiment config

======================================================================
GraphSAGE-T Training Complete!
Results saved to: C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\results\HI-Small_Trans_RAT_medium\seed4_seed5_experiment\graphsage-t
======================================================================

================================================================================
Logging ended at: 2026-02-25 20:12:19.632837
================================================================================
