
================================================================================
Logging started at: 2025-11-28 03:24:23
Log file: C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\logs\HI-Small_Trans_RAT_low\graphsage_t\RAT_low_GraphSAGE_T_20251128_032423.txt
================================================================================

[INFO] Logging to: C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\logs\HI-Small_Trans_RAT_low\graphsage_t\RAT_low_GraphSAGE_T_20251128_032423.txt
================================================================================
EXPERIMENT CONFIGURATION
================================================================================

[DATASET]
  Theory:       RAT
  Intensity:    low
  Full name:    HI-Small_Trans_RAT_low

[MODEL] GraphSAGE-T
  Hidden dim:   128
  Num layers:   2
  Dropout:      0.2

[TRAINING]
  Device:       cuda
  Batch size:   8192
  Learning rate: 0.0005
  Epochs:       350
  Early stop:   25 epochs

[PATHS]
  Graphs:       C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\graphs\HI-Small_Trans_RAT_low
  Splits:       C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\splits\HI-Small_Trans_RAT_low
  Results:      C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\results\HI-Small_Trans_RAT_low\graphsage_t
  Logs:         C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\logs\HI-Small_Trans_RAT_low\graphsage_t

[EXPERIMENT]
  Seed:         42
  Exp name:     default_experiment

================================================================================

[DEVICE] cuda


======= GRAPHSAGE-T TRAINING (FP32) =======
Batch size:       8192
Eval batch size:  16384
Device:           cuda
===========================================


Starting training for 350 epochs...

Epoch 001 | train_loss=18.0829 val_loss=0.4107 P=0.023 R=0.023 F1=0.023 ROC-AUC=0.589 AUPR=0.002 time=49.40s
Epoch 002 | train_loss=0.5433 val_loss=0.4404 P=0.029 R=0.042 F1=0.035 ROC-AUC=0.706 AUPR=0.004 time=50.26s
Epoch 003 | train_loss=0.4544 val_loss=0.3408 P=0.060 R=0.084 F1=0.070 ROC-AUC=0.771 AUPR=0.017 time=50.34s
Epoch 004 | train_loss=0.3670 val_loss=0.2991 P=0.044 R=0.128 F1=0.065 ROC-AUC=0.804 AUPR=0.014 time=50.63s
Epoch 005 | train_loss=0.3681 val_loss=0.2905 P=0.072 R=0.110 F1=0.087 ROC-AUC=0.798 AUPR=0.022 time=50.52s
Epoch 006 | train_loss=0.3479 val_loss=0.2985 P=0.062 R=0.136 F1=0.085 ROC-AUC=0.798 AUPR=0.024 time=49.84s
Epoch 007 | train_loss=0.3219 val_loss=0.2719 P=0.100 R=0.087 F1=0.093 ROC-AUC=0.819 AUPR=0.026 time=49.97s
Epoch 008 | train_loss=0.3463 val_loss=0.2578 P=0.080 R=0.151 F1=0.104 ROC-AUC=0.831 AUPR=0.030 time=50.58s
Epoch 009 | train_loss=0.2988 val_loss=0.3890 P=0.090 R=0.152 F1=0.113 ROC-AUC=0.834 AUPR=0.034 time=49.17s
Epoch 010 | train_loss=0.3172 val_loss=0.1910 P=0.177 R=0.230 F1=0.200 ROC-AUC=0.920 AUPR=0.112 time=49.72s
Epoch 011 | train_loss=0.2689 val_loss=0.1318 P=0.199 R=0.296 F1=0.238 ROC-AUC=0.971 AUPR=0.151 time=50.26s
Epoch 012 | train_loss=0.2107 val_loss=0.1757 P=0.266 R=0.228 F1=0.245 ROC-AUC=0.918 AUPR=0.163 time=49.86s
Epoch 013 | train_loss=0.1861 val_loss=0.1202 P=0.218 R=0.333 F1=0.264 ROC-AUC=0.975 AUPR=0.162 time=49.75s
Epoch 014 | train_loss=0.1646 val_loss=0.1458 P=0.250 R=0.339 F1=0.287 ROC-AUC=0.943 AUPR=0.192 time=50.00s
Epoch 015 | train_loss=0.1689 val_loss=0.1107 P=0.276 R=0.314 F1=0.294 ROC-AUC=0.979 AUPR=0.201 time=48.88s
Epoch 016 | train_loss=0.1907 val_loss=0.1048 P=0.310 R=0.319 F1=0.315 ROC-AUC=0.981 AUPR=0.229 time=48.93s
Epoch 017 | train_loss=0.1610 val_loss=0.5028 P=0.307 R=0.316 F1=0.311 ROC-AUC=0.891 AUPR=0.227 time=48.97s
Epoch 018 | train_loss=0.1672 val_loss=0.1126 P=0.294 R=0.332 F1=0.312 ROC-AUC=0.975 AUPR=0.232 time=48.98s
Epoch 019 | train_loss=0.1532 val_loss=0.1077 P=0.288 R=0.309 F1=0.298 ROC-AUC=0.978 AUPR=0.207 time=49.17s
Epoch 020 | train_loss=0.1297 val_loss=0.1066 P=0.320 R=0.329 F1=0.325 ROC-AUC=0.979 AUPR=0.249 time=49.90s
Epoch 021 | train_loss=0.1751 val_loss=0.1047 P=0.329 R=0.343 F1=0.336 ROC-AUC=0.980 AUPR=0.259 time=49.18s
Epoch 022 | train_loss=0.1245 val_loss=0.1025 P=0.349 R=0.321 F1=0.335 ROC-AUC=0.980 AUPR=0.252 time=49.31s
Epoch 023 | train_loss=0.1113 val_loss=0.1010 P=0.328 R=0.372 F1=0.349 ROC-AUC=0.981 AUPR=0.264 time=49.07s
Epoch 024 | train_loss=0.1647 val_loss=0.0996 P=0.368 R=0.314 F1=0.339 ROC-AUC=0.982 AUPR=0.259 time=49.03s
Epoch 025 | train_loss=0.1113 val_loss=0.1056 P=0.334 R=0.334 F1=0.334 ROC-AUC=0.981 AUPR=0.252 time=48.99s
Epoch 026 | train_loss=0.1108 val_loss=0.0982 P=0.330 R=0.363 F1=0.346 ROC-AUC=0.982 AUPR=0.272 time=48.90s
Epoch 027 | train_loss=0.0992 val_loss=0.0973 P=0.347 R=0.355 F1=0.351 ROC-AUC=0.982 AUPR=0.279 time=48.98s
Epoch 028 | train_loss=0.1063 val_loss=0.0968 P=0.331 R=0.400 F1=0.362 ROC-AUC=0.983 AUPR=0.292 time=48.77s
Epoch 029 | train_loss=0.1331 val_loss=0.0966 P=0.370 R=0.356 F1=0.363 ROC-AUC=0.983 AUPR=0.294 time=49.31s
Epoch 030 | train_loss=0.1075 val_loss=0.0999 P=0.357 R=0.394 F1=0.375 ROC-AUC=0.982 AUPR=0.306 time=49.03s
Epoch 031 | train_loss=0.0986 val_loss=0.0941 P=0.380 R=0.369 F1=0.375 ROC-AUC=0.983 AUPR=0.315 time=49.03s
Epoch 032 | train_loss=0.1113 val_loss=0.0953 P=0.405 R=0.345 F1=0.372 ROC-AUC=0.984 AUPR=0.311 time=49.10s
Epoch 033 | train_loss=0.1206 val_loss=0.0971 P=0.364 R=0.406 F1=0.384 ROC-AUC=0.983 AUPR=0.319 time=48.77s
Epoch 034 | train_loss=0.0948 val_loss=0.0929 P=0.418 R=0.351 F1=0.382 ROC-AUC=0.984 AUPR=0.329 time=48.72s
Epoch 035 | train_loss=0.1600 val_loss=0.0956 P=0.387 R=0.374 F1=0.380 ROC-AUC=0.983 AUPR=0.311 time=48.87s
Epoch 036 | train_loss=0.1419 val_loss=0.0927 P=0.406 R=0.351 F1=0.377 ROC-AUC=0.984 AUPR=0.317 time=49.00s
Epoch 037 | train_loss=0.1008 val_loss=0.0933 P=0.394 R=0.379 F1=0.387 ROC-AUC=0.983 AUPR=0.329 time=48.84s
Epoch 038 | train_loss=0.1329 val_loss=0.0943 P=0.417 R=0.367 F1=0.390 ROC-AUC=0.983 AUPR=0.326 time=48.88s
Epoch 039 | train_loss=0.1519 val_loss=0.0917 P=0.434 R=0.374 F1=0.402 ROC-AUC=0.984 AUPR=0.345 time=48.91s
Epoch 040 | train_loss=0.0997 val_loss=0.0929 P=0.440 R=0.347 F1=0.388 ROC-AUC=0.984 AUPR=0.334 time=48.88s
Epoch 041 | train_loss=0.1193 val_loss=0.0911 P=0.463 R=0.364 F1=0.407 ROC-AUC=0.984 AUPR=0.358 time=48.92s
Epoch 042 | train_loss=0.0940 val_loss=0.0935 P=0.416 R=0.391 F1=0.403 ROC-AUC=0.984 AUPR=0.351 time=48.96s
Epoch 043 | train_loss=0.0996 val_loss=0.0925 P=0.419 R=0.391 F1=0.404 ROC-AUC=0.984 AUPR=0.350 time=48.88s
Epoch 044 | train_loss=0.1494 val_loss=0.0926 P=0.432 R=0.365 F1=0.395 ROC-AUC=0.984 AUPR=0.341 time=49.17s
Epoch 045 | train_loss=0.0937 val_loss=0.0912 P=0.435 R=0.382 F1=0.407 ROC-AUC=0.984 AUPR=0.351 time=49.01s
Epoch 046 | train_loss=0.2647 val_loss=0.0913 P=0.447 R=0.364 F1=0.401 ROC-AUC=0.984 AUPR=0.357 time=49.07s
Epoch 047 | train_loss=0.1092 val_loss=0.0920 P=0.468 R=0.354 F1=0.403 ROC-AUC=0.984 AUPR=0.354 time=48.97s
Epoch 048 | train_loss=0.1072 val_loss=0.0907 P=0.401 R=0.395 F1=0.398 ROC-AUC=0.985 AUPR=0.355 time=48.93s
Epoch 049 | train_loss=0.0931 val_loss=0.0912 P=0.423 R=0.396 F1=0.409 ROC-AUC=0.984 AUPR=0.361 time=48.87s
Epoch 050 | train_loss=0.1578 val_loss=0.0922 P=0.488 R=0.353 F1=0.410 ROC-AUC=0.983 AUPR=0.366 time=48.95s
Epoch 051 | train_loss=0.1180 val_loss=0.0908 P=0.510 R=0.334 F1=0.404 ROC-AUC=0.984 AUPR=0.366 time=48.84s
Epoch 052 | train_loss=0.0918 val_loss=0.0904 P=0.424 R=0.399 F1=0.411 ROC-AUC=0.985 AUPR=0.366 time=48.93s
Epoch 053 | train_loss=0.1007 val_loss=0.0909 P=0.466 R=0.353 F1=0.402 ROC-AUC=0.984 AUPR=0.357 time=49.03s
Epoch 054 | train_loss=0.1153 val_loss=0.0926 P=0.400 R=0.397 F1=0.398 ROC-AUC=0.984 AUPR=0.353 time=48.98s
Epoch 055 | train_loss=0.1264 val_loss=0.0903 P=0.474 R=0.368 F1=0.414 ROC-AUC=0.984 AUPR=0.370 time=49.15s
Epoch 056 | train_loss=0.2257 val_loss=0.0909 P=0.463 R=0.370 F1=0.411 ROC-AUC=0.984 AUPR=0.367 time=49.14s
Epoch 057 | train_loss=0.1001 val_loss=0.0907 P=0.437 R=0.375 F1=0.404 ROC-AUC=0.984 AUPR=0.362 time=48.73s
Epoch 058 | train_loss=0.1134 val_loss=0.0901 P=0.449 R=0.403 F1=0.424 ROC-AUC=0.984 AUPR=0.385 time=48.84s
Epoch 059 | train_loss=0.0974 val_loss=0.0903 P=0.481 R=0.375 F1=0.422 ROC-AUC=0.984 AUPR=0.387 time=48.94s
Epoch 060 | train_loss=0.1008 val_loss=0.0901 P=0.423 R=0.399 F1=0.410 ROC-AUC=0.984 AUPR=0.373 time=49.12s
Epoch 061 | train_loss=0.1396 val_loss=0.0912 P=0.458 R=0.370 F1=0.409 ROC-AUC=0.984 AUPR=0.374 time=48.78s
Epoch 062 | train_loss=0.1173 val_loss=0.1000 P=0.527 R=0.344 F1=0.416 ROC-AUC=0.982 AUPR=0.374 time=48.70s
Epoch 063 | train_loss=0.0891 val_loss=0.0911 P=0.445 R=0.389 F1=0.415 ROC-AUC=0.984 AUPR=0.378 time=49.10s
Epoch 064 | train_loss=0.0874 val_loss=0.0905 P=0.438 R=0.391 F1=0.413 ROC-AUC=0.985 AUPR=0.381 time=48.76s
Epoch 065 | train_loss=0.0948 val_loss=0.0913 P=0.466 R=0.375 F1=0.416 ROC-AUC=0.984 AUPR=0.376 time=48.58s
Epoch 066 | train_loss=0.1878 val_loss=0.0891 P=0.518 R=0.370 F1=0.432 ROC-AUC=0.985 AUPR=0.390 time=49.10s
Epoch 067 | train_loss=0.1167 val_loss=0.0900 P=0.500 R=0.381 F1=0.433 ROC-AUC=0.984 AUPR=0.393 time=48.72s
Epoch 068 | train_loss=0.1254 val_loss=0.0896 P=0.458 R=0.394 F1=0.423 ROC-AUC=0.985 AUPR=0.391 time=48.95s
Epoch 069 | train_loss=0.2106 val_loss=0.0960 P=0.443 R=0.402 F1=0.421 ROC-AUC=0.984 AUPR=0.378 time=49.11s
Epoch 070 | train_loss=0.1515 val_loss=0.0920 P=0.456 R=0.387 F1=0.419 ROC-AUC=0.984 AUPR=0.378 time=48.70s
Epoch 071 | train_loss=0.2072 val_loss=0.0901 P=0.545 R=0.350 F1=0.427 ROC-AUC=0.985 AUPR=0.393 time=48.93s
Epoch 072 | train_loss=0.0927 val_loss=0.0892 P=0.424 R=0.418 F1=0.421 ROC-AUC=0.985 AUPR=0.387 time=48.80s
Epoch 073 | train_loss=0.0926 val_loss=0.0898 P=0.452 R=0.393 F1=0.420 ROC-AUC=0.985 AUPR=0.383 time=48.81s
Epoch 074 | train_loss=0.2438 val_loss=0.0900 P=0.411 R=0.428 F1=0.419 ROC-AUC=0.985 AUPR=0.390 time=49.11s
Epoch 075 | train_loss=0.1112 val_loss=0.0892 P=0.480 R=0.400 F1=0.436 ROC-AUC=0.985 AUPR=0.400 time=49.35s
Epoch 076 | train_loss=0.1346 val_loss=0.0902 P=0.557 R=0.330 F1=0.415 ROC-AUC=0.985 AUPR=0.385 time=50.01s
Epoch 077 | train_loss=0.1191 val_loss=0.0904 P=0.543 R=0.347 F1=0.423 ROC-AUC=0.985 AUPR=0.393 time=50.04s
Epoch 078 | train_loss=0.1021 val_loss=0.0898 P=0.501 R=0.359 F1=0.418 ROC-AUC=0.985 AUPR=0.391 time=48.87s
Epoch 079 | train_loss=0.1032 val_loss=0.0887 P=0.467 R=0.397 F1=0.429 ROC-AUC=0.985 AUPR=0.400 time=48.83s
Epoch 080 | train_loss=0.1494 val_loss=0.0890 P=0.528 R=0.352 F1=0.423 ROC-AUC=0.985 AUPR=0.397 time=48.88s
Epoch 081 | train_loss=0.1492 val_loss=0.0890 P=0.484 R=0.383 F1=0.428 ROC-AUC=0.985 AUPR=0.394 time=48.89s
Epoch 082 | train_loss=0.0918 val_loss=0.0900 P=0.494 R=0.388 F1=0.435 ROC-AUC=0.985 AUPR=0.402 time=48.99s
Epoch 083 | train_loss=0.1191 val_loss=0.0909 P=0.539 R=0.359 F1=0.431 ROC-AUC=0.985 AUPR=0.403 time=50.21s
Epoch 084 | train_loss=0.0998 val_loss=0.0902 P=0.483 R=0.392 F1=0.433 ROC-AUC=0.985 AUPR=0.405 time=50.44s
Epoch 085 | train_loss=0.1029 val_loss=0.0886 P=0.498 R=0.381 F1=0.432 ROC-AUC=0.985 AUPR=0.404 time=49.76s
Epoch 086 | train_loss=0.1444 val_loss=0.0903 P=0.512 R=0.379 F1=0.436 ROC-AUC=0.985 AUPR=0.405 time=48.96s
Epoch 087 | train_loss=0.1675 val_loss=0.0925 P=0.470 R=0.378 F1=0.419 ROC-AUC=0.984 AUPR=0.376 time=48.92s
Epoch 088 | train_loss=0.1152 val_loss=0.0895 P=0.542 R=0.370 F1=0.439 ROC-AUC=0.985 AUPR=0.406 time=50.14s
Epoch 089 | train_loss=0.0953 val_loss=0.0903 P=0.546 R=0.369 F1=0.440 ROC-AUC=0.985 AUPR=0.404 time=48.80s
Epoch 090 | train_loss=0.1083 val_loss=0.0914 P=0.446 R=0.405 F1=0.425 ROC-AUC=0.984 AUPR=0.390 time=49.19s
Epoch 091 | train_loss=0.1737 val_loss=0.1770 P=0.454 R=0.396 F1=0.423 ROC-AUC=0.944 AUPR=0.388 time=48.90s
Epoch 092 | train_loss=0.0925 val_loss=0.0902 P=0.540 R=0.341 F1=0.418 ROC-AUC=0.985 AUPR=0.386 time=49.10s
Epoch 093 | train_loss=0.0920 val_loss=0.0919 P=0.498 R=0.380 F1=0.431 ROC-AUC=0.984 AUPR=0.398 time=49.88s
Epoch 094 | train_loss=0.0876 val_loss=0.0924 P=0.509 R=0.390 F1=0.442 ROC-AUC=0.984 AUPR=0.395 time=49.98s
Epoch 095 | train_loss=0.1516 val_loss=0.0899 P=0.523 R=0.357 F1=0.424 ROC-AUC=0.984 AUPR=0.394 time=49.91s
Epoch 096 | train_loss=0.1208 val_loss=0.0895 P=0.506 R=0.387 F1=0.438 ROC-AUC=0.985 AUPR=0.407 time=50.30s
Epoch 097 | train_loss=0.1229 val_loss=0.0933 P=0.463 R=0.382 F1=0.419 ROC-AUC=0.984 AUPR=0.384 time=50.66s
Epoch 098 | train_loss=0.1084 val_loss=0.0919 P=0.529 R=0.381 F1=0.443 ROC-AUC=0.984 AUPR=0.407 time=49.15s
Epoch 099 | train_loss=0.1085 val_loss=0.0906 P=0.507 R=0.369 F1=0.427 ROC-AUC=0.985 AUPR=0.392 time=48.96s
Epoch 100 | train_loss=0.1028 val_loss=0.0909 P=0.509 R=0.392 F1=0.443 ROC-AUC=0.984 AUPR=0.410 time=48.92s
Epoch 101 | train_loss=0.1213 val_loss=0.0894 P=0.478 R=0.394 F1=0.432 ROC-AUC=0.985 AUPR=0.402 time=48.95s
Epoch 102 | train_loss=0.1390 val_loss=0.0911 P=0.522 R=0.359 F1=0.426 ROC-AUC=0.984 AUPR=0.392 time=48.96s
Epoch 103 | train_loss=0.1511 val_loss=0.0901 P=0.582 R=0.350 F1=0.437 ROC-AUC=0.985 AUPR=0.410 time=49.06s
Epoch 104 | train_loss=0.1083 val_loss=0.0898 P=0.528 R=0.375 F1=0.439 ROC-AUC=0.985 AUPR=0.403 time=48.96s
Epoch 105 | train_loss=0.1215 val_loss=0.0887 P=0.525 R=0.385 F1=0.444 ROC-AUC=0.985 AUPR=0.409 time=49.67s
Epoch 106 | train_loss=0.1691 val_loss=0.0950 P=0.570 R=0.354 F1=0.437 ROC-AUC=0.984 AUPR=0.406 time=50.68s
Epoch 107 | train_loss=0.1115 val_loss=0.0929 P=0.455 R=0.394 F1=0.422 ROC-AUC=0.984 AUPR=0.395 time=50.91s
Epoch 108 | train_loss=0.1028 val_loss=0.0907 P=0.474 R=0.394 F1=0.430 ROC-AUC=0.985 AUPR=0.399 time=49.59s
Epoch 109 | train_loss=0.3352 val_loss=0.0904 P=0.520 R=0.364 F1=0.428 ROC-AUC=0.985 AUPR=0.399 time=50.19s
Epoch 110 | train_loss=0.1295 val_loss=0.0916 P=0.540 R=0.380 F1=0.446 ROC-AUC=0.984 AUPR=0.407 time=50.18s
Epoch 111 | train_loss=0.2065 val_loss=0.0904 P=0.515 R=0.374 F1=0.433 ROC-AUC=0.984 AUPR=0.398 time=50.14s
Epoch 112 | train_loss=0.2184 val_loss=0.0923 P=0.537 R=0.365 F1=0.434 ROC-AUC=0.984 AUPR=0.405 time=48.78s
Epoch 113 | train_loss=0.1849 val_loss=0.0908 P=0.491 R=0.383 F1=0.430 ROC-AUC=0.984 AUPR=0.399 time=49.05s
Epoch 114 | train_loss=0.1034 val_loss=0.0914 P=0.506 R=0.382 F1=0.436 ROC-AUC=0.984 AUPR=0.407 time=49.35s
Epoch 115 | train_loss=0.0913 val_loss=0.0919 P=0.609 R=0.325 F1=0.424 ROC-AUC=0.984 AUPR=0.394 time=49.94s
Epoch 116 | train_loss=0.0984 val_loss=0.0924 P=0.484 R=0.386 F1=0.430 ROC-AUC=0.984 AUPR=0.383 time=49.64s
Epoch 117 | train_loss=0.1026 val_loss=0.0931 P=0.466 R=0.404 F1=0.433 ROC-AUC=0.984 AUPR=0.400 time=50.14s
Epoch 118 | train_loss=0.1573 val_loss=0.0914 P=0.456 R=0.417 F1=0.435 ROC-AUC=0.984 AUPR=0.405 time=50.23s
Epoch 119 | train_loss=0.1061 val_loss=0.0905 P=0.488 R=0.413 F1=0.447 ROC-AUC=0.984 AUPR=0.414 time=50.14s
Epoch 120 | train_loss=0.2117 val_loss=0.0920 P=0.476 R=0.410 F1=0.441 ROC-AUC=0.984 AUPR=0.400 time=50.29s
Epoch 121 | train_loss=0.1189 val_loss=0.0929 P=0.514 R=0.381 F1=0.438 ROC-AUC=0.984 AUPR=0.405 time=48.95s
Epoch 122 | train_loss=0.1943 val_loss=0.0949 P=0.504 R=0.383 F1=0.435 ROC-AUC=0.984 AUPR=0.393 time=48.86s
Epoch 123 | train_loss=0.1757 val_loss=0.0918 P=0.492 R=0.403 F1=0.444 ROC-AUC=0.984 AUPR=0.409 time=49.05s
Epoch 124 | train_loss=0.0855 val_loss=0.0905 P=0.505 R=0.402 F1=0.448 ROC-AUC=0.984 AUPR=0.408 time=49.09s
Epoch 125 | train_loss=0.2347 val_loss=0.0906 P=0.541 R=0.374 F1=0.442 ROC-AUC=0.984 AUPR=0.407 time=49.30s
Epoch 126 | train_loss=0.1520 val_loss=0.0913 P=0.523 R=0.376 F1=0.438 ROC-AUC=0.984 AUPR=0.406 time=49.13s
Epoch 127 | train_loss=0.0922 val_loss=0.0914 P=0.531 R=0.367 F1=0.434 ROC-AUC=0.984 AUPR=0.404 time=48.89s
Epoch 128 | train_loss=0.0915 val_loss=0.0911 P=0.525 R=0.377 F1=0.439 ROC-AUC=0.984 AUPR=0.403 time=50.27s
Epoch 129 | train_loss=0.2575 val_loss=0.0930 P=0.564 R=0.364 F1=0.442 ROC-AUC=0.984 AUPR=0.408 time=49.58s
Epoch 130 | train_loss=0.1410 val_loss=0.0920 P=0.522 R=0.394 F1=0.449 ROC-AUC=0.984 AUPR=0.410 time=49.89s
Epoch 131 | train_loss=0.1521 val_loss=0.0911 P=0.516 R=0.395 F1=0.447 ROC-AUC=0.984 AUPR=0.409 time=49.29s
Epoch 132 | train_loss=0.1102 val_loss=0.0926 P=0.536 R=0.395 F1=0.455 ROC-AUC=0.984 AUPR=0.418 time=50.12s
Epoch 133 | train_loss=0.1103 val_loss=0.0910 P=0.535 R=0.377 F1=0.443 ROC-AUC=0.984 AUPR=0.415 time=49.22s
Epoch 134 | train_loss=0.1027 val_loss=0.0932 P=0.555 R=0.367 F1=0.442 ROC-AUC=0.984 AUPR=0.409 time=49.17s
Epoch 135 | train_loss=0.2700 val_loss=0.0941 P=0.523 R=0.385 F1=0.444 ROC-AUC=0.984 AUPR=0.402 time=49.20s
Epoch 136 | train_loss=0.1400 val_loss=0.0908 P=0.561 R=0.362 F1=0.440 ROC-AUC=0.984 AUPR=0.403 time=48.99s
Epoch 137 | train_loss=0.1516 val_loss=0.0909 P=0.476 R=0.412 F1=0.442 ROC-AUC=0.984 AUPR=0.410 time=49.03s
Epoch 138 | train_loss=0.1928 val_loss=0.0950 P=0.543 R=0.366 F1=0.437 ROC-AUC=0.984 AUPR=0.400 time=48.90s
Epoch 139 | train_loss=0.0909 val_loss=0.0923 P=0.527 R=0.389 F1=0.448 ROC-AUC=0.984 AUPR=0.409 time=49.09s
Epoch 140 | train_loss=0.1222 val_loss=0.1913 P=0.474 R=0.399 F1=0.433 ROC-AUC=0.944 AUPR=0.395 time=49.91s
Epoch 141 | train_loss=0.1421 val_loss=0.1032 P=0.483 R=0.386 F1=0.429 ROC-AUC=0.983 AUPR=0.398 time=49.99s
Epoch 142 | train_loss=0.1122 val_loss=0.0932 P=0.489 R=0.401 F1=0.441 ROC-AUC=0.984 AUPR=0.404 time=49.98s
Epoch 143 | train_loss=0.1014 val_loss=0.0939 P=0.542 R=0.366 F1=0.437 ROC-AUC=0.984 AUPR=0.399 time=49.12s
Epoch 144 | train_loss=0.1194 val_loss=0.0918 P=0.497 R=0.401 F1=0.444 ROC-AUC=0.984 AUPR=0.410 time=49.04s
Epoch 145 | train_loss=0.0895 val_loss=0.0906 P=0.487 R=0.401 F1=0.440 ROC-AUC=0.984 AUPR=0.405 time=49.13s
Epoch 146 | train_loss=0.1985 val_loss=0.0984 P=0.450 R=0.417 F1=0.433 ROC-AUC=0.983 AUPR=0.395 time=48.96s
Epoch 147 | train_loss=0.1407 val_loss=0.0958 P=0.544 R=0.370 F1=0.440 ROC-AUC=0.983 AUPR=0.409 time=48.96s
Epoch 148 | train_loss=0.1560 val_loss=0.0942 P=0.569 R=0.344 F1=0.428 ROC-AUC=0.983 AUPR=0.401 time=50.36s
Epoch 149 | train_loss=0.1319 val_loss=0.0924 P=0.529 R=0.382 F1=0.444 ROC-AUC=0.984 AUPR=0.404 time=48.97s
Epoch 150 | train_loss=0.1909 val_loss=0.0900 P=0.479 R=0.404 F1=0.439 ROC-AUC=0.984 AUPR=0.407 time=49.08s
Epoch 151 | train_loss=0.1146 val_loss=0.0923 P=0.543 R=0.373 F1=0.442 ROC-AUC=0.984 AUPR=0.406 time=49.26s
Epoch 152 | train_loss=0.1557 val_loss=0.0945 P=0.457 R=0.416 F1=0.436 ROC-AUC=0.984 AUPR=0.395 time=49.86s
Epoch 153 | train_loss=0.1825 val_loss=0.6248 P=0.441 R=0.336 F1=0.381 ROC-AUC=0.900 AUPR=0.320 time=50.14s
Epoch 154 | train_loss=0.1736 val_loss=0.0921 P=0.438 R=0.432 F1=0.435 ROC-AUC=0.984 AUPR=0.407 time=50.52s
Epoch 155 | train_loss=0.1579 val_loss=0.0908 P=0.534 R=0.375 F1=0.440 ROC-AUC=0.984 AUPR=0.408 time=49.08s
Epoch 156 | train_loss=0.1375 val_loss=0.0955 P=0.545 R=0.373 F1=0.443 ROC-AUC=0.984 AUPR=0.411 time=50.26s
Epoch 157 | train_loss=0.0966 val_loss=0.0951 P=0.566 R=0.357 F1=0.438 ROC-AUC=0.983 AUPR=0.404 time=50.14s

Early stopping at epoch 157

Total training time: 7748.09s (129.1 min)

Loading best model from epoch 132...
Evaluating final model...
======================================================================
EVALUATION RESULTS: RAT_low_GraphSAGE_T TRAIN
======================================================================

STANDARD METRICS:
  Precision:        0.5432
  Recall:           0.4111
  F1-Score:         0.4680
  ROC-AUC:          0.9908
  AUPR:             0.4591
  Balanced Acc:     0.7054

IMBALANCED-AWARE METRICS:
  MCC:              0.4721
  Cohen Kappa:      0.4676
  Specificity:      0.9996

THRESHOLD: 0.960

CONFUSION MATRIX:
  True Negatives:   3042869
  False Positives:  1074
  False Negatives:  1829
  True Positives:   1277

TOP-K PRECISION:
  precision_at_100: 0.9800
  precision_at_500: 0.9220
  precision_at_1000: 0.8050
======================================================================
======================================================================
EVALUATION RESULTS: RAT_low_GraphSAGE_T VAL
======================================================================

STANDARD METRICS:
  Precision:        0.5360
  Recall:           0.3948
  F1-Score:         0.4547
  ROC-AUC:          0.9841
  AUPR:             0.4178
  Balanced Acc:     0.6972

IMBALANCED-AWARE METRICS:
  MCC:              0.4596
  Cohen Kappa:      0.4542
  Specificity:      0.9997

THRESHOLD: 0.965

CONFUSION MATRIX:
  True Negatives:   1014293
  False Positives:  354
  False Negatives:  627
  True Positives:   409

TOP-K PRECISION:
  precision_at_100: 0.9400
  precision_at_500: 0.6500
  precision_at_1000: 0.4490
======================================================================
======================================================================
EVALUATION RESULTS: RAT_low_GraphSAGE_T TEST
======================================================================

STANDARD METRICS:
  Precision:        0.5233
  Recall:           0.4010
  F1-Score:         0.4540
  ROC-AUC:          0.9828
  AUPR:             0.4229
  Balanced Acc:     0.7003

IMBALANCED-AWARE METRICS:
  MCC:              0.4576
  Cohen Kappa:      0.4536
  Specificity:      0.9996

THRESHOLD: 0.960

CONFUSION MATRIX:
  True Negatives:   1014270
  False Positives:  378
  False Negatives:  620
  True Positives:   415

TOP-K PRECISION:
  precision_at_100: 0.9400
  precision_at_500: 0.6620
  precision_at_1000: 0.4410
======================================================================

Saving results...
 Saved metrics to: C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\results\HI-Small_Trans_RAT_low\graphsage_t\metrics.json
 Saved prediction probabilities
 Saved experiment config

======================================================================
GraphSAGE-T Training Complete!
Results saved to: C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\results\HI-Small_Trans_RAT_low\graphsage_t
======================================================================

================================================================================
Logging ended at: 2025-11-28 05:34:55
================================================================================
