
================================================================================
Logging started at: 2025-11-28 20:01:59
Log file: C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\logs\HI-Small_Trans_RAT_high\graphsage_t\RAT_high_GraphSAGE_T_20251128_200159.txt
================================================================================

[INFO] Logging to: C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\logs\HI-Small_Trans_RAT_high\graphsage_t\RAT_high_GraphSAGE_T_20251128_200159.txt
================================================================================
EXPERIMENT CONFIGURATION
================================================================================

[DATASET]
  Theory:       RAT
  Intensity:    high
  Full name:    HI-Small_Trans_RAT_high

[MODEL] GraphSAGE-T
  Hidden dim:   128
  Num layers:   2
  Dropout:      0.2

[TRAINING]
  Device:       cuda
  Batch size:   8192
  Learning rate: 0.0005
  Epochs:       350
  Early stop:   25 epochs

[PATHS]
  Graphs:       C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\graphs\HI-Small_Trans_RAT_high
  Splits:       C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\splits\HI-Small_Trans_RAT_high
  Results:      C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\results\HI-Small_Trans_RAT_high\graphsage_t
  Logs:         C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\logs\HI-Small_Trans_RAT_high\graphsage_t

[EXPERIMENT]
  Seed:         42
  Exp name:     default_experiment

================================================================================

[DEVICE] cuda


======= GRAPHSAGE-T TRAINING (FP32) =======
Batch size:       8192
Eval batch size:  16384
Device:           cuda
===========================================


Starting training for 350 epochs...

Epoch 001 | train_loss=17.9548 val_loss=0.3979 P=0.002 R=0.020 F1=0.004 ROC-AUC=0.603 AUPR=0.001 time=57.08s
Epoch 002 | train_loss=0.5205 val_loss=0.3484 P=0.020 R=0.021 F1=0.021 ROC-AUC=0.699 AUPR=0.003 time=54.76s
Epoch 003 | train_loss=0.4350 val_loss=0.3316 P=0.051 R=0.072 F1=0.060 ROC-AUC=0.755 AUPR=0.011 time=53.32s
Epoch 004 | train_loss=0.3925 val_loss=0.3197 P=0.062 R=0.083 F1=0.071 ROC-AUC=0.765 AUPR=0.017 time=53.44s
Epoch 005 | train_loss=0.3882 val_loss=0.2987 P=0.052 R=0.117 F1=0.072 ROC-AUC=0.778 AUPR=0.017 time=56.17s
Epoch 006 | train_loss=0.3783 val_loss=0.2975 P=0.064 R=0.115 F1=0.082 ROC-AUC=0.807 AUPR=0.024 time=53.60s
Epoch 007 | train_loss=0.3453 val_loss=0.2728 P=0.089 R=0.105 F1=0.096 ROC-AUC=0.814 AUPR=0.025 time=51.43s
Epoch 008 | train_loss=0.3593 val_loss=0.2752 P=0.081 R=0.135 F1=0.101 ROC-AUC=0.804 AUPR=0.027 time=52.07s
Epoch 009 | train_loss=0.3015 val_loss=0.2629 P=0.084 R=0.156 F1=0.109 ROC-AUC=0.819 AUPR=0.034 time=52.55s
Epoch 010 | train_loss=0.3313 val_loss=0.2473 P=0.111 R=0.132 F1=0.121 ROC-AUC=0.847 AUPR=0.040 time=52.06s
Epoch 011 | train_loss=0.2876 val_loss=0.2452 P=0.135 R=0.141 F1=0.138 ROC-AUC=0.841 AUPR=0.050 time=55.51s
Epoch 012 | train_loss=0.3198 val_loss=0.4113 P=0.133 R=0.164 F1=0.147 ROC-AUC=0.797 AUPR=0.051 time=51.56s
Epoch 013 | train_loss=0.3076 val_loss=0.1793 P=0.202 R=0.226 F1=0.213 ROC-AUC=0.923 AUPR=0.115 time=53.86s
Epoch 014 | train_loss=0.2534 val_loss=0.1559 P=0.246 R=0.259 F1=0.252 ROC-AUC=0.930 AUPR=0.152 time=55.19s
Epoch 015 | train_loss=0.1661 val_loss=0.1168 P=0.271 R=0.303 F1=0.286 ROC-AUC=0.977 AUPR=0.186 time=56.77s
Epoch 016 | train_loss=0.1429 val_loss=0.1091 P=0.289 R=0.313 F1=0.300 ROC-AUC=0.979 AUPR=0.205 time=55.88s
Epoch 017 | train_loss=0.1409 val_loss=0.1054 P=0.297 R=0.290 F1=0.293 ROC-AUC=0.981 AUPR=0.206 time=50.69s
Epoch 018 | train_loss=0.1426 val_loss=0.1059 P=0.322 R=0.304 F1=0.313 ROC-AUC=0.979 AUPR=0.234 time=58.73s
Epoch 019 | train_loss=0.1513 val_loss=0.1028 P=0.295 R=0.313 F1=0.303 ROC-AUC=0.981 AUPR=0.223 time=59.18s
Epoch 020 | train_loss=0.1674 val_loss=0.1007 P=0.280 R=0.372 F1=0.320 ROC-AUC=0.981 AUPR=0.240 time=58.44s
Epoch 021 | train_loss=0.1646 val_loss=0.0998 P=0.279 R=0.392 F1=0.326 ROC-AUC=0.981 AUPR=0.249 time=53.92s
Epoch 022 | train_loss=0.1543 val_loss=0.0976 P=0.331 R=0.323 F1=0.327 ROC-AUC=0.983 AUPR=0.242 time=54.23s
Epoch 023 | train_loss=0.1097 val_loss=0.0974 P=0.367 R=0.304 F1=0.333 ROC-AUC=0.983 AUPR=0.249 time=58.34s
Epoch 024 | train_loss=0.1258 val_loss=0.0955 P=0.372 R=0.305 F1=0.335 ROC-AUC=0.983 AUPR=0.254 time=51.80s
Epoch 025 | train_loss=0.1231 val_loss=0.0986 P=0.322 R=0.346 F1=0.333 ROC-AUC=0.982 AUPR=0.252 time=51.53s
Epoch 026 | train_loss=0.1117 val_loss=0.0936 P=0.328 R=0.362 F1=0.344 ROC-AUC=0.984 AUPR=0.271 time=68.10s
Epoch 027 | train_loss=0.1274 val_loss=0.1021 P=0.314 R=0.366 F1=0.338 ROC-AUC=0.981 AUPR=0.257 time=52.25s
Epoch 028 | train_loss=0.1377 val_loss=0.1052 P=0.318 R=0.364 F1=0.340 ROC-AUC=0.978 AUPR=0.266 time=50.61s
Epoch 029 | train_loss=0.1347 val_loss=0.1034 P=0.387 R=0.300 F1=0.338 ROC-AUC=0.980 AUPR=0.272 time=49.84s
Epoch 030 | train_loss=0.1033 val_loss=0.1065 P=0.314 R=0.370 F1=0.339 ROC-AUC=0.980 AUPR=0.269 time=49.59s
Epoch 031 | train_loss=0.1299 val_loss=0.0995 P=0.320 R=0.398 F1=0.355 ROC-AUC=0.982 AUPR=0.288 time=51.89s
Epoch 032 | train_loss=0.1127 val_loss=0.1018 P=0.319 R=0.395 F1=0.353 ROC-AUC=0.981 AUPR=0.277 time=52.72s
Epoch 033 | train_loss=0.1563 val_loss=0.0999 P=0.384 R=0.350 F1=0.366 ROC-AUC=0.982 AUPR=0.294 time=52.72s
Epoch 034 | train_loss=0.1099 val_loss=0.0969 P=0.321 R=0.412 F1=0.361 ROC-AUC=0.982 AUPR=0.297 time=52.58s
Epoch 035 | train_loss=0.0988 val_loss=0.0997 P=0.340 R=0.399 F1=0.367 ROC-AUC=0.982 AUPR=0.296 time=53.30s
Epoch 036 | train_loss=0.1391 val_loss=0.0964 P=0.348 R=0.376 F1=0.361 ROC-AUC=0.982 AUPR=0.291 time=53.31s
Epoch 037 | train_loss=0.1350 val_loss=0.0966 P=0.444 R=0.314 F1=0.368 ROC-AUC=0.982 AUPR=0.304 time=54.50s
Epoch 038 | train_loss=0.1000 val_loss=0.0967 P=0.384 R=0.331 F1=0.356 ROC-AUC=0.983 AUPR=0.280 time=56.73s
Epoch 039 | train_loss=0.1202 val_loss=0.0950 P=0.364 R=0.396 F1=0.379 ROC-AUC=0.983 AUPR=0.303 time=56.53s
Epoch 040 | train_loss=0.1067 val_loss=0.0949 P=0.441 R=0.330 F1=0.377 ROC-AUC=0.983 AUPR=0.305 time=53.58s
Epoch 041 | train_loss=0.1285 val_loss=0.0933 P=0.418 R=0.360 F1=0.387 ROC-AUC=0.983 AUPR=0.321 time=55.58s
Epoch 042 | train_loss=0.1069 val_loss=0.0954 P=0.418 R=0.371 F1=0.393 ROC-AUC=0.983 AUPR=0.323 time=54.83s
Epoch 043 | train_loss=0.1012 val_loss=0.0946 P=0.402 R=0.382 F1=0.392 ROC-AUC=0.983 AUPR=0.325 time=52.08s
Epoch 044 | train_loss=0.1221 val_loss=0.0942 P=0.460 R=0.334 F1=0.387 ROC-AUC=0.984 AUPR=0.321 time=49.34s
Epoch 045 | train_loss=0.1060 val_loss=0.0931 P=0.456 R=0.352 F1=0.398 ROC-AUC=0.984 AUPR=0.325 time=49.47s
Epoch 046 | train_loss=0.1102 val_loss=0.0930 P=0.480 R=0.325 F1=0.388 ROC-AUC=0.983 AUPR=0.328 time=49.48s
Epoch 047 | train_loss=0.1046 val_loss=0.0942 P=0.405 R=0.363 F1=0.383 ROC-AUC=0.984 AUPR=0.319 time=49.53s
Epoch 048 | train_loss=0.1082 val_loss=0.0920 P=0.434 R=0.353 F1=0.390 ROC-AUC=0.984 AUPR=0.331 time=55.24s
Epoch 049 | train_loss=0.0922 val_loss=0.0917 P=0.451 R=0.354 F1=0.397 ROC-AUC=0.984 AUPR=0.338 time=53.24s
Epoch 050 | train_loss=0.0977 val_loss=0.0933 P=0.419 R=0.375 F1=0.395 ROC-AUC=0.983 AUPR=0.341 time=56.21s
Epoch 051 | train_loss=0.5781 val_loss=0.0919 P=0.420 R=0.373 F1=0.395 ROC-AUC=0.984 AUPR=0.338 time=54.37s
Epoch 052 | train_loss=0.0912 val_loss=0.0919 P=0.452 R=0.362 F1=0.402 ROC-AUC=0.984 AUPR=0.338 time=58.84s
Epoch 053 | train_loss=0.2240 val_loss=0.0929 P=0.412 R=0.364 F1=0.387 ROC-AUC=0.983 AUPR=0.334 time=58.20s
Epoch 054 | train_loss=0.1105 val_loss=0.0939 P=0.401 R=0.401 F1=0.401 ROC-AUC=0.983 AUPR=0.345 time=63.02s
Epoch 055 | train_loss=0.0981 val_loss=0.0937 P=0.450 R=0.369 F1=0.405 ROC-AUC=0.984 AUPR=0.356 time=54.04s
Epoch 056 | train_loss=0.1337 val_loss=0.0954 P=0.438 R=0.366 F1=0.399 ROC-AUC=0.983 AUPR=0.357 time=62.16s
Epoch 057 | train_loss=0.1013 val_loss=0.0918 P=0.389 R=0.392 F1=0.390 ROC-AUC=0.984 AUPR=0.342 time=50.62s
Epoch 058 | train_loss=0.1635 val_loss=0.0917 P=0.437 R=0.377 F1=0.405 ROC-AUC=0.984 AUPR=0.354 time=50.78s
Epoch 059 | train_loss=0.1129 val_loss=0.0919 P=0.392 R=0.401 F1=0.396 ROC-AUC=0.984 AUPR=0.352 time=53.27s
Epoch 060 | train_loss=0.1074 val_loss=0.0913 P=0.440 R=0.364 F1=0.399 ROC-AUC=0.984 AUPR=0.352 time=51.14s
Epoch 061 | train_loss=0.1431 val_loss=0.0925 P=0.458 R=0.347 F1=0.395 ROC-AUC=0.984 AUPR=0.349 time=50.75s
Epoch 062 | train_loss=0.1535 val_loss=0.0921 P=0.460 R=0.360 F1=0.404 ROC-AUC=0.984 AUPR=0.354 time=50.90s
Epoch 063 | train_loss=0.1048 val_loss=0.0918 P=0.499 R=0.344 F1=0.407 ROC-AUC=0.984 AUPR=0.365 time=51.11s
Epoch 064 | train_loss=0.1498 val_loss=0.6385 P=0.441 R=0.375 F1=0.405 ROC-AUC=0.903 AUPR=0.367 time=50.53s
Epoch 065 | train_loss=0.1642 val_loss=0.0965 P=0.434 R=0.356 F1=0.391 ROC-AUC=0.982 AUPR=0.349 time=51.04s
Epoch 066 | train_loss=0.1419 val_loss=0.0921 P=0.435 R=0.388 F1=0.410 ROC-AUC=0.984 AUPR=0.365 time=50.92s
Epoch 067 | train_loss=0.1114 val_loss=0.0919 P=0.457 R=0.369 F1=0.408 ROC-AUC=0.984 AUPR=0.366 time=51.35s
Epoch 068 | train_loss=0.1521 val_loss=0.0906 P=0.460 R=0.364 F1=0.406 ROC-AUC=0.984 AUPR=0.363 time=56.25s
Epoch 069 | train_loss=0.1577 val_loss=0.1000 P=0.511 R=0.324 F1=0.397 ROC-AUC=0.982 AUPR=0.352 time=52.88s
Epoch 070 | train_loss=0.1071 val_loss=0.0915 P=0.428 R=0.392 F1=0.409 ROC-AUC=0.984 AUPR=0.372 time=54.84s
Epoch 071 | train_loss=0.1056 val_loss=0.0924 P=0.462 R=0.382 F1=0.418 ROC-AUC=0.984 AUPR=0.372 time=52.82s
Epoch 072 | train_loss=0.0949 val_loss=0.0912 P=0.385 R=0.414 F1=0.399 ROC-AUC=0.984 AUPR=0.359 time=51.31s
Epoch 073 | train_loss=0.1082 val_loss=0.0909 P=0.546 R=0.328 F1=0.410 ROC-AUC=0.984 AUPR=0.374 time=53.82s
Epoch 074 | train_loss=0.1704 val_loss=0.0905 P=0.475 R=0.343 F1=0.398 ROC-AUC=0.984 AUPR=0.368 time=54.72s
Epoch 075 | train_loss=0.1403 val_loss=0.0910 P=0.521 R=0.343 F1=0.413 ROC-AUC=0.984 AUPR=0.375 time=55.42s
Epoch 076 | train_loss=0.1839 val_loss=0.0915 P=0.477 R=0.369 F1=0.416 ROC-AUC=0.984 AUPR=0.377 time=55.05s
Epoch 077 | train_loss=0.2242 val_loss=0.0912 P=0.510 R=0.358 F1=0.421 ROC-AUC=0.984 AUPR=0.383 time=56.55s
Epoch 078 | train_loss=0.1161 val_loss=0.0903 P=0.442 R=0.394 F1=0.417 ROC-AUC=0.984 AUPR=0.377 time=54.34s
Epoch 079 | train_loss=0.1366 val_loss=0.0901 P=0.497 R=0.358 F1=0.416 ROC-AUC=0.984 AUPR=0.384 time=55.03s
Epoch 080 | train_loss=0.0956 val_loss=0.1104 P=0.404 R=0.400 F1=0.402 ROC-AUC=0.977 AUPR=0.365 time=56.86s
Epoch 081 | train_loss=0.0957 val_loss=0.0902 P=0.458 R=0.376 F1=0.413 ROC-AUC=0.984 AUPR=0.379 time=56.08s
Epoch 082 | train_loss=0.1133 val_loss=0.0902 P=0.450 R=0.377 F1=0.411 ROC-AUC=0.985 AUPR=0.378 time=56.95s
Epoch 083 | train_loss=0.1054 val_loss=0.0921 P=0.437 R=0.391 F1=0.413 ROC-AUC=0.984 AUPR=0.380 time=51.71s
Epoch 084 | train_loss=0.0900 val_loss=0.0918 P=0.427 R=0.405 F1=0.416 ROC-AUC=0.984 AUPR=0.382 time=55.31s
Epoch 085 | train_loss=0.1229 val_loss=0.1329 P=0.457 R=0.396 F1=0.424 ROC-AUC=0.961 AUPR=0.388 time=54.17s
Epoch 086 | train_loss=0.1071 val_loss=0.0904 P=0.511 R=0.371 F1=0.430 ROC-AUC=0.985 AUPR=0.390 time=55.36s
Epoch 087 | train_loss=0.1890 val_loss=0.0943 P=0.492 R=0.347 F1=0.407 ROC-AUC=0.984 AUPR=0.370 time=55.05s
Epoch 088 | train_loss=0.1327 val_loss=0.0901 P=0.464 R=0.383 F1=0.420 ROC-AUC=0.984 AUPR=0.385 time=52.95s
Epoch 089 | train_loss=0.1229 val_loss=0.0910 P=0.440 R=0.387 F1=0.412 ROC-AUC=0.984 AUPR=0.373 time=55.21s
Epoch 090 | train_loss=0.1174 val_loss=0.0923 P=0.457 R=0.371 F1=0.409 ROC-AUC=0.984 AUPR=0.373 time=53.87s
Epoch 091 | train_loss=0.1107 val_loss=0.0931 P=0.421 R=0.410 F1=0.415 ROC-AUC=0.984 AUPR=0.376 time=56.10s
Epoch 092 | train_loss=0.1321 val_loss=0.0908 P=0.454 R=0.349 F1=0.395 ROC-AUC=0.984 AUPR=0.368 time=56.37s
Epoch 093 | train_loss=0.2018 val_loss=0.0906 P=0.474 R=0.384 F1=0.424 ROC-AUC=0.984 AUPR=0.393 time=51.95s
Epoch 094 | train_loss=0.0975 val_loss=0.0923 P=0.452 R=0.388 F1=0.417 ROC-AUC=0.984 AUPR=0.372 time=53.61s
Epoch 095 | train_loss=0.1117 val_loss=0.0898 P=0.415 R=0.411 F1=0.413 ROC-AUC=0.985 AUPR=0.384 time=49.78s
Epoch 096 | train_loss=0.0929 val_loss=0.0915 P=0.461 R=0.374 F1=0.413 ROC-AUC=0.984 AUPR=0.378 time=49.49s
Epoch 097 | train_loss=0.1708 val_loss=0.0911 P=0.488 R=0.364 F1=0.417 ROC-AUC=0.984 AUPR=0.388 time=49.39s
Epoch 098 | train_loss=0.1564 val_loss=0.0917 P=0.437 R=0.396 F1=0.415 ROC-AUC=0.984 AUPR=0.382 time=49.48s
Epoch 099 | train_loss=0.2122 val_loss=0.0930 P=0.513 R=0.358 F1=0.422 ROC-AUC=0.984 AUPR=0.384 time=51.68s
Epoch 100 | train_loss=0.1516 val_loss=0.0886 P=0.464 R=0.429 F1=0.446 ROC-AUC=0.985 AUPR=0.416 time=56.23s
Epoch 101 | train_loss=0.1058 val_loss=0.0886 P=0.461 R=0.412 F1=0.435 ROC-AUC=0.985 AUPR=0.411 time=53.55s
Epoch 102 | train_loss=0.1286 val_loss=0.0884 P=0.521 R=0.380 F1=0.440 ROC-AUC=0.985 AUPR=0.405 time=54.04s
Epoch 103 | train_loss=0.1070 val_loss=0.0889 P=0.501 R=0.381 F1=0.433 ROC-AUC=0.985 AUPR=0.400 time=50.55s
Epoch 104 | train_loss=0.1221 val_loss=0.1237 P=0.478 R=0.404 F1=0.438 ROC-AUC=0.972 AUPR=0.414 time=49.36s
Epoch 105 | train_loss=0.1107 val_loss=0.0891 P=0.503 R=0.391 F1=0.440 ROC-AUC=0.985 AUPR=0.412 time=54.93s
Epoch 106 | train_loss=0.1153 val_loss=0.1324 P=0.488 R=0.382 F1=0.429 ROC-AUC=0.971 AUPR=0.379 time=62.49s
Epoch 107 | train_loss=0.0960 val_loss=0.0879 P=0.511 R=0.382 F1=0.437 ROC-AUC=0.985 AUPR=0.417 time=54.76s
Epoch 108 | train_loss=0.1014 val_loss=0.0860 P=0.505 R=0.400 F1=0.446 ROC-AUC=0.986 AUPR=0.416 time=56.05s
Epoch 109 | train_loss=0.0973 val_loss=0.0845 P=0.495 R=0.413 F1=0.450 ROC-AUC=0.986 AUPR=0.424 time=53.07s
Epoch 110 | train_loss=0.0898 val_loss=0.0892 P=0.566 R=0.368 F1=0.446 ROC-AUC=0.985 AUPR=0.430 time=52.69s
Epoch 111 | train_loss=0.0994 val_loss=0.1536 P=0.570 R=0.339 F1=0.425 ROC-AUC=0.951 AUPR=0.390 time=54.63s
Epoch 112 | train_loss=0.0976 val_loss=0.0846 P=0.529 R=0.404 F1=0.458 ROC-AUC=0.986 AUPR=0.439 time=55.62s
Epoch 113 | train_loss=0.0914 val_loss=0.0900 P=0.470 R=0.394 F1=0.429 ROC-AUC=0.984 AUPR=0.389 time=55.54s
Epoch 114 | train_loss=0.0941 val_loss=0.0864 P=0.581 R=0.394 F1=0.470 ROC-AUC=0.986 AUPR=0.442 time=54.31s
Epoch 115 | train_loss=0.0997 val_loss=0.0849 P=0.574 R=0.381 F1=0.458 ROC-AUC=0.986 AUPR=0.437 time=51.83s
Epoch 116 | train_loss=0.0896 val_loss=0.0854 P=0.510 R=0.431 F1=0.467 ROC-AUC=0.986 AUPR=0.440 time=53.48s
Epoch 117 | train_loss=0.1666 val_loss=0.0876 P=0.541 R=0.413 F1=0.469 ROC-AUC=0.985 AUPR=0.438 time=49.45s
Epoch 118 | train_loss=0.0961 val_loss=0.0855 P=0.601 R=0.374 F1=0.461 ROC-AUC=0.986 AUPR=0.446 time=49.61s
Epoch 119 | train_loss=0.0828 val_loss=0.0853 P=0.579 R=0.396 F1=0.470 ROC-AUC=0.986 AUPR=0.445 time=49.75s
Epoch 120 | train_loss=0.0997 val_loss=0.0854 P=0.521 R=0.422 F1=0.466 ROC-AUC=0.986 AUPR=0.441 time=55.16s
Epoch 121 | train_loss=0.0862 val_loss=0.0880 P=0.485 R=0.431 F1=0.457 ROC-AUC=0.985 AUPR=0.434 time=54.55s
Epoch 122 | train_loss=0.0973 val_loss=0.0898 P=0.589 R=0.388 F1=0.468 ROC-AUC=0.985 AUPR=0.430 time=51.36s
Epoch 123 | train_loss=0.0928 val_loss=0.0864 P=0.581 R=0.410 F1=0.481 ROC-AUC=0.986 AUPR=0.453 time=49.41s
Epoch 124 | train_loss=0.0846 val_loss=0.0849 P=0.656 R=0.382 F1=0.483 ROC-AUC=0.986 AUPR=0.465 time=53.72s
Epoch 125 | train_loss=0.1152 val_loss=0.0854 P=0.612 R=0.401 F1=0.484 ROC-AUC=0.986 AUPR=0.459 time=53.75s
Epoch 126 | train_loss=0.0980 val_loss=0.0868 P=0.599 R=0.392 F1=0.474 ROC-AUC=0.986 AUPR=0.447 time=57.98s
Epoch 127 | train_loss=0.1354 val_loss=0.0858 P=0.512 R=0.421 F1=0.462 ROC-AUC=0.986 AUPR=0.435 time=53.68s
Epoch 128 | train_loss=0.1002 val_loss=0.0852 P=0.518 R=0.428 F1=0.468 ROC-AUC=0.986 AUPR=0.447 time=54.32s
Epoch 129 | train_loss=0.0924 val_loss=0.0885 P=0.475 R=0.445 F1=0.459 ROC-AUC=0.985 AUPR=0.436 time=52.50s
Epoch 130 | train_loss=0.0898 val_loss=0.0868 P=0.613 R=0.410 F1=0.492 ROC-AUC=0.986 AUPR=0.466 time=54.16s
Epoch 131 | train_loss=0.0860 val_loss=0.0881 P=0.584 R=0.400 F1=0.474 ROC-AUC=0.985 AUPR=0.440 time=55.53s
Epoch 132 | train_loss=0.1063 val_loss=0.0861 P=0.576 R=0.411 F1=0.480 ROC-AUC=0.986 AUPR=0.454 time=55.29s
Epoch 133 | train_loss=0.0913 val_loss=0.1967 P=0.501 R=0.405 F1=0.448 ROC-AUC=0.948 AUPR=0.417 time=55.58s
Epoch 134 | train_loss=0.0929 val_loss=0.0837 P=0.565 R=0.419 F1=0.481 ROC-AUC=0.986 AUPR=0.463 time=54.48s
Epoch 135 | train_loss=0.1071 val_loss=0.0875 P=0.637 R=0.396 F1=0.488 ROC-AUC=0.986 AUPR=0.463 time=54.85s
Epoch 136 | train_loss=0.1016 val_loss=0.0838 P=0.631 R=0.410 F1=0.497 ROC-AUC=0.986 AUPR=0.465 time=55.10s
Epoch 137 | train_loss=0.0877 val_loss=0.0844 P=0.573 R=0.420 F1=0.485 ROC-AUC=0.986 AUPR=0.462 time=55.09s
Epoch 138 | train_loss=0.1337 val_loss=0.0857 P=0.611 R=0.386 F1=0.473 ROC-AUC=0.986 AUPR=0.452 time=54.65s
Epoch 139 | train_loss=0.0992 val_loss=0.0917 P=0.539 R=0.390 F1=0.452 ROC-AUC=0.985 AUPR=0.426 time=52.90s
Epoch 140 | train_loss=0.1268 val_loss=0.0849 P=0.570 R=0.407 F1=0.475 ROC-AUC=0.986 AUPR=0.452 time=56.41s
Epoch 141 | train_loss=0.1314 val_loss=0.0856 P=0.510 R=0.440 F1=0.473 ROC-AUC=0.986 AUPR=0.455 time=55.66s
Epoch 142 | train_loss=0.0992 val_loss=0.0874 P=0.585 R=0.411 F1=0.483 ROC-AUC=0.986 AUPR=0.455 time=52.06s
Epoch 143 | train_loss=0.0969 val_loss=0.0848 P=0.543 R=0.418 F1=0.472 ROC-AUC=0.986 AUPR=0.457 time=49.76s
Epoch 144 | train_loss=0.1435 val_loss=0.0848 P=0.576 R=0.426 F1=0.489 ROC-AUC=0.986 AUPR=0.472 time=49.29s
Epoch 145 | train_loss=0.0851 val_loss=0.0829 P=0.547 R=0.447 F1=0.492 ROC-AUC=0.987 AUPR=0.465 time=49.18s
Epoch 146 | train_loss=0.1172 val_loss=0.0928 P=0.561 R=0.432 F1=0.488 ROC-AUC=0.985 AUPR=0.449 time=49.37s
Epoch 147 | train_loss=0.0908 val_loss=0.0888 P=0.663 R=0.388 F1=0.490 ROC-AUC=0.986 AUPR=0.467 time=49.54s
Epoch 148 | train_loss=0.1044 val_loss=0.0838 P=0.627 R=0.411 F1=0.497 ROC-AUC=0.986 AUPR=0.478 time=50.81s
Epoch 149 | train_loss=0.0959 val_loss=0.0854 P=0.601 R=0.412 F1=0.489 ROC-AUC=0.986 AUPR=0.467 time=54.86s
Epoch 150 | train_loss=0.0884 val_loss=0.4361 P=0.596 R=0.293 F1=0.393 ROC-AUC=0.884 AUPR=0.349 time=56.94s
Epoch 151 | train_loss=0.1150 val_loss=0.0886 P=0.574 R=0.416 F1=0.482 ROC-AUC=0.985 AUPR=0.457 time=53.53s
Epoch 152 | train_loss=0.1164 val_loss=0.0893 P=0.556 R=0.419 F1=0.478 ROC-AUC=0.985 AUPR=0.453 time=54.88s
Epoch 153 | train_loss=0.1201 val_loss=0.0866 P=0.579 R=0.393 F1=0.468 ROC-AUC=0.986 AUPR=0.464 time=53.71s
Epoch 154 | train_loss=0.1141 val_loss=0.1119 P=0.499 R=0.416 F1=0.454 ROC-AUC=0.978 AUPR=0.418 time=56.38s
Epoch 155 | train_loss=0.1394 val_loss=0.0852 P=0.679 R=0.372 F1=0.480 ROC-AUC=0.986 AUPR=0.473 time=54.99s
Epoch 156 | train_loss=0.1139 val_loss=0.0882 P=0.580 R=0.436 F1=0.498 ROC-AUC=0.985 AUPR=0.475 time=54.76s
Epoch 157 | train_loss=0.0885 val_loss=0.0875 P=0.538 R=0.451 F1=0.491 ROC-AUC=0.986 AUPR=0.470 time=54.28s
Epoch 158 | train_loss=0.1315 val_loss=0.0861 P=0.583 R=0.425 F1=0.491 ROC-AUC=0.986 AUPR=0.468 time=56.44s
Epoch 159 | train_loss=0.1443 val_loss=0.0854 P=0.569 R=0.429 F1=0.489 ROC-AUC=0.986 AUPR=0.461 time=56.27s
Epoch 160 | train_loss=0.1125 val_loss=0.0885 P=0.556 R=0.372 F1=0.445 ROC-AUC=0.985 AUPR=0.437 time=53.95s
Epoch 161 | train_loss=0.1074 val_loss=0.0845 P=0.530 R=0.434 F1=0.477 ROC-AUC=0.986 AUPR=0.466 time=55.90s
Epoch 162 | train_loss=0.1397 val_loss=0.0855 P=0.529 R=0.437 F1=0.479 ROC-AUC=0.986 AUPR=0.464 time=51.03s
Epoch 163 | train_loss=0.1211 val_loss=0.0888 P=0.507 R=0.445 F1=0.474 ROC-AUC=0.986 AUPR=0.451 time=53.52s
Epoch 164 | train_loss=0.1264 val_loss=0.0853 P=0.565 R=0.430 F1=0.488 ROC-AUC=0.986 AUPR=0.474 time=54.07s
Epoch 165 | train_loss=0.1199 val_loss=0.0853 P=0.545 R=0.427 F1=0.479 ROC-AUC=0.986 AUPR=0.467 time=56.04s
Epoch 166 | train_loss=0.1362 val_loss=0.0869 P=0.595 R=0.412 F1=0.487 ROC-AUC=0.986 AUPR=0.467 time=55.84s
Epoch 167 | train_loss=0.1365 val_loss=0.0998 P=0.535 R=0.426 F1=0.474 ROC-AUC=0.984 AUPR=0.453 time=50.61s
Epoch 168 | train_loss=0.1065 val_loss=0.0874 P=0.594 R=0.410 F1=0.485 ROC-AUC=0.986 AUPR=0.467 time=58.33s
Epoch 169 | train_loss=0.1152 val_loss=0.0888 P=0.569 R=0.423 F1=0.485 ROC-AUC=0.986 AUPR=0.468 time=50.78s
Epoch 170 | train_loss=0.1261 val_loss=0.0847 P=0.593 R=0.404 F1=0.481 ROC-AUC=0.986 AUPR=0.477 time=56.05s
Epoch 171 | train_loss=0.1073 val_loss=0.0855 P=0.592 R=0.417 F1=0.489 ROC-AUC=0.986 AUPR=0.477 time=54.77s
Epoch 172 | train_loss=0.0894 val_loss=0.0874 P=0.539 R=0.445 F1=0.487 ROC-AUC=0.986 AUPR=0.472 time=51.60s
Epoch 173 | train_loss=0.1367 val_loss=0.1188 P=0.575 R=0.388 F1=0.463 ROC-AUC=0.975 AUPR=0.442 time=50.95s

Early stopping at epoch 173

Total training time: 9311.61s (155.2 min)

Loading best model from epoch 148...
Evaluating final model...
======================================================================
EVALUATION RESULTS: RAT_high_GraphSAGE_T TRAIN
======================================================================

STANDARD METRICS:
  Precision:        0.6080
  Recall:           0.4604
  F1-Score:         0.5240
  ROC-AUC:          0.9920
  AUPR:             0.5201
  Balanced Acc:     0.7300

IMBALANCED-AWARE METRICS:
  MCC:              0.5287
  Cohen Kappa:      0.5236
  Specificity:      0.9997

THRESHOLD: 0.980

CONFUSION MATRIX:
  True Negatives:   3043021
  False Positives:  922
  False Negatives:  1676
  True Positives:   1430

TOP-K PRECISION:
  precision_at_100: 0.9900
  precision_at_500: 0.9460
  precision_at_1000: 0.8670
======================================================================
======================================================================
EVALUATION RESULTS: RAT_high_GraphSAGE_T VAL
======================================================================

STANDARD METRICS:
  Precision:        0.6274
  Recall:           0.4112
  F1-Score:         0.4968
  ROC-AUC:          0.9859
  AUPR:             0.4779
  Balanced Acc:     0.7055

IMBALANCED-AWARE METRICS:
  MCC:              0.5075
  Cohen Kappa:      0.4964
  Specificity:      0.9998

THRESHOLD: 0.985

CONFUSION MATRIX:
  True Negatives:   1014394
  False Positives:  253
  False Negatives:  610
  True Positives:   426

TOP-K PRECISION:
  precision_at_100: 0.9500
  precision_at_500: 0.7360
  precision_at_1000: 0.4990
======================================================================
======================================================================
EVALUATION RESULTS: RAT_high_GraphSAGE_T TEST
======================================================================

STANDARD METRICS:
  Precision:        0.6309
  Recall:           0.3981
  F1-Score:         0.4882
  ROC-AUC:          0.9852
  AUPR:             0.4664
  Balanced Acc:     0.6989

IMBALANCED-AWARE METRICS:
  MCC:              0.5008
  Cohen Kappa:      0.4877
  Specificity:      0.9998

THRESHOLD: 0.985

CONFUSION MATRIX:
  True Negatives:   1014407
  False Positives:  241
  False Negatives:  623
  True Positives:   412

TOP-K PRECISION:
  precision_at_100: 0.9600
  precision_at_500: 0.7260
  precision_at_1000: 0.4830
======================================================================

Saving results...
 Saved metrics to: C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\results\HI-Small_Trans_RAT_high\graphsage_t\metrics.json
 Saved prediction probabilities
 Saved experiment config

======================================================================
GraphSAGE-T Training Complete!
Results saved to: C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\results\HI-Small_Trans_RAT_high\graphsage_t
======================================================================

================================================================================
Logging ended at: 2025-11-28 22:38:32
================================================================================
