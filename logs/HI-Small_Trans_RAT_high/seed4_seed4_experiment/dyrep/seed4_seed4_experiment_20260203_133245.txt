
================================================================================
Logging started at: 2026-02-03 13:32:45.822959
Log file: C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\logs\HI-Small_Trans_RAT_high\seed4_seed4_experiment\dyrep\seed4_seed4_experiment_20260203_133245.txt
================================================================================

[INFO] Logging to: C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\logs\HI-Small_Trans_RAT_high\seed4_seed4_experiment\dyrep\seed4_seed4_experiment_20260203_133245.txt
================================================================================
EXPERIMENT CONFIG SUMMARY
================================================================================

[DATASET]
  Name:       HI-Small_Trans_RAT_high
  Theory:     RAT
  Intensity:  high

[MODEL]
  DyRep
  Hidden dim: 128

[PATHS]
  Graphs:     C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\graphs\HI-Small_Trans_RAT_high
  Splits:     C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\splits\HI-Small_Trans_RAT_high
  Results:    C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\results\HI-Small_Trans_RAT_high\seed4_seed4_experiment\dyrep
  Logs:       C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\logs\HI-Small_Trans_RAT_high\seed4_seed4_experiment\dyrep
================================================================================

[DEVICE] cuda

[INFO] Using default graph directory: C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\graphs_dyrep\HI-Small_Trans_RAT_high

======= DYREP-STYLE EVENT MODEL TRAINING (FP32) =======
Graph folder:    C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\graphs_dyrep\HI-Small_Trans_RAT_high
Splits folder:   C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\splits_dyrep\HI-Small_Trans_RAT_high
Results folder:  C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\results\HI-Small_Trans_RAT_high\seed4_seed4_experiment\dyrep
Batch size:      8192
Eval batch size: 16384
Device:          cuda
=======================================================

Num nodes:        515,080
Num events/edges: 5,078,415
Node feat dim:    12
Edge feat dim:    58
Event types:      7

Split sizes:
  Train: 3,047,049
  Val:   1,015,683
  Test:  1,015,683

pos_weight (train) = 100.000

Starting training for up to 350 epochs...

Epoch 001 | train_loss=8.7851 val_loss=0.3402 P=0.040 R=0.096 F1=0.057 ROC-AUC=0.852 AUPR=0.022 time=22.35s
Epoch 002 | train_loss=0.7853 val_loss=0.5454 P=0.064 R=0.196 F1=0.097 ROC-AUC=0.889 AUPR=0.043 time=21.82s
Epoch 003 | train_loss=1.1411 val_loss=2.1773 P=0.061 R=0.262 F1=0.100 ROC-AUC=0.849 AUPR=0.044 time=22.39s
Epoch 004 | train_loss=0.5017 val_loss=0.2946 P=0.009 R=0.612 F1=0.017 ROC-AUC=0.919 AUPR=0.007 time=23.27s
Epoch 005 | train_loss=0.5434 val_loss=0.3844 P=0.131 R=0.215 F1=0.163 ROC-AUC=0.906 AUPR=0.081 time=22.83s
Epoch 006 | train_loss=0.6208 val_loss=1.9000 P=0.173 R=0.224 F1=0.195 ROC-AUC=0.864 AUPR=0.112 time=22.86s
Epoch 007 | train_loss=0.3961 val_loss=2.8630 P=0.144 R=0.189 F1=0.163 ROC-AUC=0.858 AUPR=0.080 time=22.30s
Epoch 008 | train_loss=0.4440 val_loss=0.1335 P=0.166 R=0.259 F1=0.202 ROC-AUC=0.975 AUPR=0.117 time=23.31s
Epoch 009 | train_loss=0.6418 val_loss=2.4607 P=0.157 R=0.259 F1=0.195 ROC-AUC=0.865 AUPR=0.119 time=22.00s
Epoch 010 | train_loss=0.6396 val_loss=0.1281 P=0.225 R=0.208 F1=0.216 ROC-AUC=0.973 AUPR=0.135 time=21.98s
Epoch 011 | train_loss=0.3966 val_loss=0.5121 P=0.196 R=0.228 F1=0.211 ROC-AUC=0.908 AUPR=0.136 time=22.33s
Epoch 012 | train_loss=0.3180 val_loss=1.0778 P=0.275 R=0.232 F1=0.252 ROC-AUC=0.875 AUPR=0.173 time=22.87s
Epoch 013 | train_loss=0.5019 val_loss=0.1185 P=0.220 R=0.229 F1=0.224 ROC-AUC=0.977 AUPR=0.153 time=23.06s
Epoch 014 | train_loss=0.4211 val_loss=1.6309 P=0.220 R=0.241 F1=0.230 ROC-AUC=0.867 AUPR=0.141 time=22.23s
Epoch 015 | train_loss=0.2234 val_loss=0.1152 P=0.253 R=0.243 F1=0.248 ROC-AUC=0.979 AUPR=0.194 time=22.68s
Epoch 016 | train_loss=0.2391 val_loss=0.1128 P=0.277 R=0.260 F1=0.268 ROC-AUC=0.980 AUPR=0.205 time=22.48s
Epoch 017 | train_loss=0.1845 val_loss=0.1162 P=0.266 R=0.254 F1=0.260 ROC-AUC=0.977 AUPR=0.196 time=22.71s
Epoch 018 | train_loss=0.1853 val_loss=0.1143 P=0.228 R=0.231 F1=0.229 ROC-AUC=0.979 AUPR=0.156 time=22.96s
Epoch 019 | train_loss=0.2671 val_loss=0.1137 P=0.285 R=0.257 F1=0.270 ROC-AUC=0.979 AUPR=0.197 time=23.40s
Epoch 020 | train_loss=0.1858 val_loss=0.1159 P=0.221 R=0.293 F1=0.252 ROC-AUC=0.977 AUPR=0.186 time=23.49s
Epoch 021 | train_loss=0.3933 val_loss=0.1143 P=0.234 R=0.276 F1=0.253 ROC-AUC=0.979 AUPR=0.181 time=23.05s
Epoch 022 | train_loss=0.1154 val_loss=0.1065 P=0.258 R=0.319 F1=0.285 ROC-AUC=0.981 AUPR=0.223 time=22.92s
Epoch 023 | train_loss=0.3057 val_loss=0.1088 P=0.301 R=0.298 F1=0.299 ROC-AUC=0.980 AUPR=0.235 time=22.39s
Epoch 024 | train_loss=0.1340 val_loss=0.1113 P=0.321 R=0.235 F1=0.271 ROC-AUC=0.980 AUPR=0.206 time=22.78s
Epoch 025 | train_loss=0.1343 val_loss=0.1130 P=0.337 R=0.258 F1=0.292 ROC-AUC=0.979 AUPR=0.227 time=22.67s
Epoch 026 | train_loss=0.1433 val_loss=0.1021 P=0.322 R=0.299 F1=0.310 ROC-AUC=0.982 AUPR=0.254 time=22.57s
Epoch 027 | train_loss=0.1222 val_loss=0.1087 P=0.336 R=0.241 F1=0.281 ROC-AUC=0.980 AUPR=0.216 time=22.99s
Epoch 028 | train_loss=0.1795 val_loss=0.1053 P=0.291 R=0.281 F1=0.286 ROC-AUC=0.982 AUPR=0.224 time=22.52s
Epoch 029 | train_loss=0.1340 val_loss=0.3552 P=0.353 R=0.275 F1=0.310 ROC-AUC=0.913 AUPR=0.251 time=23.37s
Epoch 030 | train_loss=0.1643 val_loss=0.1045 P=0.341 R=0.245 F1=0.285 ROC-AUC=0.982 AUPR=0.223 time=22.74s
Epoch 031 | train_loss=0.2441 val_loss=0.1046 P=0.280 R=0.274 F1=0.277 ROC-AUC=0.981 AUPR=0.218 time=22.07s
Epoch 032 | train_loss=0.0948 val_loss=0.1058 P=0.295 R=0.317 F1=0.305 ROC-AUC=0.982 AUPR=0.249 time=22.53s
Epoch 033 | train_loss=0.1515 val_loss=0.1040 P=0.285 R=0.290 F1=0.288 ROC-AUC=0.982 AUPR=0.239 time=22.86s
Epoch 034 | train_loss=0.1376 val_loss=0.1522 P=0.371 R=0.241 F1=0.292 ROC-AUC=0.967 AUPR=0.209 time=24.20s
Epoch 035 | train_loss=0.1270 val_loss=0.3043 P=0.300 R=0.317 F1=0.308 ROC-AUC=0.916 AUPR=0.259 time=22.73s
Epoch 036 | train_loss=0.1227 val_loss=0.1270 P=0.249 R=0.233 F1=0.241 ROC-AUC=0.976 AUPR=0.177 time=22.46s
Epoch 037 | train_loss=0.1078 val_loss=0.1122 P=0.290 R=0.265 F1=0.277 ROC-AUC=0.979 AUPR=0.221 time=22.37s
Epoch 038 | train_loss=0.1958 val_loss=0.1010 P=0.323 R=0.267 F1=0.292 ROC-AUC=0.983 AUPR=0.233 time=23.06s
Epoch 039 | train_loss=0.1174 val_loss=0.1047 P=0.322 R=0.255 F1=0.285 ROC-AUC=0.982 AUPR=0.241 time=22.52s
Epoch 040 | train_loss=0.1125 val_loss=7.1614 P=0.008 R=0.935 F1=0.017 ROC-AUC=0.900 AUPR=0.005 time=23.38s
Epoch 041 | train_loss=0.1115 val_loss=0.1073 P=0.358 R=0.255 F1=0.298 ROC-AUC=0.982 AUPR=0.243 time=22.45s
Epoch 042 | train_loss=0.1479 val_loss=0.1011 P=0.380 R=0.251 F1=0.303 ROC-AUC=0.984 AUPR=0.256 time=23.12s
Epoch 043 | train_loss=0.0882 val_loss=0.1024 P=0.298 R=0.253 F1=0.274 ROC-AUC=0.982 AUPR=0.227 time=23.47s
Epoch 044 | train_loss=0.1078 val_loss=0.1072 P=0.305 R=0.293 F1=0.299 ROC-AUC=0.981 AUPR=0.248 time=23.12s
Epoch 045 | train_loss=0.0917 val_loss=0.1004 P=0.333 R=0.300 F1=0.316 ROC-AUC=0.983 AUPR=0.279 time=22.67s
Epoch 046 | train_loss=0.1122 val_loss=0.1060 P=0.384 R=0.267 F1=0.315 ROC-AUC=0.981 AUPR=0.264 time=22.91s
Epoch 047 | train_loss=0.0804 val_loss=0.1001 P=0.409 R=0.269 F1=0.325 ROC-AUC=0.983 AUPR=0.277 time=23.13s
Epoch 048 | train_loss=0.0973 val_loss=0.1005 P=0.351 R=0.286 F1=0.315 ROC-AUC=0.983 AUPR=0.274 time=23.00s
Epoch 049 | train_loss=0.0906 val_loss=0.1066 P=0.443 R=0.246 F1=0.316 ROC-AUC=0.981 AUPR=0.265 time=23.17s
Epoch 050 | train_loss=0.0767 val_loss=0.1022 P=0.358 R=0.291 F1=0.321 ROC-AUC=0.983 AUPR=0.272 time=22.75s
Epoch 051 | train_loss=0.0829 val_loss=0.0990 P=0.373 R=0.278 F1=0.319 ROC-AUC=0.984 AUPR=0.270 time=22.56s
Epoch 052 | train_loss=0.0837 val_loss=0.1038 P=0.343 R=0.280 F1=0.308 ROC-AUC=0.982 AUPR=0.262 time=22.77s
Epoch 053 | train_loss=0.1789 val_loss=0.1162 P=0.273 R=0.287 F1=0.279 ROC-AUC=0.979 AUPR=0.232 time=24.02s
Epoch 054 | train_loss=0.1604 val_loss=0.1039 P=0.391 R=0.290 F1=0.333 ROC-AUC=0.981 AUPR=0.289 time=22.99s
Epoch 055 | train_loss=0.0940 val_loss=0.1024 P=0.405 R=0.264 F1=0.320 ROC-AUC=0.982 AUPR=0.277 time=22.48s
Epoch 056 | train_loss=0.2276 val_loss=0.4447 P=0.334 R=0.301 F1=0.317 ROC-AUC=0.917 AUPR=0.282 time=22.55s
Epoch 057 | train_loss=0.0860 val_loss=0.1107 P=0.319 R=0.252 F1=0.282 ROC-AUC=0.981 AUPR=0.233 time=22.19s
Epoch 058 | train_loss=0.1121 val_loss=0.0966 P=0.460 R=0.270 F1=0.340 ROC-AUC=0.984 AUPR=0.303 time=22.61s
Epoch 059 | train_loss=0.0831 val_loss=0.1069 P=0.394 R=0.270 F1=0.320 ROC-AUC=0.981 AUPR=0.276 time=23.23s
Epoch 060 | train_loss=0.1011 val_loss=0.1044 P=0.381 R=0.301 F1=0.336 ROC-AUC=0.983 AUPR=0.290 time=22.30s
Epoch 061 | train_loss=0.0952 val_loss=0.5261 P=0.336 R=0.281 F1=0.306 ROC-AUC=0.913 AUPR=0.267 time=22.33s
Epoch 062 | train_loss=0.0865 val_loss=0.1079 P=0.364 R=0.267 F1=0.308 ROC-AUC=0.981 AUPR=0.269 time=22.44s
Epoch 063 | train_loss=0.1016 val_loss=0.1031 P=0.304 R=0.312 F1=0.308 ROC-AUC=0.983 AUPR=0.276 time=22.57s
Epoch 064 | train_loss=0.0727 val_loss=0.1131 P=0.409 R=0.290 F1=0.339 ROC-AUC=0.982 AUPR=0.305 time=23.31s
Epoch 065 | train_loss=0.0802 val_loss=0.1077 P=0.382 R=0.258 F1=0.308 ROC-AUC=0.981 AUPR=0.271 time=23.20s
Epoch 066 | train_loss=0.0840 val_loss=0.1134 P=0.361 R=0.277 F1=0.314 ROC-AUC=0.979 AUPR=0.275 time=23.01s
Epoch 067 | train_loss=0.0768 val_loss=0.1303 P=0.342 R=0.274 F1=0.305 ROC-AUC=0.976 AUPR=0.251 time=23.17s
Epoch 068 | train_loss=0.0966 val_loss=0.1067 P=0.390 R=0.294 F1=0.335 ROC-AUC=0.982 AUPR=0.301 time=23.22s
Epoch 069 | train_loss=0.0710 val_loss=0.1063 P=0.336 R=0.299 F1=0.316 ROC-AUC=0.982 AUPR=0.284 time=22.76s
Epoch 070 | train_loss=0.0894 val_loss=0.1149 P=0.369 R=0.262 F1=0.307 ROC-AUC=0.979 AUPR=0.267 time=22.93s
Epoch 071 | train_loss=0.0694 val_loss=0.1075 P=0.411 R=0.283 F1=0.335 ROC-AUC=0.981 AUPR=0.293 time=22.86s
Epoch 072 | train_loss=0.0697 val_loss=0.1021 P=0.466 R=0.312 F1=0.374 ROC-AUC=0.982 AUPR=0.344 time=22.65s
Epoch 073 | train_loss=0.0669 val_loss=0.1319 P=0.319 R=0.262 F1=0.288 ROC-AUC=0.976 AUPR=0.263 time=22.53s
Epoch 074 | train_loss=0.1316 val_loss=0.1302 P=0.290 R=0.262 F1=0.275 ROC-AUC=0.975 AUPR=0.241 time=22.85s
Epoch 075 | train_loss=0.0758 val_loss=0.1127 P=0.414 R=0.325 F1=0.364 ROC-AUC=0.979 AUPR=0.342 time=22.94s
Epoch 076 | train_loss=0.0860 val_loss=0.1250 P=0.465 R=0.251 F1=0.326 ROC-AUC=0.977 AUPR=0.293 time=22.65s
Epoch 077 | train_loss=0.1237 val_loss=0.1013 P=0.464 R=0.312 F1=0.373 ROC-AUC=0.983 AUPR=0.344 time=23.15s
Epoch 078 | train_loss=0.0656 val_loss=0.1251 P=0.389 R=0.280 F1=0.326 ROC-AUC=0.977 AUPR=0.295 time=21.84s
Epoch 079 | train_loss=0.0646 val_loss=0.1204 P=0.464 R=0.275 F1=0.346 ROC-AUC=0.978 AUPR=0.304 time=23.41s
Epoch 080 | train_loss=0.1429 val_loss=0.1086 P=0.455 R=0.329 F1=0.382 ROC-AUC=0.980 AUPR=0.352 time=23.73s
Epoch 081 | train_loss=0.0648 val_loss=0.1575 P=0.347 R=0.310 F1=0.327 ROC-AUC=0.970 AUPR=0.287 time=22.68s
Epoch 082 | train_loss=0.1101 val_loss=0.1236 P=0.384 R=0.305 F1=0.340 ROC-AUC=0.977 AUPR=0.306 time=23.29s
Epoch 083 | train_loss=0.2073 val_loss=0.1457 P=0.400 R=0.291 F1=0.337 ROC-AUC=0.972 AUPR=0.314 time=22.60s
Epoch 084 | train_loss=0.0595 val_loss=0.1172 P=0.448 R=0.334 F1=0.382 ROC-AUC=0.977 AUPR=0.358 time=23.16s
Epoch 085 | train_loss=0.1003 val_loss=0.1702 P=0.351 R=0.276 F1=0.309 ROC-AUC=0.966 AUPR=0.267 time=22.74s
Epoch 086 | train_loss=0.0622 val_loss=0.1465 P=0.415 R=0.352 F1=0.381 ROC-AUC=0.971 AUPR=0.349 time=22.90s
Epoch 087 | train_loss=0.0670 val_loss=0.1422 P=0.366 R=0.304 F1=0.332 ROC-AUC=0.972 AUPR=0.310 time=22.43s
Epoch 088 | train_loss=0.1904 val_loss=0.1262 P=0.412 R=0.303 F1=0.349 ROC-AUC=0.975 AUPR=0.314 time=22.72s
Epoch 089 | train_loss=0.0808 val_loss=0.1432 P=0.387 R=0.337 F1=0.361 ROC-AUC=0.971 AUPR=0.333 time=22.64s
Epoch 090 | train_loss=0.0755 val_loss=0.1344 P=0.337 R=0.341 F1=0.339 ROC-AUC=0.973 AUPR=0.303 time=22.98s
Epoch 091 | train_loss=0.1451 val_loss=0.1431 P=0.418 R=0.305 F1=0.353 ROC-AUC=0.972 AUPR=0.315 time=22.59s
Epoch 092 | train_loss=0.1365 val_loss=0.1244 P=0.439 R=0.355 F1=0.392 ROC-AUC=0.981 AUPR=0.364 time=22.79s
Epoch 093 | train_loss=0.1321 val_loss=0.1388 P=0.410 R=0.319 F1=0.359 ROC-AUC=0.972 AUPR=0.314 time=22.74s
Epoch 094 | train_loss=0.1130 val_loss=0.1173 P=0.417 R=0.293 F1=0.344 ROC-AUC=0.977 AUPR=0.314 time=22.76s
Epoch 095 | train_loss=0.1215 val_loss=1.9333 P=0.349 R=0.297 F1=0.321 ROC-AUC=0.871 AUPR=0.281 time=23.14s
Epoch 096 | train_loss=0.1481 val_loss=0.2112 P=0.354 R=0.240 F1=0.286 ROC-AUC=0.956 AUPR=0.217 time=22.56s
Epoch 097 | train_loss=0.2226 val_loss=0.1648 P=0.362 R=0.267 F1=0.307 ROC-AUC=0.966 AUPR=0.267 time=22.78s
Epoch 098 | train_loss=0.0943 val_loss=0.1271 P=0.400 R=0.320 F1=0.355 ROC-AUC=0.975 AUPR=0.323 time=23.05s
Epoch 099 | train_loss=0.1080 val_loss=0.1920 P=0.314 R=0.307 F1=0.311 ROC-AUC=0.961 AUPR=0.249 time=22.58s
Epoch 100 | train_loss=0.0687 val_loss=0.1927 P=0.435 R=0.259 F1=0.324 ROC-AUC=0.961 AUPR=0.279 time=22.79s
Epoch 101 | train_loss=0.2920 val_loss=0.1782 P=0.325 R=0.294 F1=0.309 ROC-AUC=0.962 AUPR=0.262 time=22.45s
Epoch 102 | train_loss=0.1525 val_loss=0.2153 P=0.373 R=0.274 F1=0.316 ROC-AUC=0.955 AUPR=0.256 time=22.79s
Epoch 103 | train_loss=0.3296 val_loss=0.7232 P=0.390 R=0.261 F1=0.312 ROC-AUC=0.910 AUPR=0.263 time=22.45s
Epoch 104 | train_loss=0.2187 val_loss=0.1284 P=0.374 R=0.289 F1=0.326 ROC-AUC=0.975 AUPR=0.298 time=22.58s
Epoch 105 | train_loss=0.1137 val_loss=0.1648 P=0.360 R=0.282 F1=0.316 ROC-AUC=0.966 AUPR=0.270 time=23.47s
Epoch 106 | train_loss=0.0990 val_loss=0.1206 P=0.479 R=0.378 F1=0.423 ROC-AUC=0.978 AUPR=0.390 time=24.41s
Epoch 107 | train_loss=0.3110 val_loss=0.1362 P=0.540 R=0.308 F1=0.392 ROC-AUC=0.973 AUPR=0.348 time=24.14s
Epoch 108 | train_loss=0.4495 val_loss=0.1540 P=0.411 R=0.321 F1=0.360 ROC-AUC=0.968 AUPR=0.314 time=22.75s
Epoch 109 | train_loss=0.1294 val_loss=0.2296 P=0.289 R=0.290 F1=0.289 ROC-AUC=0.951 AUPR=0.222 time=22.52s
Epoch 110 | train_loss=0.3093 val_loss=0.1277 P=0.468 R=0.299 F1=0.365 ROC-AUC=0.976 AUPR=0.332 time=24.19s
Epoch 111 | train_loss=0.3370 val_loss=0.1952 P=0.473 R=0.261 F1=0.336 ROC-AUC=0.959 AUPR=0.281 time=23.53s
Epoch 112 | train_loss=0.8516 val_loss=0.1760 P=0.342 R=0.292 F1=0.315 ROC-AUC=0.963 AUPR=0.273 time=22.67s
Epoch 113 | train_loss=0.2900 val_loss=1.9385 P=0.279 R=0.223 F1=0.248 ROC-AUC=0.868 AUPR=0.193 time=23.72s
Epoch 114 | train_loss=0.1325 val_loss=0.1961 P=0.305 R=0.320 F1=0.312 ROC-AUC=0.958 AUPR=0.258 time=22.85s
Epoch 115 | train_loss=0.0701 val_loss=0.2015 P=0.293 R=0.272 F1=0.282 ROC-AUC=0.959 AUPR=0.218 time=22.68s
Epoch 116 | train_loss=0.1730 val_loss=0.2460 P=0.311 R=0.274 F1=0.291 ROC-AUC=0.948 AUPR=0.223 time=22.38s
Epoch 117 | train_loss=0.2121 val_loss=0.1475 P=0.275 R=0.311 F1=0.292 ROC-AUC=0.968 AUPR=0.251 time=22.76s
Epoch 118 | train_loss=0.1328 val_loss=0.2312 P=0.284 R=0.253 F1=0.268 ROC-AUC=0.949 AUPR=0.206 time=23.22s
Epoch 119 | train_loss=0.1854 val_loss=11.3121 P=0.290 R=0.243 F1=0.264 ROC-AUC=0.865 AUPR=0.211 time=23.07s
Epoch 120 | train_loss=0.3970 val_loss=0.2411 P=0.343 R=0.245 F1=0.286 ROC-AUC=0.949 AUPR=0.220 time=22.79s
Epoch 121 | train_loss=0.0960 val_loss=0.1614 P=0.340 R=0.266 F1=0.299 ROC-AUC=0.969 AUPR=0.248 time=22.71s

Early stopping at epoch 121 (no val AUPR improvement for 15 epochs)

Total training time: 2779.51s (46.3 min)

Loading best model from epoch 106...
Evaluating final model on train/val/test...
======================================================================
EVALUATION RESULTS: seed4_seed4_experiment TRAIN
======================================================================

STANDARD METRICS:
  Precision:        0.7587
  Recall:           0.7581
  F1-Score:         0.7584
  ROC-AUC:          0.9919
  AUPR:             0.7405
  Balanced Acc:     0.8789

IMBALANCED-AWARE METRICS:
  MCC:              0.7582
  Cohen Kappa:      0.7582
  Specificity:      0.9998

THRESHOLD: 0.916

CONFUSION MATRIX:
  True Negatives:   3044197
  False Positives:  554
  False Negatives:  556
  True Positives:   1742

TOP-K PRECISION:
  precision_at_100: 1.0000
  precision_at_500: 0.9460
  precision_at_1000: 0.8990
======================================================================
======================================================================
EVALUATION RESULTS: seed4_seed4_experiment VAL
======================================================================

STANDARD METRICS:
  Precision:        0.4789
  Recall:           0.3780
  F1-Score:         0.4225
  ROC-AUC:          0.9778
  AUPR:             0.3896
  Balanced Acc:     0.6888

IMBALANCED-AWARE METRICS:
  MCC:              0.4249
  Cohen Kappa:      0.4220
  Specificity:      0.9996

THRESHOLD: 0.832

CONFUSION MATRIX:
  True Negatives:   1014156
  False Positives:  445
  False Negatives:  673
  True Positives:   409

TOP-K PRECISION:
  precision_at_100: 0.9800
  precision_at_500: 0.6480
  precision_at_1000: 0.4350
======================================================================
======================================================================
EVALUATION RESULTS: seed4_seed4_experiment TEST
======================================================================

STANDARD METRICS:
  Precision:        0.5080
  Recall:           0.4235
  F1-Score:         0.4619
  ROC-AUC:          0.9822
  AUPR:             0.4506
  Balanced Acc:     0.7114

IMBALANCED-AWARE METRICS:
  MCC:              0.4630
  Cohen Kappa:      0.4610
  Specificity:      0.9993

THRESHOLD: 0.877

CONFUSION MATRIX:
  True Negatives:   1013149
  False Positives:  737
  False Negatives:  1036
  True Positives:   761

TOP-K PRECISION:
  precision_at_100: 0.9600
  precision_at_500: 0.8020
  precision_at_1000: 0.6120
======================================================================

Saving results...
  - Saved metrics to: C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\results\HI-Small_Trans_RAT_high\seed4_seed4_experiment\dyrep\metrics.json
  - Saved prediction probabilities
  - Saved experiment config

======================================================================
DyRep-style Training Complete!
Results saved to: C:\Users\yasmi\OneDrive\Desktop\Uni - Master's\Fall 2025\MLR 570\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\results\HI-Small_Trans_RAT_high\seed4_seed4_experiment\dyrep
======================================================================

================================================================================
Logging ended at: 2026-02-03 14:20:20.785149
================================================================================
