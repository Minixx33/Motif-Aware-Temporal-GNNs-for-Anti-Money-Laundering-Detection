{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d39ea1d8",
   "metadata": {},
   "source": [
    "# Training a PyG Temporal Graph\n",
    "\n",
    "This notebook is for training a PyTorch Geometric Temporal (PYG-TGN/TGAT)\n",
    "\n",
    "> !NOTE\n",
    "> MUST REFERENCE THIS PAPER: https://arxiv.org/abs/2006.10637"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caacdc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root directory: c:\\Users\\yasmi\\OneDrive\\Desktop\\Uni - Master's\\Fall 2025\\MLR 570\\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Go up 3 levels: rat ‚Üí ibm_transactions_scripts ‚Üí scripts ‚Üí ROOT\n",
    "ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), \"../../../\"))\n",
    "print(\"Root directory:\", ROOT_DIR)\n",
    "\n",
    "# Add root directory to Python path\n",
    "sys.path.append(ROOT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "269041c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Nov 23 00:10:20 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 581.29                 Driver Version: 581.29         CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4080 ...  WDDM  |   00000000:01:00.0  On |                  N/A |\n",
      "| N/A   56C    P5             27W /  159W |    1076MiB /  12282MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            2372    C+G   ....0.3595.90\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A            3732    C+G   ...N\\v4.2.2\\ProtonVPN.Client.exe      N/A      |\n",
      "|    0   N/A  N/A            4588    C+G   ...yApp\\MicrosoftSecurityApp.exe      N/A      |\n",
      "|    0   N/A  N/A            7004    C+G   ...AcrobatNotificationClient.exe      N/A      |\n",
      "|    0   N/A  N/A            8692    C+G   ...App_cw5n1h2txyewy\\LockApp.exe      N/A      |\n",
      "|    0   N/A  N/A           11376    C+G   ...4__8wekyb3d8bbwe\\ms-teams.exe      N/A      |\n",
      "|    0   N/A  N/A           14664    C+G   C:\\Windows\\explorer.exe               N/A      |\n",
      "|    0   N/A  N/A           15964    C+G   ...2txyewy\\CrossDeviceResume.exe      N/A      |\n",
      "|    0   N/A  N/A           17476    C+G   ..._cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
      "|    0   N/A  N/A           17628    C+G   ...y\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A           20060    C+G   ....0.3595.90\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A           20512    C+G   ....0.3595.90\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A           20944    C+G   ...1g1gvanyjgm\\WhatsApp.Root.exe      N/A      |\n",
      "|    0   N/A  N/A           21000    C+G   ...mba6cd70vzyy\\ArmouryCrate.exe      N/A      |\n",
      "|    0   N/A  N/A           24140    C+G   ...rvices\\BlueStacksServices.exe      N/A      |\n",
      "|    0   N/A  N/A           26456    C+G   ...8bbwe\\PhoneExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A           28392    C+G   ...lpaper_engine\\wallpaper32.exe      N/A      |\n",
      "|    0   N/A  N/A           30632    C+G   ...4__8wekyb3d8bbwe\\ms-teams.exe      N/A      |\n",
      "|    0   N/A  N/A           32612    C+G   ...64__zpdnekdrzrea0\\Spotify.exe      N/A      |\n",
      "|    0   N/A  N/A           34132    C+G   ...5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A           42868    C+G   ...ms\\Microsoft VS Code\\Code.exe      N/A      |\n",
      "|    0   N/A  N/A           44364    C+G   ...crosoft OneDrive\\OneDrive.exe      N/A      |\n",
      "|    0   N/A  N/A           51528    C+G   ...ogram Files\\Zotero\\zotero.exe      N/A      |\n",
      "|    0   N/A  N/A           57448    C+G   ....0.3595.94\\msedgewebview2.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "print(subprocess.getoutput(\"nvidia-smi\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a53ddabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.5.1+cu121\n",
      "CUDA version: 12.1\n",
      "Is CUDA available: True\n",
      "Device: NVIDIA GeForce RTX 4080 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"Is CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7560854d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yasmi\\AppData\\Local\\Temp\\ipykernel_15432\\1901266164.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  edge_index = torch.load(r\"C:\\Users\\yasmi\\OneDrive\\Desktop\\Uni - Master's\\Fall 2025\\MLR 570\\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\\ibm_transcations_datasets\\RAT\\pyg_graph_hismall\\edge_index.pt\")\n",
      "C:\\Users\\yasmi\\AppData\\Local\\Temp\\ipykernel_15432\\1901266164.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  x = torch.load(r\"C:\\Users\\yasmi\\OneDrive\\Desktop\\Uni - Master's\\Fall 2025\\MLR 570\\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\\ibm_transcations_datasets\\RAT\\pyg_graph_hismall\\x.pt\")\n",
      "C:\\Users\\yasmi\\AppData\\Local\\Temp\\ipykernel_15432\\1901266164.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  edge_attr = torch.load(r\"C:\\Users\\yasmi\\OneDrive\\Desktop\\Uni - Master's\\Fall 2025\\MLR 570\\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\\ibm_transcations_datasets\\RAT\\pyg_graph_hismall\\edge_attr.pt\")\n",
      "C:\\Users\\yasmi\\AppData\\Local\\Temp\\ipykernel_15432\\1901266164.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  timestamps = torch.load(r\"C:\\Users\\yasmi\\OneDrive\\Desktop\\Uni - Master's\\Fall 2025\\MLR 570\\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\\ibm_transcations_datasets\\RAT\\pyg_graph_hismall\\timestamps.pt\")\n",
      "C:\\Users\\yasmi\\AppData\\Local\\Temp\\ipykernel_15432\\1901266164.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  y_edge = torch.load(r\"C:\\Users\\yasmi\\OneDrive\\Desktop\\Uni - Master's\\Fall 2025\\MLR 570\\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\\ibm_transcations_datasets\\RAT\\pyg_graph_hismall\\y_edge.pt\")\n",
      "C:\\Users\\yasmi\\AppData\\Local\\Temp\\ipykernel_15432\\1901266164.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  y_node = torch.load(r\"C:\\Users\\yasmi\\OneDrive\\Desktop\\Uni - Master's\\Fall 2025\\MLR 570\\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\\ibm_transcations_datasets\\RAT\\pyg_graph_hismall\\y_node.pt\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tgn.modules.memory import Memory\n",
    "from tgn.model.tgn import TGN\n",
    "\n",
    "edge_index = torch.load(r\"C:\\Users\\yasmi\\OneDrive\\Desktop\\Uni - Master's\\Fall 2025\\MLR 570\\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\\ibm_transcations_datasets\\RAT\\pyg_graph_hismall\\edge_index.pt\")\n",
    "x = torch.load(r\"C:\\Users\\yasmi\\OneDrive\\Desktop\\Uni - Master's\\Fall 2025\\MLR 570\\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\\ibm_transcations_datasets\\RAT\\pyg_graph_hismall\\x.pt\")\n",
    "edge_attr = torch.load(r\"C:\\Users\\yasmi\\OneDrive\\Desktop\\Uni - Master's\\Fall 2025\\MLR 570\\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\\ibm_transcations_datasets\\RAT\\pyg_graph_hismall\\edge_attr.pt\")\n",
    "timestamps = torch.load(r\"C:\\Users\\yasmi\\OneDrive\\Desktop\\Uni - Master's\\Fall 2025\\MLR 570\\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\\ibm_transcations_datasets\\RAT\\pyg_graph_hismall\\timestamps.pt\")\n",
    "y_edge = torch.load(r\"C:\\Users\\yasmi\\OneDrive\\Desktop\\Uni - Master's\\Fall 2025\\MLR 570\\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\\ibm_transcations_datasets\\RAT\\pyg_graph_hismall\\y_edge.pt\")\n",
    "y_node = torch.load(r\"C:\\Users\\yasmi\\OneDrive\\Desktop\\Uni - Master's\\Fall 2025\\MLR 570\\Motif-Aware-Temporal-GNNs-for-Anti-Money-Laundering-Detection\\ibm_transcations_datasets\\RAT\\pyg_graph_hismall\\y_node.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37857ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge_index: torch.Size([2, 5082345])\n",
      "x: torch.Size([515080, 27])\n",
      "edge_attr: torch.Size([5082345, 3])\n",
      "timestamps: torch.Size([5082345])\n",
      "y_edge: torch.Size([5082345])\n",
      "y_node: torch.Size([515080])\n",
      "labels distribution: tensor([5073168,    9177])\n"
     ]
    }
   ],
   "source": [
    "print(\"edge_index:\", edge_index.shape)\n",
    "print(\"x:\", x.shape)\n",
    "print(\"edge_attr:\", edge_attr.shape)\n",
    "print(\"timestamps:\", timestamps.shape)\n",
    "print(\"y_edge:\", y_edge.shape)\n",
    "print(\"y_node:\", y_node.shape)\n",
    "\n",
    "print(\"labels distribution:\", y_edge.bincount())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7af1aa4",
   "metadata": {},
   "source": [
    "## Create a time-sorted view + train/val/test split\n",
    "\n",
    "TGN is temporal, so splitting must be chronological, not random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffbe3b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num edges (sorted): 5082345\n",
      "Train edges: 3557641\n",
      "Val edges: 762352\n",
      "Test edges: 762352\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Sort edges by time (important for temporal models)\n",
    "sorted_idx = torch.argsort(timestamps)\n",
    "\n",
    "edge_index_sorted = edge_index[:, sorted_idx]\n",
    "edge_attr_sorted  = edge_attr[sorted_idx]\n",
    "timestamps_sorted = timestamps[sorted_idx]\n",
    "y_edge_sorted     = y_edge[sorted_idx]\n",
    "\n",
    "num_edges = edge_index_sorted.size(1)\n",
    "print(\"Num edges (sorted):\", num_edges)\n",
    "\n",
    "# Temporal split: 70% train, 15% val, 15% test\n",
    "train_ratio = 0.7\n",
    "val_ratio   = 0.15\n",
    "\n",
    "train_end = int(train_ratio * num_edges)\n",
    "val_end   = int((train_ratio + val_ratio) * num_edges)\n",
    "\n",
    "train_slice = slice(0, train_end)\n",
    "val_slice   = slice(train_end, val_end)\n",
    "test_slice  = slice(val_end, num_edges)\n",
    "\n",
    "print(\"Train edges:\", train_end)\n",
    "print(\"Val edges:\", val_end - train_end)\n",
    "print(\"Test edges:\", num_edges - val_end)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f100b0",
   "metadata": {},
   "source": [
    "## Build a tiny helper ‚Äúdataset‚Äù class for TGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f0258a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeSequence:\n",
    "    \"\"\"\n",
    "    Thin wrapper around your sorted edge tensors.\n",
    "    This matches the typical TGN-style interface:\n",
    "      - sources\n",
    "      - destinations\n",
    "      - timestamps\n",
    "      - edge_features\n",
    "      - labels\n",
    "    \"\"\"\n",
    "    def __init__(self, edge_index, edge_attr, timestamps, labels):\n",
    "        # edge_index: [2, E]\n",
    "        self.sources      = edge_index[0]      # [E]\n",
    "        self.destinations = edge_index[1]      # [E]\n",
    "        self.timestamps   = timestamps         # [E]\n",
    "        self.edge_features = edge_attr         # [E, D_e]\n",
    "        self.labels       = labels             # [E]\n",
    "\n",
    "        self.num_edges = self.sources.size(0)\n",
    "\n",
    "    def get_batch(self, start, end):\n",
    "        \"\"\"\n",
    "        Return a slice batch [start:end] as\n",
    "        (src, dst, t, edge_feat, labels)\n",
    "        \"\"\"\n",
    "        s = slice(start, end)\n",
    "        return (self.sources[s],\n",
    "                self.destinations[s],\n",
    "                self.timestamps[s],\n",
    "                self.edge_features[s],\n",
    "                self.labels[s])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ac40379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train edges: 3557641\n",
      "Val edges: 762352\n",
      "Test edges: 762352\n"
     ]
    }
   ],
   "source": [
    "train_data = EdgeSequence(\n",
    "    edge_index_sorted[:, train_slice],\n",
    "    edge_attr_sorted[train_slice],\n",
    "    timestamps_sorted[train_slice],\n",
    "    y_edge_sorted[train_slice]\n",
    ")\n",
    "\n",
    "val_data = EdgeSequence(\n",
    "    edge_index_sorted[:, val_slice],\n",
    "    edge_attr_sorted[val_slice],\n",
    "    timestamps_sorted[val_slice],\n",
    "    y_edge_sorted[val_slice]\n",
    ")\n",
    "\n",
    "test_data = EdgeSequence(\n",
    "    edge_index_sorted[:, test_slice],\n",
    "    edge_attr_sorted[test_slice],\n",
    "    timestamps_sorted[test_slice],\n",
    "    y_edge_sorted[test_slice]\n",
    ")\n",
    "\n",
    "print(\"Train edges:\", train_data.num_edges)\n",
    "print(\"Val edges:\", val_data.num_edges)\n",
    "print(\"Test edges:\", test_data.num_edges)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69421d93",
   "metadata": {},
   "source": [
    "## Plugging into TGN implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffeba33",
   "metadata": {},
   "source": [
    "#### Creating a tiny MLP link predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "696353c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LinkPredictor(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(2 * in_dim, in_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, src_emb, dst_emb):\n",
    "        x = torch.cat([src_emb, dst_emb], dim=-1)\n",
    "        return torch.sigmoid(self.mlp(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea045770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Convert node & edge features to numpy for TGN\n",
    "node_features_np = x.cpu().numpy().astype(np.float32)\n",
    "edge_features_np = edge_attr_sorted.cpu().numpy().astype(np.float32)\n",
    "\n",
    "num_nodes = node_features_np.shape[0]\n",
    "num_edges = edge_features_np.shape[0]\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 1. Build NeighborFinder (REQUIRED BY TGN)\n",
    "# ----------------------------------------------------\n",
    "from tgn.utils.neighbor_finder import NeighborFinder\n",
    "\n",
    "src_np = edge_index_sorted[0].cpu().numpy()\n",
    "dst_np = edge_index_sorted[1].cpu().numpy()\n",
    "ts_np  = timestamps_sorted.cpu().numpy()\n",
    "eid_np = np.arange(num_edges)\n",
    "\n",
    "adj_list = [[] for _ in range(num_nodes)]\n",
    "for s, d, t, eid in zip(src_np, dst_np, ts_np, eid_np):\n",
    "    adj_list[s].append((d, eid, t))\n",
    "    adj_list[d].append((s, eid, t))   # IBM is undirected temporal\n",
    "\n",
    "neighbor_finder = NeighborFinder(adj_list, uniform=True)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 2. Create TGN model properly\n",
    "# ----------------------------------------------------\n",
    "from tgn.model.tgn import TGN\n",
    "\n",
    "feat_dim = node_features_np.shape[1]   # this is 27 in your case\n",
    "\n",
    "tgn = TGN(\n",
    "    neighbor_finder=neighbor_finder,\n",
    "    node_features=node_features_np,\n",
    "    edge_features=edge_features_np,\n",
    "    device=device,\n",
    "    n_layers=1,\n",
    "    n_heads=1,\n",
    "    dropout=0.1,\n",
    "    use_memory=False,            #  üî• TURN MEMORY OFF\n",
    "    embedding_module_type=\"graph_attention\",\n",
    "    message_function=\"mlp\",\n",
    "    n_neighbors=5,\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 3. Link predictor MLP\n",
    "# ----------------------------------------------------\n",
    "class LinkPredictor(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(emb_dim * 2, emb_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(emb_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, src, dst):\n",
    "        return self.mlp(torch.cat([src, dst], dim=-1)).squeeze(-1)\n",
    "\n",
    "link_predictor = LinkPredictor(emb_dim=node_features_np.shape[1]).to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(list(tgn.parameters()) + list(link_predictor.parameters()), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c90a9da",
   "metadata": {},
   "source": [
    "## Building training dataset with negative sampling\n",
    "\n",
    "TGN requires training triplets:\n",
    "(source_node, positive_destination_node, negative_destination_node)\n",
    "at timestamp t\n",
    "\n",
    "This means we need to construct:\n",
    "\n",
    "- src_np ‚Äî source nodes\n",
    "\n",
    "- dst_np ‚Äî destination nodes\n",
    "\n",
    "- neg_dst_np ‚Äî randomly sampled negative nodes\n",
    "\n",
    "- ts_np ‚Äî timestamps\n",
    "\n",
    "- eid_np ‚Äî edge indices\n",
    "\n",
    "- y_np ‚Äî labels (1 for real edges, 0 for negative edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8936a4f4",
   "metadata": {},
   "source": [
    "#### Creating training triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6de39b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_edges = edge_index_sorted.shape[1]\n",
    "num_nodes = x.shape[0]\n",
    "\n",
    "# Positive edges\n",
    "src = edge_index_sorted[0].cpu().numpy()\n",
    "dst = edge_index_sorted[1].cpu().numpy()\n",
    "ts  = timestamps_sorted.cpu().numpy()\n",
    "eid = np.arange(num_edges)\n",
    "\n",
    "# Negative sampling: sample random destination nodes\n",
    "neg_dst = np.random.randint(0, num_nodes, size=num_edges)\n",
    "\n",
    "# Convert to tensors for TGN\n",
    "src_t = torch.from_numpy(src).long().to(device)\n",
    "dst_t = torch.from_numpy(dst).long().to(device)\n",
    "neg_t = torch.from_numpy(neg_dst).long().to(device)\n",
    "ts_t  = torch.from_numpy(ts).float().to(device)\n",
    "eid_t = torch.from_numpy(eid).long().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e58b2c0",
   "metadata": {},
   "source": [
    "### Build a DataLoader for batching\n",
    "TGN must process interactions in temporal order, so a simple sequential batch loader works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3bb55344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_batch_indices(batch_size, total):\n",
    "    return [(i, min(i+batch_size, total)) for i in range(0, total, batch_size)]\n",
    "\n",
    "batches = get_batch_indices(20000, num_edges)\n",
    "len(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1921dab",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b2a7471a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 20,000 edges for debug training\n",
      "Total batches per epoch: 20\n",
      "\n",
      "=== Epoch 1/15 ===\n",
      "  Batch 1/20 [edges 0:1000] loss=0.6715  time=2.345s\n",
      "  Batch 2/20 [edges 1000:2000] loss=0.6617  time=1.997s\n",
      "  Batch 3/20 [edges 2000:3000] loss=0.6684  time=1.191s\n",
      "  Batch 4/20 [edges 3000:4000] loss=0.6678  time=2.654s\n",
      "  Batch 5/20 [edges 4000:5000] loss=0.6733  time=1.388s\n",
      "  Batch 6/20 [edges 5000:6000] loss=0.6714  time=1.130s\n",
      "  Batch 7/20 [edges 6000:7000] loss=0.6678  time=1.539s\n",
      "  Batch 8/20 [edges 7000:8000] loss=0.6727  time=2.188s\n",
      "  Batch 9/20 [edges 8000:9000] loss=0.6742  time=2.665s\n",
      "  Batch 10/20 [edges 9000:10000] loss=0.6783  time=1.749s\n",
      "  Batch 11/20 [edges 10000:11000] loss=0.6802  time=1.216s\n",
      "  Batch 12/20 [edges 11000:12000] loss=0.6672  time=1.949s\n",
      "  Batch 13/20 [edges 12000:13000] loss=0.6730  time=2.497s\n",
      "  Batch 14/20 [edges 13000:14000] loss=0.6749  time=2.663s\n",
      "  Batch 15/20 [edges 14000:15000] loss=0.6777  time=2.065s\n",
      "  Batch 16/20 [edges 15000:16000] loss=0.6771  time=2.190s\n",
      "  Batch 17/20 [edges 16000:17000] loss=0.6734  time=2.420s\n",
      "  Batch 18/20 [edges 17000:18000] loss=0.6685  time=2.163s\n",
      "  Batch 19/20 [edges 18000:19000] loss=0.6917  time=1.036s\n",
      "  Batch 20/20 [edges 19000:20000] loss=0.6676  time=1.891s\n",
      "Epoch 1 total loss: 13.4583\n",
      "Epoch 1 time: 38.94s\n",
      "\n",
      "=== Epoch 2/15 ===\n",
      "  Batch 1/20 [edges 0:1000] loss=0.6686  time=2.327s\n",
      "  Batch 2/20 [edges 1000:2000] loss=0.6688  time=1.739s\n",
      "  Batch 3/20 [edges 2000:3000] loss=0.6677  time=1.178s\n",
      "  Batch 4/20 [edges 3000:4000] loss=0.6677  time=2.585s\n",
      "  Batch 5/20 [edges 4000:5000] loss=0.6697  time=1.318s\n",
      "  Batch 6/20 [edges 5000:6000] loss=0.6713  time=1.147s\n",
      "  Batch 7/20 [edges 6000:7000] loss=0.6669  time=1.564s\n",
      "  Batch 8/20 [edges 7000:8000] loss=0.6704  time=2.199s\n",
      "  Batch 9/20 [edges 8000:9000] loss=0.6701  time=2.666s\n",
      "  Batch 10/20 [edges 9000:10000] loss=0.6760  time=1.778s\n",
      "  Batch 11/20 [edges 10000:11000] loss=0.6782  time=1.292s\n",
      "  Batch 12/20 [edges 11000:12000] loss=0.6667  time=1.939s\n",
      "  Batch 13/20 [edges 12000:13000] loss=0.6673  time=2.438s\n",
      "  Batch 14/20 [edges 13000:14000] loss=0.6733  time=2.713s\n",
      "  Batch 15/20 [edges 14000:15000] loss=0.6722  time=2.019s\n",
      "  Batch 16/20 [edges 15000:16000] loss=0.6750  time=2.226s\n",
      "  Batch 17/20 [edges 16000:17000] loss=0.6664  time=2.396s\n",
      "  Batch 18/20 [edges 17000:18000] loss=0.6638  time=2.176s\n",
      "  Batch 19/20 [edges 18000:19000] loss=0.6878  time=1.004s\n",
      "  Batch 20/20 [edges 19000:20000] loss=0.6643  time=1.834s\n",
      "Epoch 2 total loss: 13.4124\n",
      "Epoch 2 time: 38.55s\n",
      "\n",
      "=== Epoch 3/15 ===\n",
      "  Batch 1/20 [edges 0:1000] loss=0.6625  time=2.359s\n",
      "  Batch 2/20 [edges 1000:2000] loss=0.6628  time=1.689s\n",
      "  Batch 3/20 [edges 2000:3000] loss=0.6650  time=1.573s\n",
      "  Batch 4/20 [edges 3000:4000] loss=0.6644  time=2.564s\n",
      "  Batch 5/20 [edges 4000:5000] loss=0.6674  time=1.355s\n",
      "  Batch 6/20 [edges 5000:6000] loss=0.6651  time=1.131s\n",
      "  Batch 7/20 [edges 6000:7000] loss=0.6676  time=1.544s\n",
      "  Batch 8/20 [edges 7000:8000] loss=0.6693  time=2.126s\n",
      "  Batch 9/20 [edges 8000:9000] loss=0.6684  time=2.651s\n",
      "  Batch 10/20 [edges 9000:10000] loss=0.6741  time=1.751s\n",
      "  Batch 11/20 [edges 10000:11000] loss=0.6744  time=1.230s\n",
      "  Batch 12/20 [edges 11000:12000] loss=0.6640  time=1.919s\n",
      "  Batch 13/20 [edges 12000:13000] loss=0.6648  time=2.490s\n",
      "  Batch 14/20 [edges 13000:14000] loss=0.6707  time=2.642s\n",
      "  Batch 15/20 [edges 14000:15000] loss=0.6690  time=1.998s\n",
      "  Batch 16/20 [edges 15000:16000] loss=0.6684  time=2.194s\n",
      "  Batch 17/20 [edges 16000:17000] loss=0.6629  time=2.386s\n",
      "  Batch 18/20 [edges 17000:18000] loss=0.6580  time=2.119s\n",
      "  Batch 19/20 [edges 18000:19000] loss=0.6785  time=1.055s\n",
      "  Batch 20/20 [edges 19000:20000] loss=0.6568  time=1.855s\n",
      "Epoch 3 total loss: 13.3341\n",
      "Epoch 3 time: 38.64s\n",
      "\n",
      "=== Epoch 4/15 ===\n",
      "  Batch 1/20 [edges 0:1000] loss=0.6592  time=2.297s\n",
      "  Batch 2/20 [edges 1000:2000] loss=0.6521  time=1.701s\n",
      "  Batch 3/20 [edges 2000:3000] loss=0.6576  time=1.121s\n",
      "  Batch 4/20 [edges 3000:4000] loss=0.6557  time=2.619s\n",
      "  Batch 5/20 [edges 4000:5000] loss=0.6565  time=1.344s\n",
      "  Batch 6/20 [edges 5000:6000] loss=0.6547  time=1.135s\n",
      "  Batch 7/20 [edges 6000:7000] loss=0.6566  time=1.527s\n",
      "  Batch 8/20 [edges 7000:8000] loss=0.6570  time=2.201s\n",
      "  Batch 9/20 [edges 8000:9000] loss=0.6598  time=2.653s\n",
      "  Batch 10/20 [edges 9000:10000] loss=0.6667  time=1.781s\n",
      "  Batch 11/20 [edges 10000:11000] loss=0.6641  time=1.254s\n",
      "  Batch 12/20 [edges 11000:12000] loss=0.6515  time=1.971s\n",
      "  Batch 13/20 [edges 12000:13000] loss=0.6531  time=2.525s\n",
      "  Batch 14/20 [edges 13000:14000] loss=0.6617  time=2.856s\n",
      "  Batch 15/20 [edges 14000:15000] loss=0.6551  time=2.007s\n",
      "  Batch 16/20 [edges 15000:16000] loss=0.6600  time=2.090s\n",
      "  Batch 17/20 [edges 16000:17000] loss=0.6541  time=2.366s\n",
      "  Batch 18/20 [edges 17000:18000] loss=0.6527  time=2.250s\n",
      "  Batch 19/20 [edges 18000:19000] loss=0.6728  time=1.080s\n",
      "  Batch 20/20 [edges 19000:20000] loss=0.6459  time=1.935s\n",
      "Epoch 4 total loss: 13.1470\n",
      "Epoch 4 time: 38.72s\n",
      "\n",
      "=== Epoch 5/15 ===\n",
      "  Batch 1/20 [edges 0:1000] loss=0.6531  time=2.460s\n",
      "  Batch 2/20 [edges 1000:2000] loss=0.6449  time=1.724s\n",
      "  Batch 3/20 [edges 2000:3000] loss=0.6508  time=1.205s\n",
      "  Batch 4/20 [edges 3000:4000] loss=0.6544  time=2.611s\n",
      "  Batch 5/20 [edges 4000:5000] loss=0.6502  time=1.391s\n",
      "  Batch 6/20 [edges 5000:6000] loss=0.6497  time=1.167s\n",
      "  Batch 7/20 [edges 6000:7000] loss=0.6532  time=1.555s\n",
      "  Batch 8/20 [edges 7000:8000] loss=0.6541  time=2.135s\n",
      "  Batch 9/20 [edges 8000:9000] loss=0.6548  time=2.667s\n",
      "  Batch 10/20 [edges 9000:10000] loss=0.6611  time=1.764s\n",
      "  Batch 11/20 [edges 10000:11000] loss=0.6556  time=1.242s\n",
      "  Batch 12/20 [edges 11000:12000] loss=0.6465  time=1.930s\n",
      "  Batch 13/20 [edges 12000:13000] loss=0.6468  time=2.521s\n",
      "  Batch 14/20 [edges 13000:14000] loss=0.6524  time=2.660s\n",
      "  Batch 15/20 [edges 14000:15000] loss=0.6470  time=1.947s\n",
      "  Batch 16/20 [edges 15000:16000] loss=0.6514  time=2.155s\n",
      "  Batch 17/20 [edges 16000:17000] loss=0.6448  time=2.409s\n",
      "  Batch 18/20 [edges 17000:18000] loss=0.6431  time=2.157s\n",
      "  Batch 19/20 [edges 18000:19000] loss=0.6604  time=1.047s\n",
      "  Batch 20/20 [edges 19000:20000] loss=0.6399  time=1.832s\n",
      "Epoch 5 total loss: 13.0140\n",
      "Epoch 5 time: 38.59s\n",
      "\n",
      "=== Epoch 6/15 ===\n",
      "  Batch 1/20 [edges 0:1000] loss=0.6425  time=2.294s\n",
      "  Batch 2/20 [edges 1000:2000] loss=0.6379  time=1.684s\n",
      "  Batch 3/20 [edges 2000:3000] loss=0.6408  time=1.131s\n",
      "  Batch 4/20 [edges 3000:4000] loss=0.6436  time=2.578s\n",
      "  Batch 5/20 [edges 4000:5000] loss=0.6396  time=1.330s\n",
      "  Batch 6/20 [edges 5000:6000] loss=0.6414  time=1.120s\n",
      "  Batch 7/20 [edges 6000:7000] loss=0.6426  time=1.536s\n",
      "  Batch 8/20 [edges 7000:8000] loss=0.6422  time=2.137s\n",
      "  Batch 9/20 [edges 8000:9000] loss=0.6425  time=2.633s\n",
      "  Batch 10/20 [edges 9000:10000] loss=0.6502  time=2.189s\n",
      "  Batch 11/20 [edges 10000:11000] loss=0.6504  time=1.264s\n",
      "  Batch 12/20 [edges 11000:12000] loss=0.6375  time=1.911s\n",
      "  Batch 13/20 [edges 12000:13000] loss=0.6432  time=2.482s\n",
      "  Batch 14/20 [edges 13000:14000] loss=0.6448  time=2.606s\n",
      "  Batch 15/20 [edges 14000:15000] loss=0.6403  time=2.010s\n",
      "  Batch 16/20 [edges 15000:16000] loss=0.6447  time=2.170s\n",
      "  Batch 17/20 [edges 16000:17000] loss=0.6370  time=2.375s\n",
      "  Batch 18/20 [edges 17000:18000] loss=0.6351  time=2.095s\n",
      "  Batch 19/20 [edges 18000:19000] loss=0.6508  time=1.022s\n",
      "  Batch 20/20 [edges 19000:20000] loss=0.6381  time=1.864s\n",
      "Epoch 6 total loss: 12.8455\n",
      "Epoch 6 time: 38.44s\n",
      "\n",
      "=== Epoch 7/15 ===\n",
      "  Batch 1/20 [edges 0:1000] loss=0.6354  time=2.238s\n",
      "  Batch 2/20 [edges 1000:2000] loss=0.6305  time=1.661s\n",
      "  Batch 3/20 [edges 2000:3000] loss=0.6352  time=1.150s\n",
      "  Batch 4/20 [edges 3000:4000] loss=0.6379  time=2.606s\n",
      "  Batch 5/20 [edges 4000:5000] loss=0.6322  time=1.316s\n",
      "  Batch 6/20 [edges 5000:6000] loss=0.6358  time=1.155s\n",
      "  Batch 7/20 [edges 6000:7000] loss=0.6386  time=1.548s\n",
      "  Batch 8/20 [edges 7000:8000] loss=0.6369  time=2.150s\n",
      "  Batch 9/20 [edges 8000:9000] loss=0.6381  time=2.652s\n",
      "  Batch 10/20 [edges 9000:10000] loss=0.6464  time=1.710s\n",
      "  Batch 11/20 [edges 10000:11000] loss=0.6450  time=1.248s\n",
      "  Batch 12/20 [edges 11000:12000] loss=0.6360  time=1.900s\n",
      "  Batch 13/20 [edges 12000:13000] loss=0.6415  time=2.407s\n",
      "  Batch 14/20 [edges 13000:14000] loss=0.6440  time=2.584s\n",
      "  Batch 15/20 [edges 14000:15000] loss=0.6372  time=1.962s\n",
      "  Batch 16/20 [edges 15000:16000] loss=0.6434  time=2.114s\n",
      "  Batch 17/20 [edges 16000:17000] loss=0.6367  time=2.325s\n",
      "  Batch 18/20 [edges 17000:18000] loss=0.6356  time=2.101s\n",
      "  Batch 19/20 [edges 18000:19000] loss=0.6522  time=1.035s\n",
      "  Batch 20/20 [edges 19000:20000] loss=0.6348  time=1.819s\n",
      "Epoch 7 total loss: 12.7735\n",
      "Epoch 7 time: 37.69s\n",
      "\n",
      "=== Epoch 8/15 ===\n",
      "  Batch 1/20 [edges 0:1000] loss=0.6363  time=2.262s\n",
      "  Batch 2/20 [edges 1000:2000] loss=0.6320  time=1.669s\n",
      "  Batch 3/20 [edges 2000:3000] loss=0.6360  time=1.143s\n",
      "  Batch 4/20 [edges 3000:4000] loss=0.6403  time=2.541s\n",
      "  Batch 5/20 [edges 4000:5000] loss=0.6334  time=1.307s\n",
      "  Batch 6/20 [edges 5000:6000] loss=0.6371  time=1.121s\n",
      "  Batch 7/20 [edges 6000:7000] loss=0.6363  time=1.529s\n",
      "  Batch 8/20 [edges 7000:8000] loss=0.6360  time=2.123s\n",
      "  Batch 9/20 [edges 8000:9000] loss=0.6378  time=2.616s\n",
      "  Batch 10/20 [edges 9000:10000] loss=0.6465  time=1.757s\n",
      "  Batch 11/20 [edges 10000:11000] loss=0.6442  time=1.271s\n",
      "  Batch 12/20 [edges 11000:12000] loss=0.6372  time=1.888s\n",
      "  Batch 13/20 [edges 12000:13000] loss=0.6375  time=2.469s\n",
      "  Batch 14/20 [edges 13000:14000] loss=0.6426  time=2.660s\n",
      "  Batch 15/20 [edges 14000:15000] loss=0.6374  time=1.990s\n",
      "  Batch 16/20 [edges 15000:16000] loss=0.6441  time=2.144s\n",
      "  Batch 17/20 [edges 16000:17000] loss=0.6368  time=2.376s\n",
      "  Batch 18/20 [edges 17000:18000] loss=0.6366  time=2.118s\n",
      "  Batch 19/20 [edges 18000:19000] loss=0.6556  time=1.009s\n",
      "  Batch 20/20 [edges 19000:20000] loss=0.6373  time=1.807s\n",
      "Epoch 8 total loss: 12.7808\n",
      "Epoch 8 time: 37.81s\n",
      "\n",
      "=== Epoch 9/15 ===\n",
      "  Batch 1/20 [edges 0:1000] loss=0.6383  time=2.347s\n",
      "  Batch 2/20 [edges 1000:2000] loss=0.6334  time=1.685s\n",
      "  Batch 3/20 [edges 2000:3000] loss=0.6362  time=1.132s\n",
      "  Batch 4/20 [edges 3000:4000] loss=0.6396  time=2.588s\n",
      "  Batch 5/20 [edges 4000:5000] loss=0.6354  time=1.342s\n",
      "  Batch 6/20 [edges 5000:6000] loss=0.6365  time=1.560s\n",
      "  Batch 7/20 [edges 6000:7000] loss=0.6405  time=1.513s\n",
      "  Batch 8/20 [edges 7000:8000] loss=0.6378  time=2.141s\n",
      "  Batch 9/20 [edges 8000:9000] loss=0.6398  time=2.642s\n",
      "  Batch 10/20 [edges 9000:10000] loss=0.6471  time=1.766s\n",
      "  Batch 11/20 [edges 10000:11000] loss=0.6479  time=1.230s\n",
      "  Batch 12/20 [edges 11000:12000] loss=0.6417  time=1.883s\n",
      "  Batch 13/20 [edges 12000:13000] loss=0.6407  time=2.445s\n",
      "  Batch 14/20 [edges 13000:14000] loss=0.6464  time=2.613s\n",
      "  Batch 15/20 [edges 14000:15000] loss=0.6402  time=1.942s\n",
      "  Batch 16/20 [edges 15000:16000] loss=0.6436  time=2.145s\n",
      "  Batch 17/20 [edges 16000:17000] loss=0.6351  time=2.372s\n",
      "  Batch 18/20 [edges 17000:18000] loss=0.6347  time=2.140s\n",
      "  Batch 19/20 [edges 18000:19000] loss=0.6542  time=1.047s\n",
      "  Batch 20/20 [edges 19000:20000] loss=0.6375  time=1.888s\n",
      "Epoch 9 total loss: 12.8065\n",
      "Epoch 9 time: 38.43s\n",
      "\n",
      "=== Epoch 10/15 ===\n",
      "  Batch 1/20 [edges 0:1000] loss=0.6372  time=2.286s\n",
      "  Batch 2/20 [edges 1000:2000] loss=0.6330  time=1.674s\n",
      "  Batch 3/20 [edges 2000:3000] loss=0.6374  time=1.146s\n",
      "  Batch 4/20 [edges 3000:4000] loss=0.6394  time=2.582s\n",
      "  Batch 5/20 [edges 4000:5000] loss=0.6354  time=1.347s\n",
      "  Batch 6/20 [edges 5000:6000] loss=0.6357  time=1.110s\n",
      "  Batch 7/20 [edges 6000:7000] loss=0.6398  time=1.540s\n",
      "  Batch 8/20 [edges 7000:8000] loss=0.6375  time=2.151s\n",
      "  Batch 9/20 [edges 8000:9000] loss=0.6415  time=2.634s\n",
      "  Batch 10/20 [edges 9000:10000] loss=0.6470  time=1.763s\n",
      "  Batch 11/20 [edges 10000:11000] loss=0.6445  time=1.232s\n",
      "  Batch 12/20 [edges 11000:12000] loss=0.6412  time=1.944s\n",
      "  Batch 13/20 [edges 12000:13000] loss=0.6407  time=2.470s\n",
      "  Batch 14/20 [edges 13000:14000] loss=0.6463  time=2.657s\n",
      "  Batch 15/20 [edges 14000:15000] loss=0.6388  time=1.981s\n",
      "  Batch 16/20 [edges 15000:16000] loss=0.6432  time=2.120s\n",
      "  Batch 17/20 [edges 16000:17000] loss=0.6351  time=2.333s\n",
      "  Batch 18/20 [edges 17000:18000] loss=0.6342  time=2.125s\n",
      "  Batch 19/20 [edges 18000:19000] loss=0.6546  time=1.021s\n",
      "  Batch 20/20 [edges 19000:20000] loss=0.6365  time=1.848s\n",
      "Epoch 10 total loss: 12.7991\n",
      "Epoch 10 time: 37.97s\n",
      "\n",
      "=== Epoch 11/15 ===\n",
      "  Batch 1/20 [edges 0:1000] loss=0.6359  time=2.282s\n",
      "  Batch 2/20 [edges 1000:2000] loss=0.6305  time=1.695s\n",
      "  Batch 3/20 [edges 2000:3000] loss=0.6364  time=1.127s\n",
      "  Batch 4/20 [edges 3000:4000] loss=0.6390  time=2.537s\n",
      "  Batch 5/20 [edges 4000:5000] loss=0.6354  time=1.305s\n",
      "  Batch 6/20 [edges 5000:6000] loss=0.6361  time=1.117s\n",
      "  Batch 7/20 [edges 6000:7000] loss=0.6351  time=1.541s\n",
      "  Batch 8/20 [edges 7000:8000] loss=0.6370  time=2.157s\n",
      "  Batch 9/20 [edges 8000:9000] loss=0.6413  time=2.647s\n",
      "  Batch 10/20 [edges 9000:10000] loss=0.6460  time=1.748s\n",
      "  Batch 11/20 [edges 10000:11000] loss=0.6428  time=1.239s\n",
      "  Batch 12/20 [edges 11000:12000] loss=0.6394  time=1.957s\n",
      "  Batch 13/20 [edges 12000:13000] loss=0.6388  time=2.445s\n",
      "  Batch 14/20 [edges 13000:14000] loss=0.6449  time=3.169s\n",
      "  Batch 15/20 [edges 14000:15000] loss=0.6375  time=2.035s\n",
      "  Batch 16/20 [edges 15000:16000] loss=0.6425  time=2.204s\n",
      "  Batch 17/20 [edges 16000:17000] loss=0.6335  time=2.446s\n",
      "  Batch 18/20 [edges 17000:18000] loss=0.6323  time=2.134s\n",
      "  Batch 19/20 [edges 18000:19000] loss=0.6499  time=1.024s\n",
      "  Batch 20/20 [edges 19000:20000] loss=0.6358  time=1.874s\n",
      "Epoch 11 total loss: 12.7701\n",
      "Epoch 11 time: 38.69s\n",
      "\n",
      "=== Epoch 12/15 ===\n",
      "  Batch 1/20 [edges 0:1000] loss=0.6337  time=2.326s\n",
      "  Batch 2/20 [edges 1000:2000] loss=0.6291  time=1.690s\n",
      "  Batch 3/20 [edges 2000:3000] loss=0.6334  time=1.097s\n",
      "  Batch 4/20 [edges 3000:4000] loss=0.6349  time=2.608s\n",
      "  Batch 5/20 [edges 4000:5000] loss=0.6323  time=1.330s\n",
      "  Batch 6/20 [edges 5000:6000] loss=0.6350  time=1.121s\n",
      "  Batch 7/20 [edges 6000:7000] loss=0.6278  time=1.481s\n",
      "  Batch 8/20 [edges 7000:8000] loss=0.6332  time=2.109s\n",
      "  Batch 9/20 [edges 8000:9000] loss=0.6331  time=2.588s\n",
      "  Batch 10/20 [edges 9000:10000] loss=0.6421  time=1.715s\n",
      "  Batch 11/20 [edges 10000:11000] loss=0.6422  time=1.218s\n",
      "  Batch 12/20 [edges 11000:12000] loss=0.6376  time=1.887s\n",
      "  Batch 13/20 [edges 12000:13000] loss=0.6384  time=2.486s\n",
      "  Batch 14/20 [edges 13000:14000] loss=0.6409  time=2.645s\n",
      "  Batch 15/20 [edges 14000:15000] loss=0.6324  time=1.977s\n",
      "  Batch 16/20 [edges 15000:16000] loss=0.6372  time=2.155s\n",
      "  Batch 17/20 [edges 16000:17000] loss=0.6316  time=2.368s\n",
      "  Batch 18/20 [edges 17000:18000] loss=0.6261  time=2.129s\n",
      "  Batch 19/20 [edges 18000:19000] loss=0.6393  time=1.022s\n",
      "  Batch 20/20 [edges 19000:20000] loss=0.6333  time=1.841s\n",
      "Epoch 12 total loss: 12.6937\n",
      "Epoch 12 time: 37.80s\n",
      "\n",
      "=== Epoch 13/15 ===\n",
      "  Batch 1/20 [edges 0:1000] loss=0.6326  time=2.304s\n",
      "  Batch 2/20 [edges 1000:2000] loss=0.6253  time=1.643s\n",
      "  Batch 3/20 [edges 2000:3000] loss=0.6336  time=1.115s\n",
      "  Batch 4/20 [edges 3000:4000] loss=0.6333  time=2.546s\n",
      "  Batch 5/20 [edges 4000:5000] loss=0.6296  time=1.325s\n",
      "  Batch 6/20 [edges 5000:6000] loss=0.6322  time=1.082s\n",
      "  Batch 7/20 [edges 6000:7000] loss=0.6271  time=1.509s\n",
      "  Batch 8/20 [edges 7000:8000] loss=0.6320  time=2.118s\n",
      "  Batch 9/20 [edges 8000:9000] loss=0.6320  time=2.647s\n",
      "  Batch 10/20 [edges 9000:10000] loss=0.6384  time=1.710s\n",
      "  Batch 11/20 [edges 10000:11000] loss=0.6388  time=1.225s\n",
      "  Batch 12/20 [edges 11000:12000] loss=0.6357  time=1.964s\n",
      "  Batch 13/20 [edges 12000:13000] loss=0.6362  time=2.454s\n",
      "  Batch 14/20 [edges 13000:14000] loss=0.6372  time=2.613s\n",
      "  Batch 15/20 [edges 14000:15000] loss=0.6326  time=1.999s\n",
      "  Batch 16/20 [edges 15000:16000] loss=0.6374  time=2.569s\n",
      "  Batch 17/20 [edges 16000:17000] loss=0.6292  time=2.306s\n",
      "  Batch 18/20 [edges 17000:18000] loss=0.6256  time=2.101s\n",
      "  Batch 19/20 [edges 18000:19000] loss=0.6348  time=1.022s\n",
      "  Batch 20/20 [edges 19000:20000] loss=0.6307  time=1.798s\n",
      "Epoch 13 total loss: 12.6544\n",
      "Epoch 13 time: 38.06s\n",
      "\n",
      "=== Epoch 14/15 ===\n",
      "  Batch 1/20 [edges 0:1000] loss=0.6337  time=2.226s\n",
      "  Batch 2/20 [edges 1000:2000] loss=0.6235  time=1.657s\n",
      "  Batch 3/20 [edges 2000:3000] loss=0.6305  time=1.125s\n",
      "  Batch 4/20 [edges 3000:4000] loss=0.6296  time=2.512s\n",
      "  Batch 5/20 [edges 4000:5000] loss=0.6283  time=1.331s\n",
      "  Batch 6/20 [edges 5000:6000] loss=0.6321  time=1.102s\n",
      "  Batch 7/20 [edges 6000:7000] loss=0.6230  time=1.554s\n",
      "  Batch 8/20 [edges 7000:8000] loss=0.6298  time=2.169s\n",
      "  Batch 9/20 [edges 8000:9000] loss=0.6326  time=2.582s\n",
      "  Batch 10/20 [edges 9000:10000] loss=0.6367  time=1.766s\n",
      "  Batch 11/20 [edges 10000:11000] loss=0.6351  time=1.255s\n",
      "  Batch 12/20 [edges 11000:12000] loss=0.6344  time=1.924s\n",
      "  Batch 13/20 [edges 12000:13000] loss=0.6367  time=2.528s\n",
      "  Batch 14/20 [edges 13000:14000] loss=0.6357  time=2.738s\n",
      "  Batch 15/20 [edges 14000:15000] loss=0.6328  time=2.144s\n",
      "  Batch 16/20 [edges 15000:16000] loss=0.6389  time=2.388s\n",
      "  Batch 17/20 [edges 16000:17000] loss=0.6290  time=2.714s\n",
      "  Batch 18/20 [edges 17000:18000] loss=0.6238  time=2.308s\n",
      "  Batch 19/20 [edges 18000:19000] loss=0.6344  time=1.011s\n",
      "  Batch 20/20 [edges 19000:20000] loss=0.6278  time=1.919s\n",
      "Epoch 14 total loss: 12.6283\n",
      "Epoch 14 time: 38.96s\n",
      "\n",
      "=== Epoch 15/15 ===\n",
      "  Batch 1/20 [edges 0:1000] loss=0.6320  time=2.277s\n",
      "  Batch 2/20 [edges 1000:2000] loss=0.6259  time=1.659s\n",
      "  Batch 3/20 [edges 2000:3000] loss=0.6301  time=1.132s\n",
      "  Batch 4/20 [edges 3000:4000] loss=0.6327  time=2.559s\n",
      "  Batch 5/20 [edges 4000:5000] loss=0.6321  time=1.391s\n",
      "  Batch 6/20 [edges 5000:6000] loss=0.6299  time=1.331s\n",
      "  Batch 7/20 [edges 6000:7000] loss=0.6218  time=1.543s\n",
      "  Batch 8/20 [edges 7000:8000] loss=0.6301  time=2.135s\n",
      "  Batch 9/20 [edges 8000:9000] loss=0.6331  time=2.677s\n",
      "  Batch 10/20 [edges 9000:10000] loss=0.6376  time=1.711s\n",
      "  Batch 11/20 [edges 10000:11000] loss=0.6345  time=1.253s\n",
      "  Batch 12/20 [edges 11000:12000] loss=0.6322  time=1.898s\n",
      "  Batch 13/20 [edges 12000:13000] loss=0.6350  time=2.434s\n",
      "  Batch 14/20 [edges 13000:14000] loss=0.6362  time=3.054s\n",
      "  Batch 15/20 [edges 14000:15000] loss=0.6295  time=2.020s\n",
      "  Batch 16/20 [edges 15000:16000] loss=0.6363  time=2.137s\n",
      "  Batch 17/20 [edges 16000:17000] loss=0.6273  time=2.363s\n",
      "  Batch 18/20 [edges 17000:18000] loss=0.6216  time=2.135s\n",
      "  Batch 19/20 [edges 18000:19000] loss=0.6322  time=1.023s\n",
      "  Batch 20/20 [edges 19000:20000] loss=0.6237  time=1.828s\n",
      "Epoch 15 total loss: 12.6137\n",
      "Epoch 15 time: 38.57s\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "import torch.nn as nn\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG: much smaller subset + smaller batches for debugging\n",
    "# ============================================================\n",
    "MAX_EDGES_DEBUG = 20_000    # <-- try 20k first, then you can scale up\n",
    "BATCH_SIZE      = 1_000     # <-- smaller batch so each step is visible\n",
    "NUM_EPOCHS      = 15         # <-- start with 1 epoch just to verify it runs\n",
    "LR              = 1e-4\n",
    "\n",
    "# Slice tensors to a manageable size\n",
    "src_debug = src_t[:MAX_EDGES_DEBUG]\n",
    "dst_debug = dst_t[:MAX_EDGES_DEBUG]\n",
    "neg_debug = neg_t[:MAX_EDGES_DEBUG]\n",
    "ts_debug  = ts_t[:MAX_EDGES_DEBUG]\n",
    "eid_debug = eid_t[:MAX_EDGES_DEBUG]\n",
    "\n",
    "num_edges_debug = src_debug.size(0)\n",
    "print(f\"Using {num_edges_debug:,} edges for debug training\")\n",
    "\n",
    "# Build batches over the debug subset\n",
    "batches = [\n",
    "    (start, min(start + BATCH_SIZE, num_edges_debug))\n",
    "    for start in range(0, num_edges_debug, BATCH_SIZE)\n",
    "]\n",
    "print(f\"Total batches per epoch: {len(batches)}\")\n",
    "\n",
    "# ============================================================\n",
    "# LOSS + OPTIMIZER\n",
    "# ============================================================\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(tgn.parameters(), lr=LR)\n",
    "\n",
    "tgn.train()\n",
    "losses = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\n=== Epoch {epoch + 1}/{NUM_EPOCHS} ===\")\n",
    "\n",
    "    # Reset memory at epoch start\n",
    "    if tgn.use_memory:\n",
    "        tgn.memory.__init_memory__()\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    epoch_start = time.perf_counter()\n",
    "\n",
    "    for batch_idx, (start, end) in enumerate(batches):\n",
    "        batch_start = time.perf_counter()\n",
    "\n",
    "        # Select batch window from the *debug* tensors\n",
    "        src_b = src_debug[start:end]\n",
    "        dst_b = dst_debug[start:end]\n",
    "        neg_b = neg_debug[start:end]\n",
    "        ts_b  = ts_debug[start:end]\n",
    "        eid_b = eid_debug[start:end]\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # Forward pass through TGN (TGN expects numpy arrays)\n",
    "        # ----------------------------------------------------\n",
    "        pos_scores, neg_scores = tgn.compute_edge_probabilities(\n",
    "            src_b.cpu().numpy(),\n",
    "            dst_b.cpu().numpy(),\n",
    "            neg_b.cpu().numpy(),\n",
    "            ts_b.cpu().numpy(),\n",
    "            eid_b.cpu().numpy(),\n",
    "        )\n",
    "\n",
    "        # Move back to device for loss computation\n",
    "        pos_scores = pos_scores.to(device)\n",
    "        neg_scores = neg_scores.to(device)\n",
    "\n",
    "        # Labels\n",
    "        pos_y = torch.ones_like(pos_scores)\n",
    "        neg_y = torch.zeros_like(neg_scores)\n",
    "\n",
    "        # Combine\n",
    "        scores = torch.cat([pos_scores, neg_scores], dim=0)\n",
    "        labels = torch.cat([pos_y, neg_y], dim=0)\n",
    "\n",
    "        # Loss + backward\n",
    "        loss = criterion(scores, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        batch_time = time.perf_counter() - batch_start\n",
    "        print(\n",
    "            f\"  Batch {batch_idx + 1}/{len(batches)} \"\n",
    "            f\"[edges {start}:{end}] \"\n",
    "            f\"loss={loss.item():.4f}  \"\n",
    "            f\"time={batch_time:.3f}s\"\n",
    "        )\n",
    "\n",
    "    epoch_time = time.perf_counter() - epoch_start\n",
    "    print(f\"Epoch {epoch + 1} total loss: {epoch_loss:.4f}\")\n",
    "    print(f\"Epoch {epoch + 1} time: {epoch_time:.2f}s\")\n",
    "\n",
    "    losses.append(epoch_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e5766793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved debug TGN checkpoint.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.save(tgn.state_dict(), \"tgn_hismall_linkpred_debug.pt\")\n",
    "print(\"Saved debug TGN checkpoint.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "423210eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link prediction AUC: 0.6314092800000001\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "tgn.eval()\n",
    "with torch.no_grad():\n",
    "    # take another 5k edges as \"test\"\n",
    "    TEST_EDGES = 5000\n",
    "    src_te = src_t[20_000:20_000 + TEST_EDGES]\n",
    "    dst_te = dst_t[20_000:20_000 + TEST_EDGES]\n",
    "    ts_te  = ts_t[20_000:20_000 + TEST_EDGES]\n",
    "    eid_te = eid_t[20_000:20_000 + TEST_EDGES]\n",
    "\n",
    "    # sample negatives\n",
    "    neg_te = torch.randint(\n",
    "        low=0,\n",
    "        high=num_nodes,\n",
    "        size=src_te.size(),\n",
    "        device=src_te.device,\n",
    "    )\n",
    "\n",
    "    pos_scores, neg_scores = tgn.compute_edge_probabilities(\n",
    "        src_te.cpu().numpy(),\n",
    "        dst_te.cpu().numpy(),\n",
    "        neg_te.cpu().numpy(),\n",
    "        ts_te.cpu().numpy(),\n",
    "        eid_te.cpu().numpy(),\n",
    "    )\n",
    "\n",
    "    pos_scores = pos_scores.cpu().numpy()\n",
    "    neg_scores = neg_scores.cpu().numpy()\n",
    "\n",
    "    y_true = np.concatenate([np.ones_like(pos_scores), np.zeros_like(neg_scores)])\n",
    "    y_score = np.concatenate([pos_scores, neg_scores])\n",
    "\n",
    "    auc = roc_auc_score(y_true, y_score)\n",
    "    print(\"Link prediction AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f26d19",
   "metadata": {},
   "source": [
    "## Visualizing in heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07f7e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# pick an index into the batch\n",
    "idx = 0  \n",
    "\n",
    "att = tgn.embedding_module.last_att_weights[idx].numpy()\n",
    "nbrs = tgn.embedding_module.last_neighbors[idx].numpy()\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.heatmap(att.reshape(1, -1), annot=False, cmap=\"viridis\")\n",
    "plt.xlabel(\"Neighbor Index (sorted by temporal closeness)\")\n",
    "plt.ylabel(\"Attention Weight\")\n",
    "plt.title(f\"Attention over neighbors for node {nbrs[idx]}\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
